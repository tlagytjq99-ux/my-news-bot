name: White House AI Scraper

on:
  schedule:
    - cron: '0 0 * * *' # ë§¤ì¼ ì•„ì¹¨ 9ì‹œ (KST) ìë™ ì‹¤í–‰
  workflow_dispatch:      # í•„ìš”í•  ë•Œ ìˆ˜ë™ìœ¼ë¡œ ë°”ë¡œ ì‹¤í–‰ ë²„íŠ¼ í™œì„±í™”

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write    # ê²°ê³¼ë¬¼ì„ ì €ì¥(Push)í•˜ê¸° ìœ„í•œ ê¶Œí•œ

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: |
          pip install crawl4ai requests pandas
          # ğŸ¯ Crawl4AIë¥¼ ìœ„í•œ ë¸Œë¼ìš°ì € ì„¤ì¹˜ (í•µì‹¬)
          python -m playwright install chromium --with-deps

      - name: Run Scraper
        run: python whitehouse_ai_crawler.py # íŒŒì¼ ì´ë¦„ì´ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!

      - name: Commit and Push
        run: |
          git config --global user.name "GitHub Action"
          git config --global user.email "action@github.com"
          git add whitehouse_ai_report.csv  # ğŸ‘ˆ íŒŒì´ì¬ì´ ë§Œë“œëŠ” ì´ë¦„ê³¼ ì¼ì¹˜í•´ì•¼ í•¨
          git commit -m "ğŸš€ ë°±ì•…ê´€ ë°ì´í„° ê°±ì‹ " || echo "ë³€ê²½ ì‚¬í•­ ì—†ìŒ"
          git push
