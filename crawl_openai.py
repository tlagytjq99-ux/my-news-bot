import requests
import xml.etree.ElementTree as ET
import pandas as pd
from datetime import datetime
from googletrans import Translator
import time
import os  # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ì„ ìœ„í•´ í•„ìš”

def crawl_openai_rss():
    file_name = "openai_news.xlsx"
    print("1. ìˆ˜ì§‘ ë° ëˆ„ì  í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
    
    url = "https://openai.com/news/rss.xml"
    headers = {"User-Agent": "Mozilla/5.0"}
    translator = Translator()
    collect_date = datetime.now().strftime("%Y-%m-%d")
    
    # ğŸ“‚ [ì¤‘ìš”] ê¸°ì¡´ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    existing_df = pd.DataFrame()
    existing_links = []
    if os.path.exists(file_name):
        try:
            existing_df = pd.read_excel(file_name)
            existing_links = existing_df['ë§í¬'].tolist()  # ì´ë¯¸ ìˆ˜ì§‘ëœ ë§í¬ ë¦¬ìŠ¤íŠ¸
            print(f"   - ê¸°ì¡´ ë°ì´í„° {len(existing_df)}ê±´ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"   - ê¸°ì¡´ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ(ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ìƒì„±): {e}")

    try:
        response = requests.get(url, headers=headers, timeout=10)
        root = ET.fromstring(response.content)
        print("2. RSS ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")
    except Exception as e:
        print(f"ì ‘ì† ì—ëŸ¬: {e}")
        return

    news_items = []
    items = root.findall(".//item")[:15] # ìµœì‹  15ê°œ í™•ì¸
    
    new_count = 0
    for i, item in enumerate(items):
        link = item.find("link").text
        
        # ğŸ›¡ï¸ [ì¤‘ë³µ ì²´í¬] ì´ë¯¸ ì—‘ì…€ì— ìˆëŠ” ë§í¬ë¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
        if link in existing_links:
            continue
            
        title_en = item.find("title").text
        pub_date_raw = item.find("pubDate").text
        
        # ë°œí–‰ì¼ í˜•ì‹ ë³€ê²½ (yyyy-mm-dd)
        try:
            # RSS ë‚ ì§œ ì˜ˆì‹œ: "Wed, 28 Jan 2026 10:00:00 GMT" -> "2026-01-28"
            date_part = pub_date_raw[5:16]
            date_obj = datetime.strptime(date_part, "%d %b %Y")
            pub_date = date_obj.strftime("%Y-%m-%d")
        except:
            pub_date = pub_date_raw

        # í•œê¸€ ë²ˆì—­
        try:
            print(f"   - [ì‹ ê·œ ê¸°ì‚¬] ë²ˆì—­ ì¤‘: {title_en[:30]}...")
            title_ko = translator.translate(title_en, src='en', dest='ko').text
            time.sleep(1.2) # ë²ˆì—­ API ì°¨ë‹¨ ë°©ì§€
        except Exception as e:
            print(f"   - ë²ˆì—­ ì‹¤íŒ¨ ({e})")
            title_ko = title_en

        news_items.append({
            "ìˆ˜ì§‘ì¼": collect_date,
            "ë°œí–‰ì¼": pub_date,
            "ê¸°ê´€": "OpenAI",
            "ì›ë¬¸ ì œëª©": title_en,
            "í•œê¸€ ë²ˆì—­ ì œëª©": title_ko,
            "ë§í¬": link
        })
        new_count += 1
    
    if new_count > 0:
        new_df = pd.DataFrame(news_items)
        # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„°ë¥¼ í•©ì¹©ë‹ˆë‹¤.
        final_df = pd.concat([new_df, existing_df], ignore_index=True)
        
        # ë°œí–‰ì¼ ê¸°ì¤€ ìµœì‹ ìˆœ ì •ë ¬
        final_df = final_df.sort_values(by="ë°œí–‰ì¼", ascending=False)
        
        # ì €ì¥
        final_df.to_excel(file_name, index=False)
        print(f"4. ì™„ë£Œ! ì‹ ê·œ {new_count}ê±´ì´ ì¶”ê°€ë˜ì–´ ì´ {len(final_df)}ê±´ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("4. ì—…ë°ì´íŠ¸í•  ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    crawl_openai_rss()
