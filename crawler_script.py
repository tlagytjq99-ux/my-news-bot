import feedparser
import csv
import urllib.parse
import time
from datetime import datetime
from googletrans import Translator
from googlenewsdecoder import gnewsdecoder

def get_config_by_country(country):
    configs = {
        "ëŒ€í•œë¯¼êµ­": {"hl": "ko", "gl": "KR"},
        "ì¼ë³¸": {"hl": "ja", "gl": "JP"},
        "ì¤‘êµ­": {"hl": "zh-CN", "gl": "CN"},
        "ëŒ€ë§Œ": {"hl": "zh-TW", "gl": "TW"},
        "í”„ë‘ìŠ¤": {"hl": "fr", "gl": "FR"},
        "ë…ì¼": {"hl": "de", "gl": "DE"},
        "ì˜¤ìŠ¤íŠ¸ë¦¬ì•„": {"hl": "de", "gl": "AT"},
        "ë„¤ëœë€ë“œ": {"hl": "nl", "gl": "NL"},
        "ë…¸ë¥´ì›¨ì´": {"hl": "no", "gl": "NO"},
        "ìŠ¤ì›¨ë´": {"hl": "sv", "gl": "SE"},
        "ë´ë§ˆí¬": {"hl": "da", "gl": "DK"},
        "í•€ë€ë“œ": {"hl": "fi", "gl": "FI"},
        "ì´ìŠ¤ë¼ì—˜": {"hl": "he", "gl": "IL"},
        "UAE": {"hl": "ar", "gl": "AE"},
        "ì‚¬ìš°ë””": {"hl": "ar", "gl": "SA"},
        "ë²¨ê¸°ì—": {"hl": "nl", "gl": "BE"}
    }
    return configs.get(country, {"hl": "en-US", "gl": "US"})

def get_localized_query(agency):
    country = agency['êµ­ê°€']
    domain = agency['ë„ë©”ì¸']
    keywords = {
        "ëŒ€í•œë¯¼êµ­": '("ì¸ê³µì§€ëŠ¥" OR AI OR "ë””ì§€í„¸" OR "ë°ì´í„°")',
        "ì¼ë³¸": '("äººå·¥çŸ¥èƒ½" OR AI OR "ãƒ‡ã‚¸ã‚¿ãƒ«æ”¿ç­–" OR "ICT")',
        "ì¤‘êµ­": '("äººå·¥æ™ºèƒ½" OR AI OR "æ•°å­—åŒ–" OR "é€šä¿¡")',
        "ëŒ€ë§Œ": '("äººå·¥æ™ºèƒ½" OR AI OR "æ•¸ä½åŒ–" OR "è³‡é€šè¨Š")',
        "ë…ì¼": '("KÃ¼nstliche Intelligenz" OR KI OR "Digitalisierung")',
        "í”„ë‘ìŠ¤": '("Intelligence Artificielle" OR IA OR "NumÃ©rique")',
        "ë„¤ëœë€ë“œ": '("Kunstmatige Intelligentie" OR AI OR "Digitalisering")'
    }
    kw = keywords.get(country, '("Artificial Intelligence" OR AI OR "Digital Policy" OR ICT)')
    return f'site:{domain} {kw}'

def main():
    # 50ê°œ ê¸°ê´€ ë¦¬ìŠ¤íŠ¸ (ì´ì „ê³¼ ë™ì¼í•˜ì—¬ ì¤‘ëµ, ì‹¤ì œ ì½”ë“œ ì‹¤í–‰ ì‹œ ì „ì²´ í¬í•¨ í•„ìš”)
    gov_agencies = [
        {"êµ­ê°€": "ë¯¸êµ­", "ê¸°ê´€": "ë°±ì•…ê´€", "ë„ë©”ì¸": "whitehouse.gov"},
        {"êµ­ê°€": "ëŒ€í•œë¯¼êµ­", "ê¸°ê´€": "ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€", "ë„ë©”ì¸": "msit.go.kr"},
        {"êµ­ê°€": "ì¼ë³¸", "ê¸°ê´€": "ë””ì§€í„¸ì²­", "ë„ë©”ì¸": "digital.go.jp"},
        # ... (ì´í•˜ 50ê°œ ê¸°ê´€ ë¦¬ìŠ¤íŠ¸)
    ]

    all_final_data = []
    seen_titles = set()
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")
    
    # ğŸš« ë…¸ì´ì¦ˆ í•„í„°ë§ í‚¤ì›Œë“œ ê°•í™” (í•œêµ­ì–´ ë° ì£¼ìš” ì™¸êµ­ì–´ í¬í•¨)
    exclude_keywords = [
        "ê²Œì‹œíŒ ì¸ì‡„", "ì¥ê´€ ì†Œê°œ", "ì±„ìš©", "ê³µê³ ", "ì¸ì‚¬", "ë¡œê·¸ì¸", "í™ˆí˜ì´ì§€", "ì°¾ì•„ì˜¤ì‹œëŠ”", 
        "RECRUITMENT", "LOGIN", "SEARCH", "ABOUT US", "CONTACT", "Q&A", "CV ", "PHOTO GALLERY",
        "æ¡ç”¨", "å‹Ÿé›†", "ãƒ­ã‚°ã‚¤ãƒ³", "ãŠå•ã„åˆã‚ã›", "OFFRE D'EMPLOI", "RECRUTEMENT"
    ]
    
    # âœ… í•„ìˆ˜ í¬í•¨ í‚¤ì›Œë“œ (ì´ ì¤‘ í•˜ë‚˜ë¼ë„ ì—†ìœ¼ë©´ íƒˆë½ì‹œì¼œ ì •í™•ë„ í–¥ìƒ)
    must_include = ["AI", "ì¸ê³µì§€ëŠ¥", "ë””ì§€í„¸", "ë°ì´í„°", "ICT", "í†µì‹ ", "í˜ì‹ ", "ê·œì œ", "STRATEGY", "POLICY", "DIGITAL", "DATA"]

    print(f"ğŸ“¡ {collected_date} ê³ ìˆœë„ ê¸€ë¡œë²Œ ì •ì±… ìˆ˜ì§‘ ê°€ë™...")

    for agency in gov_agencies:
        config = get_config_by_country(agency['êµ­ê°€'])
        query = get_localized_query(agency)
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl={config['hl']}&gl={config['gl']}&ceid={config['gl']}:{config['hl']}"

        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:5]: # ìƒìœ„ 5ê°œ í™•ì¸
                raw_title = entry.title.split(' - ')[0].strip()
                
                # 1. ì¤‘ë³µ ì œê±°
                if raw_title in seen_titles: continue
                
                # 2. ì œì™¸ í‚¤ì›Œë“œ í•„í„° (ë…¸ì´ì¦ˆ ì œê±°)
                if any(ex in raw_title.upper() for ex in exclude_keywords): continue
                
                # 3. ì œëª© ê¸¸ì´ í•„í„° (ë„ˆë¬´ ì§§ì€ ë©”ë‰´ëª… ë“± ì œê±°)
                if len(raw_title) < 12: continue

                # 4. ë‚ ì§œ í•„í„° (2025ë…„ ì´í›„ ë°ì´í„° ìš°ì„ )
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    if entry.published_parsed[0] < 2024: continue
                    pub_date = datetime(*entry.published_parsed[:3]).strftime('%Y-%m-%d')
                else: continue

                # 5. í•œêµ­ì–´ ë²ˆì—­
                try:
                    title_ko = raw_title if agency['êµ­ê°€'] == "ëŒ€í•œë¯¼êµ­" else translator.translate(raw_title, dest='ko').text
                except: title_ko = raw_title

                # 6. ë²ˆì—­ë³¸ ê¸°ë°˜ í•„ìˆ˜ í‚¤ì›Œë“œ ê²€ì¦ (í•œ ë²ˆ ë” í•„í„°ë§)
                if not any(word in title_ko.upper() for word in must_include): continue

                # 7. ë§í¬ í•´ë…
                try:
                    decoded = gnewsdecoder(entry.link)
                    actual_link = decoded.get('decoded_url', entry.link)
                except: actual_link = entry.link

                all_final_data.append({
                    "êµ­ê°€": agency["êµ­ê°€"], "ê¸°ê´€": agency["ê¸°ê´€"], "ë°œí–‰ì¼": pub_date,
                    "ì œëª©": title_ko, "ì›ë¬¸": raw_title, "ë§í¬": actual_link, "ìˆ˜ì§‘ì¼": collected_date
                })
                seen_titles.add(raw_title)
            
            time.sleep(1)
        except Exception as e:
            print(f"âŒ {agency['ê¸°ê´€']} ì˜¤ë¥˜: {e}")

    # ìµœì¢… ì •ë ¬: ìµœì‹ ìˆœ
    all_final_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)

    # CSV ì €ì¥
    file_name = f'global_ict_clean_{collected_date}.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["êµ­ê°€", "ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
        writer.writeheader()
        writer.writerows(all_final_data)
        
    print(f"âœ… í•„í„°ë§ ì™„ë£Œ: ì´ {len(all_final_data)}ê±´ì˜ ê³ ìˆœë„ ë°ì´í„° ì €ì¥.")

if __name__ == "__main__":
    main()
