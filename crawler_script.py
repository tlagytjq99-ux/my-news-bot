import feedparser
import csv
import urllib.parse
import time
from datetime import datetime
from googletrans import Translator

def get_config_by_country(country):
    """êµ­ê°€ë³„ êµ¬ê¸€ ë‰´ìŠ¤ ì–¸ì–´(hl) ë° ì§€ì—­(gl) íŒŒë¼ë¯¸í„°"""
    configs = {
        "ëŒ€í•œë¯¼êµ­": {"hl": "ko", "gl": "KR"},
        "ì¼ë³¸": {"hl": "ja", "gl": "JP"},
        "ì¤‘êµ­": {"hl": "zh-CN", "gl": "CN"},
        "ëŒ€ë§Œ": {"hl": "zh-TW", "gl": "TW"},
        "í”„ë‘ìŠ¤": {"hl": "fr", "gl": "FR"},
        "ë…ì¼": {"hl": "de", "gl": "DE"},
        "ì˜¤ìŠ¤íŠ¸ë¦¬ì•„": {"hl": "de", "gl": "AT"},
        "ë„¤ëœë€ë“œ": {"hl": "nl", "gl": "NL"},
        "ë…¸ë¥´ì›¨ì´": {"hl": "no", "gl": "NO"},
        "ìŠ¤ì›¨ë´": {"hl": "sv", "gl": "SE"},
        "ë´ë§ˆí¬": {"hl": "da", "gl": "DK"},
        "í•€ë€ë“œ": {"hl": "fi", "gl": "FI"},
        "ì´ìŠ¤ë¼ì—˜": {"hl": "he", "gl": "IL"},
        "UAE": {"hl": "ar", "gl": "AE"},
        "ì‚¬ìš°ë””": {"hl": "ar", "gl": "SA"}
    }
    return configs.get(country, {"hl": "en-US", "gl": "US"})

def main():
    # ğŸ¯ 50ê°œ ê¸°ê´€ ì „ìˆ˜ ì¡°ì‚¬ ë¦¬ìŠ¤íŠ¸
    gov_agencies = [
        {"êµ­ê°€": "ë¯¸êµ­", "ê¸°ê´€": "ë°±ì•…ê´€", "ë„ë©”ì¸": "whitehouse.gov"},
        {"êµ­ê°€": "ë¯¸êµ­", "ê¸°ê´€": "DOC", "ë„ë©”ì¸": "commerce.gov"},
        {"êµ­ê°€": "ë¯¸êµ­", "ê¸°ê´€": "NTIA", "ë„ë©”ì¸": "ntia.gov"},
        {"êµ­ê°€": "ì¤‘êµ­", "ê¸°ê´€": "CAC", "ë„ë©”ì¸": "cac.gov.cn"},
        {"êµ­ê°€": "ì¤‘êµ­", "ê¸°ê´€": "MIIT", "ë„ë©”ì¸": "miit.gov.cn"},
        {"êµ­ê°€": "ëŒ€í•œë¯¼êµ­", "ê¸°ê´€": "ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€", "ë„ë©”ì¸": "msit.go.kr"},
        {"êµ­ê°€": "ëŒ€í•œë¯¼êµ­", "ê¸°ê´€": "ì‚°ì—…í†µìƒìì›ë¶€", "ë„ë©”ì¸": "motie.go.kr"},
        {"êµ­ê°€": "ì‹±ê°€í¬ë¥´", "ê¸°ê´€": "MDDI", "ë„ë©”ì¸": "mddi.gov.sg"},
        {"êµ­ê°€": "ì‹±ê°€í¬ë¥´", "ê¸°ê´€": "IMDA", "ë„ë©”ì¸": "imda.gov.sg"},
        {"êµ­ê°€": "ë…ì¼", "ê¸°ê´€": "BMDV", "ë„ë©”ì¸": "bmdv.bund.de"},
        {"êµ­ê°€": "ë…ì¼", "ê¸°ê´€": "BMWK", "ë„ë©”ì¸": "bmwk.de"},
        {"êµ­ê°€": "ì¼ë³¸", "ê¸°ê´€": "MIC", "ë„ë©”ì¸": "soumu.go.jp"},
        {"êµ­ê°€": "ì¼ë³¸", "ê¸°ê´€": "ë””ì§€í„¸ì²­", "ë„ë©”ì¸": "digital.go.jp"},
        {"êµ­ê°€": "ì¼ë³¸", "ê¸°ê´€": "METI", "ë„ë©”ì¸": "meti.go.jp"},
        {"êµ­ê°€": "ì˜êµ­", "ê¸°ê´€": "DSIT", "ë„ë©”ì¸": "gov.uk"},
        {"êµ­ê°€": "ë„¤ëœë€ë“œ", "ê¸°ê´€": "EZK", "ë„ë©”ì¸": "government.nl"},
        {"êµ­ê°€": "ë„¤ëœë€ë“œ", "ê¸°ê´€": "Digital", "ë„ë©”ì¸": "nldigitalgovernment.nl"},
        {"êµ­ê°€": "ìŠ¤ì›¨ë´", "ê¸°ê´€": "Finance", "ë„ë©”ì¸": "government.se"},
        {"êµ­ê°€": "í•€ë€ë“œ", "ê¸°ê´€": "LVM", "ë„ë©”ì¸": "lvm.fi"},
        {"êµ­ê°€": "í•€ë€ë“œ", "ê¸°ê´€": "MEE", "ë„ë©”ì¸": "tem.fi"},
        {"êµ­ê°€": "ìŠ¤ìœ„ìŠ¤", "ê¸°ê´€": "OFCOM", "ë„ë©”ì¸": "bakom.admin.ch"},
        {"êµ­ê°€": "ìŠ¤ìœ„ìŠ¤", "ê¸°ê´€": "WBF", "ë„ë©”ì¸": "wbf.admin.ch"},
        {"êµ­ê°€": "ë´ë§ˆí¬", "ê¸°ê´€": "DIGST", "ë„ë©”ì¸": "digst.dk"},
        {"êµ­ê°€": "ë…¸ë¥´ì›¨ì´", "ê¸°ê´€": "KDD", "ë„ë©”ì¸": "regjeringen.no"},
        {"êµ­ê°€": "ì´ìŠ¤ë¼ì—˜", "ê¸°ê´€": "IIA", "ë„ë©”ì¸": "innovationisrael.org.il"},
        {"êµ­ê°€": "ìºë‚˜ë‹¤", "ê¸°ê´€": "ISED", "ë„ë©”ì¸": "ised-isde.canada.ca"},
        {"êµ­ê°€": "í”„ë‘ìŠ¤", "ê¸°ê´€": "Bercy", "ë„ë©”ì¸": "economie.gouv.fr"},
        {"êµ­ê°€": "í”„ë‘ìŠ¤", "ê¸°ê´€": "DGE", "ë„ë©”ì¸": "entreprises.gouv.fr"},
        {"êµ­ê°€": "í˜¸ì£¼", "ê¸°ê´€": "DISR", "ë„ë©”ì¸": "industry.gov.au"},
        {"êµ­ê°€": "ì•„ì¼ëœë“œ", "ê¸°ê´€": "DETE", "ë„ë©”ì¸": "enterprise.gov.ie"},
        {"êµ­ê°€": "ì˜¤ìŠ¤íŠ¸ë¦¬ì•„", "ê¸°ê´€": "BMF", "ë„ë©”ì¸": "bmf.gv.at"},
        {"êµ­ê°€": "ë²¨ê¸°ì—", "ê¸°ê´€": "BIPT", "ë„ë©”ì¸": "bipt.be"},
        {"êµ­ê°€": "ëŒ€ë§Œ", "ê¸°ê´€": "moda", "ë„ë©”ì¸": "moda.gov.tw"},
        {"êµ­ê°€": "ëŒ€ë§Œ", "ê¸°ê´€": "MOEA", "ë„ë©”ì¸": "moea.gov.tw"},
        {"êµ­ê°€": "UAE", "ê¸°ê´€": "TDRA", "ë„ë©”ì¸": "tdra.gov.ae"},
        {"êµ­ê°€": "ì‚¬ìš°ë””", "ê¸°ê´€": "MCIT", "ë„ë©”ì¸": "mcit.gov.sa"}
    ]

    all_final_data = []
    seen_titles = set()
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")
    
    # ì œì™¸ í‚¤ì›Œë“œ ìµœì†Œí™” (ì±„ìš© ë° ë¡œê·¸ì¸ë§Œ ì œì™¸)
    exclude_keywords = ["LOGIN", "SEARCH", "RECRUITMENT", "CONTACT US", "ë¡œê·¸ì¸", "ì±„ìš©", "é‡‡ç”¨"]

    print(f"ğŸ“¡ {collected_date} ê¸€ë¡œë²Œ ì „ìˆ˜ ì¡°ì‚¬ ì—”ì§„ ê°€ë™ (ì´ {len(gov_agencies)}ê°œ ê¸°ê´€)...")

    for agency in gov_agencies:
        config = get_config_by_country(agency['êµ­ê°€'])
        
        # ì¿¼ë¦¬ë¥¼ ê°€ì¥ ë„“ê²Œ ì¡ìŒ (AIë‚˜ ë””ì§€í„¸ì´ í¬í•¨ëœ ëª¨ë“  ì†Œì‹)
        query = f"site:{agency['ë„ë©”ì¸']} (AI OR Artificial Intelligence OR Digital OR ICT)"
        encoded_query = urllib.parse.quote(query)
        
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl={config['hl']}&gl={config['gl']}&ceid={config['gl']}:{config['hl']}"

        try:
            feed = feedparser.parse(rss_url)
            count_before = len(all_final_data)
            
            for entry in feed.entries[:10]: # ê¸°ê´€ë‹¹ ìµœëŒ€ 10ê°œê¹Œì§€ ë„‰ë„‰íˆ í™•ì¸
                raw_title = entry.title.split(' - ')[0].strip()
                
                # ì¤‘ë³µ ë° ìµœì†Œ ë…¸ì´ì¦ˆ ì²´í¬
                if raw_title in seen_titles or any(ex in raw_title.upper() for ex in exclude_keywords):
                    continue

                # ë‚ ì§œ ì¶”ì¶œ (ì‹¤íŒ¨ ì‹œ ì˜¤ëŠ˜ ë‚ ì§œ)
                pub_date = collected_date
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_date = datetime(*entry.published_parsed[:3]).strftime('%Y-%m-%d')

                # ë²ˆì—­ (í˜„ì§€ì–´ -> í•œêµ­ì–´)
                try:
                    title_ko = raw_title if agency['êµ­ê°€'] == "ëŒ€í•œë¯¼êµ­" else translator.translate(raw_title, dest='ko').text
                except:
                    title_ko = raw_title
                
                all_final_data.append({
                    "êµ­ê°€": agency["êµ­ê°€"], "ê¸°ê´€": agency["ê¸°ê´€"], "ë°œí–‰ì¼": pub_date,
                    "ì œëª©": title_ko, "ì›ë¬¸": raw_title, "ë§í¬": entry.link, "ìˆ˜ì§‘ì¼": collected_date
                })
                seen_titles.add(raw_title)
            
            added = len(all_final_data) - count_before
            print(f"âœ… [{agency['êµ­ê°€']}] {agency['ê¸°ê´€']}: {added}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            time.sleep(0.5) # ì†ë„ë¥¼ ìœ„í•´ ë”œë ˆì´ ë‹¨ì¶•

        except Exception as e:
            print(f"âŒ {agency['ê¸°ê´€']} ì—°ê²° ì‹¤íŒ¨: {e}")

    # ìµœì‹ ìˆœ ì •ë ¬
    all_final_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)

    # CSV ì €ì¥
    file_name = f'global_ict_wide_search_{collected_date}.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["êµ­ê°€", "ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
        writer.writeheader()
        writer.writerows(all_final_data)
        
    print(f"\nğŸš€ ì „ì²´ ìˆ˜ì§‘ ì¢…ë£Œ! ì´ {len(all_final_data)}ê±´ì˜ ë°ì´í„°ê°€ '{file_name}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
