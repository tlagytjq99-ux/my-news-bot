import feedparser
import csv
import urllib.parse
import time
from datetime import datetime
from googletrans import Translator
from googlenewsdecoder import gnewsdecoder

def get_config_by_country(country):
    configs = {
        "ëŒ€í•œë¯¼êµ­": {"hl": "ko", "gl": "KR"},
        "ì¼ë³¸": {"hl": "ja", "gl": "JP"},
        "ì¤‘êµ­": {"hl": "zh-CN", "gl": "CN"},
        "ëŒ€ë§Œ": {"hl": "zh-TW", "gl": "TW"},
        "í”„ë‘ìŠ¤": {"hl": "fr", "gl": "FR"},
        "ë…ì¼": {"hl": "de", "gl": "DE"},
        "ë„¤ëœë€ë“œ": {"hl": "nl", "gl": "NL"},
        "í•€ë€ë“œ": {"hl": "fi", "gl": "FI"},
        "ì´ìŠ¤ë¼ì—˜": {"hl": "he", "gl": "IL"},
        "UAE": {"hl": "ar", "gl": "AE"},
        "ì‚¬ìš°ë””": {"hl": "ar", "gl": "SA"}
    }
    return configs.get(country, {"hl": "en-US", "gl": "US"})

def main():
    # ğŸ¯ 50ê°œ ê¸°ê´€ ë¦¬ìŠ¤íŠ¸ (ì „ì²´ í¬í•¨ í•„ìˆ˜)
    gov_agencies = [
        {"êµ­ê°€": "ë¯¸êµ­", "ê¸°ê´€": "ë°±ì•…ê´€", "ë„ë©”ì¸": "whitehouse.gov"},
        {"êµ­ê°€": "ë¯¸êµ­", "ê¸°ê´€": "DOC", "ë„ë©”ì¸": "commerce.gov"},
        {"êµ­ê°€": "ë¯¸êµ­", "ê¸°ê´€": "NTIA", "ë„ë©”ì¸": "ntia.gov"},
        {"êµ­ê°€": "ì¤‘êµ­", "ê¸°ê´€": "CAC", "ë„ë©”ì¸": "cac.gov.cn"},
        {"êµ­ê°€": "ì¤‘êµ­", "ê¸°ê´€": "MIIT", "ë„ë©”ì¸": "miit.gov.cn"},
        {"êµ­ê°€": "ëŒ€í•œë¯¼êµ­", "ê¸°ê´€": "ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€", "ë„ë©”ì¸": "msit.go.kr"},
        {"êµ­ê°€": "ëŒ€í•œë¯¼êµ­", "ê¸°ê´€": "ì‚°ì—…í†µìƒìì›ë¶€", "ë„ë©”ì¸": "motie.go.kr"},
        {"êµ­ê°€": "ì‹±ê°€í¬ë¥´", "ê¸°ê´€": "MDDI", "ë„ë©”ì¸": "mddi.gov.sg"},
        {"êµ­ê°€": "ì‹±ê°€í¬ë¥´", "ê¸°ê´€": "IMDA", "ë„ë©”ì¸": "imda.gov.sg"},
        {"êµ­ê°€": "ë…ì¼", "ê¸°ê´€": "BMDV", "ë„ë©”ì¸": "bmdv.bund.de"},
        {"êµ­ê°€": "ë…ì¼", "ê¸°ê´€": "BMWK", "ë„ë©”ì¸": "bmwk.de"},
        {"êµ­ê°€": "ì¼ë³¸", "ê¸°ê´€": "MIC", "ë„ë©”ì¸": "soumu.go.jp"},
        {"êµ­ê°€": "ì¼ë³¸", "ê¸°ê´€": "ë””ì§€í„¸ì²­", "ë„ë©”ì¸": "digital.go.jp"},
        {"êµ­ê°€": "ì¼ë³¸", "ê¸°ê´€": "METI", "ë„ë©”ì¸": "meti.go.jp"},
        {"êµ­ê°€": "ì˜êµ­", "ê¸°ê´€": "DSIT", "ë„ë©”ì¸": "gov.uk"},
        {"êµ­ê°€": "ë„¤ëœë€ë“œ", "ê¸°ê´€": "EZK", "ë„ë©”ì¸": "government.nl"},
        {"êµ­ê°€": "ìŠ¤ì›¨ë´", "ê¸°ê´€": "Finance", "ë„ë©”ì¸": "government.se"},
        {"êµ­ê°€": "í•€ë€ë“œ", "ê¸°ê´€": "LVM", "ë„ë©”ì¸": "lvm.fi"},
        {"êµ­ê°€": "í•€ë€ë“œ", "ê¸°ê´€": "MEE", "ë„ë©”ì¸": "tem.fi"},
        {"êµ­ê°€": "ìŠ¤ìœ„ìŠ¤", "ê¸°ê´€": "OFCOM", "ë„ë©”ì¸": "bakom.admin.ch"},
        {"êµ­ê°€": "ë´ë§ˆí¬", "ê¸°ê´€": "DIGST", "ë„ë©”ì¸": "digst.dk"},
        {"êµ­ê°€": "ë…¸ë¥´ì›¨ì´", "ê¸°ê´€": "KDD", "ë„ë©”ì¸": "regjeringen.no"},
        {"êµ­ê°€": "ì´ìŠ¤ë¼ì—˜", "ê¸°ê´€": "IIA", "ë„ë©”ì¸": "innovationisrael.org.il"},
        {"êµ­ê°€": "ìºë‚˜ë‹¤", "ê¸°ê´€": "ISED", "ë„ë©”ì¸": "ised-isde.canada.ca"},
        {"êµ­ê°€": "í”„ë‘ìŠ¤", "ê¸°ê´€": "Bercy", "ë„ë©”ì¸": "economie.gouv.fr"},
        {"êµ­ê°€": "í˜¸ì£¼", "ê¸°ê´€": "DISR", "ë„ë©”ì¸": "industry.gov.au"},
        {"êµ­ê°€": "ëŒ€ë§Œ", "ê¸°ê´€": "moda", "ë„ë©”ì¸": "moda.gov.tw"},
        {"êµ­ê°€": "UAE", "ê¸°ê´€": "TDRA", "ë„ë©”ì¸": "tdra.gov.ae"},
        {"êµ­ê°€": "ì‚¬ìš°ë””", "ê¸°ê´€": "MCIT", "ë„ë©”ì¸": "mcit.gov.sa"}
    ]

    all_final_data = []
    seen_titles = set()
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")
    
    # ğŸš« ë…¸ì´ì¦ˆ ì°¨ë‹¨ ëª©ë¡ (ê°•í™”)
    exclude_keywords = [
        "ê²Œì‹œíŒ ì¸ì‡„", "ë¡œê·¸ì¸", "LOGIN", "SEARCH", "RECRUITMENT", "ì±„ìš©", "æ¡ç”¨", 
        "CONTACT US", "ABOUT US", "í™ˆí˜ì´ì§€", "HOME", "FAQ", "Q&A", "FORM", 
        "ë¹„ë°€ë²ˆí˜¸", "PASSWORD", "SIGN IN", "SIGN UP", "OFFICIAL SITE"
    ]

    # âœ… í•„ìˆ˜ ê¸°ìˆ  í‚¤ì›Œë“œ (ì´ ë‹¨ì–´ë“¤ì´ ìˆì–´ì•¼ ì •ì±…ìœ¼ë¡œ ê°„ì£¼)
    must_include = ["AI", "ì¸ê³µì§€ëŠ¥", "ë””ì§€í„¸", "DIGITAL", "ICT", "DATA", "ë°ì´í„°", "POLICY", "ì •ì±…", "STRATEGY", "ì „ëµ"]

    print(f"ğŸ“¡ {collected_date} ê¸°ê´€ë³„ ì •ë ¬ ë° í•„í„°ë§ ìˆ˜ì§‘ ê°€ë™...")

    for agency in gov_agencies:
        config = get_config_by_country(agency['êµ­ê°€'])
        query = f"site:{agency['ë„ë©”ì¸']} (AI OR Digital OR ICT)"
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl={config['hl']}&gl={config['gl']}&ceid={config['gl']}:{config['hl']}"

        try:
            feed = feedparser.parse(rss_url)
            collected_count = 0
            
            for entry in feed.entries:
                if collected_count >= 2: break 
                
                raw_title = entry.title.split(' - ')[0].strip()
                upper_title = raw_title.upper()
                
                # [í•„í„°] ì¤‘ë³µ, ë…¸ì´ì¦ˆ, í‚¤ì›Œë“œ ë¯¸í¬í•¨ ì‹œ íŒ¨ìŠ¤
                if raw_title in seen_titles: continue
                if any(ex in upper_title for ex in exclude_keywords): continue
                if not any(must in upper_title for must in must_include): continue

                # [í•„í„°] ë‚ ì§œ (2024ë…„ ì´í›„ë§Œ)
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_year = entry.published_parsed[0]
                    if pub_year < 2024: continue
                    pub_date = datetime(*entry.published_parsed[:3]).strftime('%Y-%m-%d')
                else: continue

                # ì›ë³¸ ë§í¬ ë””ì½”ë”©
                try:
                    decoded = gnewsdecoder(entry.link)
                    actual_link = decoded.get('decoded_url', entry.link)
                except: actual_link = entry.link

                # ë²ˆì—­
                try:
                    title_ko = raw_title if agency['êµ­ê°€'] == "ëŒ€í•œë¯¼êµ­" else translator.translate(raw_title, dest='ko').text
                except: title_ko = raw_title
                
                all_final_data.append({
                    "êµ­ê°€": agency["êµ­ê°€"], "ê¸°ê´€": agency["ê¸°ê´€"], "ë°œí–‰ì¼": pub_date,
                    "ì œëª©": title_ko, "ì›ë¬¸": raw_title, "ë§í¬": actual_link, "ìˆ˜ì§‘ì¼": collected_date
                })
                seen_titles.add(raw_title)
                collected_count += 1
            
            print(f"âœ… [{agency['êµ­ê°€']}] {agency['ê¸°ê´€']} ì™„ë£Œ")
            time.sleep(0.5)
        except: continue

    # ğŸ—‚ï¸ í•µì‹¬ ìˆ˜ì •: êµ­ê°€ë³„ -> ê¸°ê´€ë³„ ê°€ë‚˜ë‹¤ìˆœ ì •ë ¬
    all_final_data.sort(key=lambda x: (x['êµ­ê°€'], x['ê¸°ê´€'], x['ë°œí–‰ì¼']), reverse=False)

    file_name = f'global_ict_report_sorted_{collected_date}.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["êµ­ê°€", "ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
        writer.writeheader()
        writer.writerows(all_final_data)
        
    print(f"\nğŸš€ ì •ë ¬ ì™„ë£Œ! '{file_name}' íŒŒì¼ì„ í™•ì¸í•´ ë³´ì„¸ìš”.")

if __name__ == "__main__":
    main()
