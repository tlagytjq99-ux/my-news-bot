import feedparser
import csv
import urllib.parse
import time
from datetime import datetime
from googletrans import Translator
from googlenewsdecoder import gnewsdecoder

def is_garbage(text):
    """ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆ(ê°€ë¹„ì§€)ë¥¼ ê±¸ëŸ¬ë‚´ëŠ” í•„í„°"""
    t = text.upper()
    # âŒ ìˆ˜ì§‘ì—ì„œ ì œì™¸í•  í‚¤ì›Œë“œ (ë¬¸í™”, ì±„ìš©, ì¼ë°˜ ì™¸êµ, ì¤‘ë… ë“±)
    negative_keywords = [
        "CULTURE", "CULTURAL", "ADDICTION", "RECOVERY", "NEWSLETTER", "SPEECH", 
        "FOREIGN POLICY", "VACANCY", "RECRUITMENT", "ë¬¸í™”", "ì¤‘ë…", "ì±„ìš©", "ë‰´ìŠ¤ë ˆí„°",
        "ì—°ì„¤", "ì™¸êµ", "í†µí™”", "CLIMATE", "ê¸°í›„", "RAMADAN", "ë¼ë§ˆë‹¨", "ANNIVERSARY"
    ]
    return any(neg in t for neg in negative_keywords)

def classify_ict_refined(text):
    """13ëŒ€ ì •ë°€ ë¶„ë¥˜ ë¡œì§"""
    t = text.upper()
    categories = {
        "1-1. ì¸í”„ë¼ ë° ë„¤íŠ¸ì›Œí¬": ["6G", "5G", "CLOUD", "NETWORK", "ë„¤íŠ¸ì›Œí¬", "ë§", "ì£¼íŒŒìˆ˜"],
        "1-2. ì§€ëŠ¥í˜• í”Œë«í¼ ë° ë°ì´í„°": ["GENERATIVE AI", "LLM", "BIG DATA", "GEN AI", "ë°ì´í„°", "ì§€ëŠ¥í˜•"],
        "1-3. ì‚°ì—… ìœµí•© ë° ë¯¸ë˜ ê¸°ìˆ ": ["ROBOT", "DIGITAL TWIN", "ë¡œë´‡", "ì–‘ì", "QUANTUM", "ë“œë¡ "],
        "2-1. IT ì†”ë£¨ì…˜ ë° ì„œë¹„ìŠ¤": ["SAAS", "B2B", "SOFTWARE", "ì†Œí”„íŠ¸ì›¨ì–´", "ì†”ë£¨ì…˜"],
        "2-2. í†µì‹  ì¸í”„ë¼ ë° ë‹¨ë§ê¸°": ["TELECOM", "SMARTPHONE", "í†µì‹ ", "ë‹¨ë§", "ê¸°ê¸°"],
        "2-3. ì •ì±… ë° ê±°ë²„ë„ŒìŠ¤": ["REGULATION", "AI ACT", "PRIVACY", "ê·œì œ", "ì •ì±…", "ë²•ì•ˆ", "ê±°ë²„ë„ŒìŠ¤"],
        "3-1. ì—”í„°í…Œì¸ë¨¼íŠ¸ ë° í”Œë«í¼": ["OTT", "STREAMING", "WEBTOON", "ì½˜í…ì¸ ", "í”Œë«í¼", "MEDIA"],
        "3-2. ê´‘ê³  ë° êµìœ¡": ["ADTECH", "EDTECH", "LMS", "êµìœ¡", "ê´‘ê³ "],
        "3-3. í”Œë«í¼ ë° ê¶Œë¦¬": ["COPYRIGHT", "NFT", "ì €ì‘ê¶Œ", "ì§€ì‹ì¬ì‚°", "IP"],
        "4-1. ì´ë™ìˆ˜ë‹¨ ë° í•­ê³µ": ["EV", "UAM", "AUTONOMOUS", "ììœ¨ì£¼í–‰", "ëª¨ë¹Œë¦¬í‹°"],
        "4-2. ì—ë„ˆì§€ ë° ìì›": ["SMART GRID", "RENEWABLE", "ì—ë„ˆì§€", "ê·¸ë¦¬ë“œ", "ì§€ì†ê°€ëŠ¥"],
        "4-3. ì œì¡° ë° ê¸°ê³„": ["FACTORY", "IOT", "ì œì¡°", "ê³µì¥", "SEMICONDUCTOR", "ë°˜ë„ì²´"],
        "4-4. ìƒëª…ê³¼í•™ ë° ì†Œë¹„ì¬": ["HEALTH", "BIO", "í—¬ìŠ¤ì¼€ì–´", "ë°”ì´ì˜¤", "DIGITAL HEALTH"]
    }
    for cat, keywords in categories.items():
        if any(kw in t for kw in keywords): return cat
    return "ê¸°íƒ€ ICT ì¼ë°˜"

def main():
    # 50ê°œ ì£¼ìš” ê¸°ê´€ ë¦¬ìŠ¤íŠ¸ (ë™ì¼)
    gov_agencies = [
        {"êµ­ê°€": "ë¯¸êµ­", "ê¸°ê´€": "ë°±ì•…ê´€", "ë„ë©”ì¸": "whitehouse.gov"}, {"êµ­ê°€": "ë¯¸êµ­", "ê¸°ê´€": "DOC", "ë„ë©”ì¸": "commerce.gov"},
        {"êµ­ê°€": "ë¯¸êµ­", "ê¸°ê´€": "NTIA", "ë„ë©”ì¸": "ntia.gov"}, {"êµ­ê°€": "ì¤‘êµ­", "ê¸°ê´€": "CAC", "ë„ë©”ì¸": "cac.gov.cn"},
        {"êµ­ê°€": "ì¤‘êµ­", "ê¸°ê´€": "MIIT", "ë„ë©”ì¸": "miit.gov.cn"}, {"êµ­ê°€": "ëŒ€í•œë¯¼êµ­", "ê¸°ê´€": "ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€", "ë„ë©”ì¸": "msit.go.kr"},
        {"êµ­ê°€": "ëŒ€í•œë¯¼êµ­", "ê¸°ê´€": "ì‚°ì—…í†µìƒìì›ë¶€", "ë„ë©”ì¸": "motie.go.kr"}, {"êµ­ê°€": "ì‹±ê°€í¬ë¥´", "ê¸°ê´€": "MDDI", "ë„ë©”ì¸": "mddi.gov.sg"},
        {"êµ­ê°€": "ì‹±ê°€í¬ë¥´", "ê¸°ê´€": "IMDA", "ë„ë©”ì¸": "imda.gov.sg"}, {"êµ­ê°€": "ë…ì¼", "ê¸°ê´€": "BMDV", "ë„ë©”ì¸": "bmdv.bund.de"},
        {"êµ­ê°€": "ë…ì¼", "ê¸°ê´€": "BMWK", "ë„ë©”ì¸": "bmwk.de"}, {"êµ­ê°€": "ì¼ë³¸", "ê¸°ê´€": "MIC", "ë„ë©”ì¸": "soumu.go.jp"},
        {"êµ­ê°€": "ì¼ë³¸", "ê¸°ê´€": "ë””ì§€í„¸ì²­", "ë„ë©”ì¸": "digital.go.jp"}, {"êµ­ê°€": "ì¼ë³¸", "ê¸°ê´€": "METI", "ë„ë©”ì¸": "meti.go.jp"},
        {"êµ­ê°€": "ì˜êµ­", "ê¸°ê´€": "DSIT", "ë„ë©”ì¸": "gov.uk"}, {"êµ­ê°€": "ì˜êµ­", "ê¸°ê´€": "DBT", "ë„ë©”ì¸": "gov.uk"},
        {"êµ­ê°€": "ë„¤ëœë€ë“œ", "ê¸°ê´€": "EZK", "ë„ë©”ì¸": "government.nl"}, {"êµ­ê°€": "ë„¤ëœë€ë“œ", "ê¸°ê´€": "Digitalisation", "ë„ë©”ì¸": "nldigitalgovernment.nl"},
        {"êµ­ê°€": "ìŠ¤ì›¨ë´", "ê¸°ê´€": "Finance", "ë„ë©”ì¸": "government.se"}, {"êµ­ê°€": "ìŠ¤ì›¨ë´", "ê¸°ê´€": "Enterprise", "ë„ë©”ì¸": "government.se"},
        {"êµ­ê°€": "í•€ë€ë“œ", "ê¸°ê´€": "LVM", "ë„ë©”ì¸": "lvm.fi"}, {"êµ­ê°€": "í•€ë€ë“œ", "ê¸°ê´€": "MEE", "ë„ë©”ì¸": "tem.fi"},
        {"êµ­ê°€": "ìŠ¤ìœ„ìŠ¤", "ê¸°ê´€": "OFCOM", "ë„ë©”ì¸": "bakom.admin.ch"}, {"êµ­ê°€": "ìŠ¤ìœ„ìŠ¤", "ê¸°ê´€": "WBF", "ë„ë©”ì¸": "wbf.admin.ch"},
        {"êµ­ê°€": "ë´ë§ˆí¬", "ê¸°ê´€": "Digitaliseringsstyrelsen", "ë„ë©”ì¸": "digst.dk"}, {"êµ­ê°€": "ë´ë§ˆí¬", "ê¸°ê´€": "Erhvervsministeriet", "ë„ë©”ì¸": "em.dk"},
        {"êµ­ê°€": "ë…¸ë¥´ì›¨ì´", "ê¸°ê´€": "KDD", "ë„ë©”ì¸": "regjeringen.no"}, {"êµ­ê°€": "ë…¸ë¥´ì›¨ì´", "ê¸°ê´€": "NFD", "ë„ë©”ì¸": "regjeringen.no"},
        {"êµ­ê°€": "ì´ìŠ¤ë¼ì—˜", "ê¸°ê´€": "IIA", "ë„ë©”ì¸": "innovationisrael.org.il"}, {"êµ­ê°€": "ì´ìŠ¤ë¼ì—˜", "ê¸°ê´€": "MoC", "ë„ë©”ì¸": "gov.il"},
        {"êµ­ê°€": "ì´ìŠ¤ë¼ì—˜", "ê¸°ê´€": "Economy", "ë„ë©”ì¸": "gov.il"}, {"êµ­ê°€": "ìºë‚˜ë‹¤", "ê¸°ê´€": "ISED", "ë„ë©”ì¸": "ised-isde.canada.ca"},
        {"êµ­ê°€": "ìºë‚˜ë‹¤", "ê¸°ê´€": "TBS", "ë„ë©”ì¸": "canada.ca"}, {"êµ­ê°€": "í”„ë‘ìŠ¤", "ê¸°ê´€": "Bercy", "ë„ë©”ì¸": "economie.gouv.fr"},
        {"êµ­ê°€": "í”„ë‘ìŠ¤", "ê¸°ê´€": "DG Entreprises", "ë„ë©”ì¸": "entreprises.gouv.fr"}, {"êµ­ê°€": "í˜¸ì£¼", "ê¸°ê´€": "DITRDCA", "ë„ë©”ì¸": "infrastructure.gov.au"},
        {"êµ­ê°€": "í˜¸ì£¼", "ê¸°ê´€": "DISR", "ë„ë©”ì¸": "industry.gov.au"}, {"êµ­ê°€": "ì•„ì¼ëœë“œ", "ê¸°ê´€": "DECC", "ë„ë©”ì¸": "gov.ie"},
        {"êµ­ê°€": "ì•„ì¼ëœë“œ", "ê¸°ê´€": "DETE", "ë„ë©”ì¸": "enterprise.gov.ie"}, {"êµ­ê°€": "ì˜¤ìŠ¤íŠ¸ë¦¬ì•„", "ê¸°ê´€": "BMF", "ë„ë©”ì¸": "bmf.gv.at"},
        {"êµ­ê°€": "ì˜¤ìŠ¤íŠ¸ë¦¬ì•„", "ê¸°ê´€": "BMAW", "ë„ë©”ì¸": "bmaw.gv.at"}, {"êµ­ê°€": "ë²¨ê¸°ì—", "ê¸°ê´€": "ì—°ë°©í˜ì‹ ê¸°ìˆ ë¶€", "ë„ë©”ì¸": "belspo.be"},
        {"êµ­ê°€": "ë²¨ê¸°ì—", "ê¸°ê´€": "BIPT", "ë„ë©”ì¸": "bipt.be"}, {"êµ­ê°€": "ë²¨ê¸°ì—", "ê¸°ê´€": "FPS Economy", "ë„ë©”ì¸": "economie.fgov.be"},
        {"êµ­ê°€": "ëŒ€ë§Œ", "ê¸°ê´€": "moda", "ë„ë©”ì¸": "moda.gov.tw"}, {"êµ­ê°€": "ëŒ€ë§Œ", "ê¸°ê´€": "MOEA", "ë„ë©”ì¸": "moea.gov.tw"},
        {"êµ­ê°€": "UAE", "ê¸°ê´€": "TDRA", "ë„ë©”ì¸": "tdra.gov.ae"}, {"êµ­ê°€": "UAE", "ê¸°ê´€": "MoIAT", "ë„ë©”ì¸": "moiat.gov.ae"},
        {"êµ­ê°€": "ì‚¬ìš°ë””", "ê¸°ê´€": "MCIT", "ë„ë©”ì¸": "mcit.gov.sa"}, {"êµ­ê°€": "ì‚¬ìš°ë””", "ê¸°ê´€": "MIM", "ë„ë©”ì¸": "mim.gov.sa"}
    ]

    all_final_data = []
    seen_titles = set()
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")
    
    # âœ… ì‹¤ì§ˆì  ê¸°ìˆ ì–´ê°€ í¬í•¨ë˜ì–´ì•¼ë§Œ ìˆ˜ì§‘ (Positive Filter)
    must_have_tech = ["AI", "DIGITAL", "ICT", "SEMICONDUCTOR", "6G", "5G", "QUANTUM", "CYBER", "ROBOT", "UAM", "PLATFORM", "ë°ì´í„°", "ë°˜ë„ì²´", "ì¸ê³µì§€ëŠ¥", "ê·œì œ"]

    print(f"ğŸ“¡ {collected_date} ê°€ë¹„ì§€ ì œê±° ëª¨ë“œ ê°€ë™ (ê¸°ê´€ë‹¹ í•µì‹¬ 1ê±´)...")

    for agency in gov_agencies:
        query = f"site:{agency['ë„ë©”ì¸']} (AI OR Digital OR ICT OR Technology)"
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en&gl=US"

        try:
            feed = feedparser.parse(rss_url)
            collected_count = 0
            for entry in feed.entries:
                if collected_count >= 1: break # ê¸°ê´€ë‹¹ ë”± 1ê°œ

                raw_title = entry.title.split(' - ')[0].strip()
                if raw_title in seen_titles: continue
                if not (hasattr(entry, 'published_parsed') and entry.published_parsed[0] >= 2024): continue
                
                # ê°€ë¹„ì§€ í•„í„°ë§ (ë¶€ì • í‚¤ì›Œë“œ ì œê±°) ğŸš€
                if is_garbage(raw_title): continue

                # ê¸°ìˆ  ë°€ë„ ì²´í¬ (í•µì‹¬ ê¸°ìˆ ì–´ í•„ìˆ˜ í¬í•¨) ğŸš€
                if not any(tech in raw_title.upper() for tech in must_have_tech): continue

                pub_date = datetime(*entry.published_parsed[:3]).strftime('%Y-%m-%d')
                try:
                    title_ko = raw_title if agency['êµ­ê°€'] == "ëŒ€í•œë¯¼êµ­" else translator.translate(raw_title, dest='ko').text
                except: title_ko = raw_title

                category = classify_ict_refined(title_ko + " " + raw_title)
                try:
                    decoded = gnewsdecoder(entry.link)
                    actual_link = decoded.get('decoded_url', entry.link)
                except: actual_link = entry.link

                all_final_data.append({
                    "êµ­ê°€": agency["êµ­ê°€"], "ê¸°ê´€": agency["ê¸°ê´€"], "ICT ë¶„ë¥˜": category,
                    "ë°œí–‰ì¼": pub_date, "ì œëª©": title_ko, "ì›ë¬¸": raw_title, "ë§í¬": actual_link, "ìˆ˜ì§‘ì¼": collected_date
                })
                seen_titles.add(raw_title)
                collected_count += 1
            
            print(f"âœ… [{agency['êµ­ê°€']}] {agency['ê¸°ê´€']} ì™„ë£Œ")
            time.sleep(0.3)
        except: continue

    file_name = f'Global_ICT_Clean_Report_{collected_date}.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["êµ­ê°€", "ê¸°ê´€", "ICT ë¶„ë¥˜", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
        writer.writeheader()
        writer.writerows(all_final_data)
        
    print(f"\nğŸš€ ì‘ì—… ì™„ë£Œ! ì •ì œëœ ë¦¬í¬íŠ¸ê°€ '{file_name}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
