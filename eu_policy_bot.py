import requests
from bs4 import BeautifulSoup
import csv
import time
import random

def fetch_eu_safe_scraping():
    # 1. 2025ë…„ ê²€ìƒ‰ ê²°ê³¼ ì£¼ì†Œ
    url = "https://op.europa.eu/en/search-results"
    params = {
        "p_p_id": "eu_europa_publications_portlet_facet_search_result_FacetedSearchResultPortlet_INSTANCE_TTTP7nyqSt8X",
        "p_p_lifecycle": "0",
        "facet.documentYear": "2025",
        "facet.collection": "EUPub",
        "resultsPerPage": "20" # í•œ ë²ˆì— ë„ˆë¬´ ë§ì´ ê°€ì ¸ì˜¤ë©´ ì˜ì‹¬ë°›ìœ¼ë‹ˆ ì ë‹¹íˆ!
    }
    
    # [í•µì‹¬] ì„œë²„ë¥¼ ì†ì´ëŠ” 'ë³€ì¥ ë„êµ¬' (ë¸Œë¼ìš°ì € ì •ë³´ ì¶”ê°€)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": "https://op.europa.eu/en/home" # ì´ì „ í˜ì´ì§€ì—ì„œ ì˜¨ ê²ƒì²˜ëŸ¼ ì†ì„
    }

    print("ğŸ›¡ï¸ ë³´ì•ˆ ëª¨ë“œë¡œ EU í¬í„¸ì— ì ‘ê·¼í•©ë‹ˆë‹¤. (ì°¨ë‹¨ ë°©ì§€ ë¡œì§ ê°€ë™)", flush=True)
    
    file_name = 'EU_Policy_2025_Full.csv'
    collected_data = []

    try:
        # ì‚¬ëŒì²˜ëŸ¼ í–‰ë™í•˜ê¸° ìœ„í•´ 1~3ì´ˆ ëœë¤ ëŒ€ê¸°
        time.sleep(random.uniform(1.0, 3.0))
        
        response = requests.get(url, params=params, headers=headers, timeout=30)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # EU í¬í„¸ì˜ ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ êµ¬ì¡° (í´ë˜ìŠ¤ëª… íƒ€ê²ŸíŒ…)
            items = soup.find_all('div', class_='search-result-item')
            
            for item in items:
                title_tag = item.find('h4').find('a') if item.find('h4') else None
                if title_tag:
                    title = title_tag.get_text(strip=True)
                    link = title_tag['href']
                    # ë‚ ì§œ ì¶”ì¶œ (metadata-value í´ë˜ìŠ¤ ì‚¬ìš©)
                    date_tag = item.find('span', class_='metadata-value')
                    date = date_tag.get_text(strip=True) if date_tag else "2025"
                    
                    collected_data.append({
                        "date": date,
                        "title": title,
                        "link": link if link.startswith('http') else f"https://op.europa.eu{link}"
                    })

            print(f"âœ… ì„±ê³µ! ë³´ì•ˆì„ ìœ ì§€í•˜ë©° {len(collected_data)}ê±´ì˜ ë°ì´í„°ë¥¼ ìºëƒˆìŠµë‹ˆë‹¤.", flush=True)
        else:
            print(f"âš ï¸ ì ‘ê·¼ ê±°ë¶€ (ì½”ë“œ: {response.status_code}). ë³´ì•ˆ ìˆ˜ì¤€ì„ ë†’ì—¬ì•¼ í•©ë‹ˆë‹¤.", flush=True)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", flush=True)

    # íŒŒì¼ ì €ì¥ (ì´ê²Œ ë˜ì–´ì•¼ 128 ì—ëŸ¬ê°€ ì•ˆ ë‚©ë‹ˆë‹¤)
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
        writer.writeheader()
        if collected_data:
            writer.writerows(collected_data)
        else:
            writer.writerow({"date": "2025-02-09", "title": "Security Check OK - No data in list", "link": "N/A"})

if __name__ == "__main__":
    fetch_eu_safe_scraping()
