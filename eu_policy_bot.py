import requests
from bs4 import BeautifulSoup
import csv
import re

def crawl_eu_2025_news_v2():
    # 2025ë…„ í•„í„°ê°€ ì ìš©ëœ URL
    target_url = "https://european-union.europa.eu/news-and-events/news-and-stories_en?f%5B0%5D=oe_news_publication_date%3Abt%7C2025-01-01T02%3A12%3A07%2B01%3A00%7C2025-12-31T02%3A12%3A07%2B01%3A00"
    file_name = 'EU_News_2025_Final.csv'
    
    # ë¸Œë¼ìš°ì €ì¸ ì²™ ì†ì´ê¸° ìœ„í•œ ê°•ë ¥í•œ í—¤ë”
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'en-US,en;q=0.9'
    }

    print("ğŸš€ [2025 ì •ë°€ ì‚¬ëƒ¥] ì´ë²ˆì—ëŠ” ë†“ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìŠ¤ìº” ì‹œì‘...", flush=True)

    try:
        response = requests.get(target_url, headers=headers, timeout=30)
        soup = BeautifulSoup(response.text, 'html.parser')

        # 1. ëª¨ë“  ë§í¬ ì¤‘ì—ì„œ '/news/'ê°€ í¬í•¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ë§Œ í•„í„°ë§
        # EU ë‰´ìŠ¤ëŠ” ì£¼ì†Œì— ë°˜ë“œì‹œ '/news/'ê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤.
        news_links = soup.find_all('a', href=re.compile(r'/news/'))
        
        final_news = []
        seen_links = set()

        for link_tag in news_links:
            url = link_tag['href']
            # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            full_url = url if url.startswith('http') else "https://european-union.europa.eu" + url
            
            # ì œëª© ì¶”ì¶œ (ì´ë¯¸ì§€ ë§í¬ ë“±ì€ ì œì™¸í•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
            title = link_tag.get_text(strip=True)
            
            # ì¤‘ë³µ ì œê±° ë° ì§§ì€ ì œëª©(ë”ë³´ê¸° ë“±) í•„í„°ë§
            if full_url not in seen_links and len(title) > 20:
                final_news.append({
                    "date": "2025",
                    "title": title,
                    "link": full_url
                })
                seen_links.add(full_url)

        if final_news:
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                writer.writeheader()
                writer.writerows(final_news)
            
            print("\n" + "ğŸ†"*20)
            print(f"2025ë…„ ë°ì´í„° ì •ë³µ ì„±ê³µ! ì´ {len(final_news)}ê±´ í™•ë³´")
            print(f"ì €ì¥ëœ íŒŒì¼: {file_name}")
            print("ğŸ†"*20)
            for i, item in enumerate(final_news[:5], 1):
                print(f"{i}. {item['title']}")
                print(f"   ğŸ”— {item['link']}\n")
        else:
            # ì‹¤íŒ¨ ì‹œ ì†ŒìŠ¤ ì½”ë“œ ì¼ë¶€ë¥¼ ì¶œë ¥í•˜ì—¬ ì œê°€ ë¶„ì„í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
            print("âš ï¸ ì—¬ì „íˆ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì†ŒìŠ¤ ë¶„ì„ì„ ìœ„í•´ ì¼ë¶€ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.")
            print(f"ê²€ìƒ‰ëœ ì´ ë§í¬ ìˆ˜: {len(soup.find_all('a'))}")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    crawl_eu_2025_news_v2()
