import asyncio
from playwright.async_api import async_playwright
import csv

async def crawl_gartner_2026_top10():
    # 2026ë…„ ìµœì‹  ë‰´ìŠ¤ê°€ ëª¨ì—¬ìˆëŠ” ë©”ì¸ í˜ì´ì§€
    url = "https://www.gartner.com/en/newsroom"
    file_name = 'Gartner_Insight_Archive.csv'
    all_data = []

    async with async_playwright() as p:
        # ê°€íŠ¸ë„ˆê°€ ì˜ì‹¬í•˜ì§€ ëª»í•˜ê²Œ 'ìœ ì € ë°ì´í„°'ë¥¼ ë” ì •êµí•˜ê²Œ ìœ„ì¥
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            viewport={'width': 1280, 'height': 800}
        )
        page = await context.new_page()

        print(f"ğŸ¯ 2026 ê°€íŠ¸ë„ˆ ìµœì‹  ìë£Œ ìˆ˜ì§‘ ì‹œì‘ (íƒ€ê²Ÿ: ë©”ì¸ í˜ì´ì§€)")
        
        try:
            # ì ‘ì† (ë„¤íŠ¸ì›Œí¬ê°€ ì¡°ìš©í•´ì§ˆ ë•Œê¹Œì§€ ë„‰ë„‰íˆ ëŒ€ê¸°)
            await page.goto(url, wait_until="networkidle", timeout=60000)
            await asyncio.sleep(5) 

            # í™”ë©´ì„ ì¡°ê¸ˆì”© ë‚´ë ¤ì„œ ìˆ¨ê²¨ì§„ ë‰´ìŠ¤ ì¹´ë“œê°€ ë‚˜íƒ€ë‚˜ê²Œ í•¨ (Lazy Loading ëŒ€ì‘)
            for _ in range(3):
                await page.evaluate("window.scrollBy(0, 500)")
                await asyncio.sleep(1)

            # ëª¨ë“  ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ
            page_data = await page.evaluate("""
                () => {
                    const results = [];
                    // ê°€íŠ¸ë„ˆ ë‰´ìŠ¤ë£¸ ë§í¬ íŒ¨í„´
                    const links = document.querySelectorAll('a[href*="/newsroom/press-releases/"]');
                    
                    // ìµœëŒ€ 20ê°œê¹Œì§€ë§Œ ìˆ˜ì§‘ (ì•ˆì „ì„± í™•ë³´)
                    const limit = Math.min(links.length, 20);
                    
                    for(let i=0; i < limit; i++) {
                        const a = links[i];
                        const text = a.innerText.trim();
                        if (text.length > 10) {
                            results.push({
                                date: "2026-Recent",
                                title: text.replace(/\\n/g, ' '),
                                link: a.href
                            });
                        }
                    }
                    return results;
                }
            """)
            
            all_data = page_data
            print(f"âœ… ìµœì‹  ê¸°ì‚¬ {len(all_data)}ê±´ ë°œê²¬!")

        except Exception as e:
            print(f"âŒ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

        await browser.close()

    # ê²°ê³¼ ì €ì¥
    if all_data:
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
            writer.writeheader()
            writer.writerows(all_data)
        print(f"âœ¨ ìˆ˜ì§‘ ì™„ë£Œ! {file_name} í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤.")
    else:
        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¹ƒí—ˆë¸Œ ì•¡ì…˜ì´ ì—ëŸ¬ë‚˜ë¯€ë¡œ ë¹ˆ íŒŒì¼ ìƒì„±
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            f.write("date,title,link\\n")
        print("ğŸš¨ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨. ê°€íŠ¸ë„ˆ ë³´ì•ˆì´ ë§¤ìš° ê°•ë ¥í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(crawl_gartner_2026_top10())
