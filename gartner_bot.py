import requests
import xml.etree.ElementTree as ET
import csv
from urllib.parse import quote

def crawl_gartner_final():
    # 1. ê²€ìƒ‰ í‚¤ì›Œë“œ ìµœì í™”: ê°€íŠ¸ë„ˆ ê³µì‹ ë³´ë„ìë£Œ ìœ„ì£¼
    # 2026ë…„ ê°€íŠ¸ë„ˆ AI ë° í…Œí¬ ê´€ë ¨ í‚¤ì›Œë“œ ì¡°í•©
    query = quote('Gartner "Press Release" AI ICT 2026')
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"
    
    file_name = 'Gartner_Insight_Archive.csv'
    all_data = []

    print(f"ğŸ“¡ êµ¬ê¸€ RSS ì¸í…”ë¦¬ì „ìŠ¤ ê°€ë™: {query}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
    }

    try:
        # RSS í”¼ë“œ ìš”ì²­
        response = requests.get(rss_url, headers=headers, timeout=20)
        
        if response.status_code == 200:
            root = ET.fromstring(response.content)
            # RSS ì•„ì´í…œ ìˆœíšŒ (ìµœì‹  10~15ê°œ)
            for item in root.findall('.//item')[:15]:
                title = item.find('title').text
                link = item.find('link').text
                pub_date = item.find('pubDate').text
                
                # ê°€íŠ¸ë„ˆ ê³µì‹ ë„ë©”ì¸ì´ í¬í•¨ëœ ê²°ê³¼ë§Œ ì—„ì„ 
                if "gartner.com" in link.lower() or "gartner" in title.lower():
                    all_data.append({
                        "date": pub_date,
                        "title": title.split(' - ')[0], # ì œëª© ë’¤ì˜ ì–¸ë¡ ì‚¬ëª… ì œê±°
                        "link": link
                    })

            if all_data:
                # CSV ì €ì¥ (ì—‘ì…€ ê¹¨ì§ ë°©ì§€ utf-8-sig)
                with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                    writer.writeheader()
                    writer.writerows(all_data)
                print(f"âœ… ìˆ˜ì§‘ ì„±ê³µ! ì´ {len(all_data)}ê±´ì˜ ê°€íŠ¸ë„ˆ ì¸ì‚¬ì´íŠ¸ í™•ë³´.")
                return
            else:
                print("âš ï¸ ì¡°ê±´ì— ë§ëŠ” ìµœì‹  ê°€íŠ¸ë„ˆ ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ ì ‘ì† ì‹¤íŒ¨ (ì½”ë“œ: {response.status_code})")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

    # ì‹¤íŒ¨ ì‹œ ë¹ˆ íŒŒì¼ ìƒì„± (ì›Œí¬í”Œë¡œìš° ì—ëŸ¬ ë°©ì§€)
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        f.write("date,title,link\n")

if __name__ == "__main__":
    crawl_gartner_final()
