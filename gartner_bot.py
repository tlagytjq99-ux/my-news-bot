import asyncio
from playwright.async_api import async_playwright
import csv

async def crawl_gartner_rss_safe():
    # ê°€íŠ¸ë„ˆê°€ ê³µì‹ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” ë‰´ìŠ¤ RSS í”¼ë“œ (ë³´ì•ˆ ê²€ì‚¬ê°€ í›¨ì”¬ ì•½í•¨)
    url = "https://www.gartner.com/it/content/xml/newsroom.xml"
    file_name = 'Gartner_Insight_Archive.csv'
    all_data = []

    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € ëŒ€ì‹  ë‹¨ìˆœ ë¦¬í€˜ìŠ¤íŠ¸ ëª¨ë“œë¡œ ë™ì‘ ì‹œë„
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        print(f"ğŸ“¡ ê°€íŠ¸ë„ˆ RSS ì „ìš© ì±„ë„ ì ‘ì† ì‹œë„...")
        
        try:
            # RSSëŠ” ê°€ë³ê¸° ë•Œë¬¸ì— íƒ€ì„ì•„ì›ƒì„ 30ì´ˆë¡œ ì¤„ì—¬ë„ ì¶©ë¶„í•©ë‹ˆë‹¤.
            response = await page.goto(url, wait_until="commit", timeout=30000)
            
            # XML ë°ì´í„° íŒŒì‹± (ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ)
            content = await page.content()
            
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ íŒŒì‹±ìœ¼ë¡œ 2026ë…„ ìµœì‹  ë°ì´í„° 10ê°œ ì¶”ì¶œ
            import re
            titles = re.findall(r'<title><!\[CDATA\[(.*?)\]\]></title>', content)
            links = re.findall(r'<link>(.*?)</link>', content)

            for i in range(min(len(titles), 15)):
                # RSS ìµœìƒë‹¨ì€ ë³´í†µ ë‰´ìŠ¤ë£¸ ë©”ì¸ì´ë¯€ë¡œ ì œì™¸
                if "Newsroom" in titles[i] and i == 0: continue
                
                all_data.append({
                    "date": "2026-Latest",
                    "title": titles[i].strip(),
                    "link": links[i].strip()
                })
            
            print(f"âœ… RSSë¥¼ í†µí•´ ìµœì‹  ìë£Œ {len(all_data)}ê±´ í™•ë³´!")

        except Exception as e:
            print(f"âŒ RSS ì ‘ì† ì‹¤íŒ¨: {e}")
            # ë§Œì•½ RSSë„ ë§‰í˜”ë‹¤ë©´, ìµœì¢… ìˆ˜ë‹¨ìœ¼ë¡œ 'êµ¬ê¸€ ë‰´ìŠ¤' ê²€ìƒ‰ ê²°ê³¼ ìš°íšŒ ì‹œë„ ì½”ë“œë¡œ ìë™ ì „í™˜ ê°€ëŠ¥

        await browser.close()

    if all_data:
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
            writer.writeheader()
            writer.writerows(all_data)
        print(f"âœ¨ [ì„±ê³µ] {file_name} ì €ì¥ ì™„ë£Œ.")
    else:
        # ë¹ˆ íŒŒì¼ì´ë¼ë„ ìƒì„±í•˜ì—¬ ì—ëŸ¬ ë°©ì§€
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            f.write("date,title,link\n")
        print("ğŸš¨ ëª¨ë“  ìš°íšŒë¡œê°€ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(crawl_gartner_rss_safe())
