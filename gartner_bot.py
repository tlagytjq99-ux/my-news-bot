import asyncio
from playwright.async_api import async_playwright
import csv

async def crawl_gartner_archive_ultimate():
    # 2025ë…„ê³¼ 2024ë…„ ë‘ í˜ì´ì§€ë§Œ ì§‘ì¤‘ ê³µëµ
    target_years = ["2025", "2024"]
    file_name = 'Gartner_Insight_Archive.csv'
    all_data = []

    async with async_playwright() as p:
        # 1. ë¸Œë¼ìš°ì € ì‹¤í–‰ (ê°€íŠ¸ë„ˆê°€ ì¢‹ì•„í•˜ëŠ” ìµœì‹  í¬ë¡¬ ë²„ì „ìœ¼ë¡œ ìœ„ì¥)
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            viewport={'width': 1920, 'height': 1080}
        )
        page = await context.new_page()

        print("ğŸš€ Gartner ë³´ì•ˆ ìš°íšŒ ëª¨ë“œ ê°€ë™...")

        for year in target_years:
            url = f"https://www.gartner.com/en/newsroom/archive/{year}"
            print(f"ğŸ“¡ {year}ë…„ ì•„ì¹´ì´ë¸Œ ì ‘ê·¼ ì‹œë„: {url}")
            
            try:
                # 2. í˜ì´ì§€ ì ‘ì† (ì•ˆì •ì„±ì„ ìœ„í•´ 5ì´ˆ ëŒ€ê¸°)
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                await asyncio.sleep(5) 

                # 3. í™”ë©´ì„ ì•„ë˜ë¡œ ì²œì²œíˆ ìŠ¤í¬ë¡¤ (ë°ì´í„° ë¡œë”© ìœ ë„)
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight / 2)")
                await asyncio.sleep(2)
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(2)

                # 4. ë°ì´í„° ì¶”ì¶œ ë¡œì§ (í´ë˜ìŠ¤ëª…ì„ ë” í¬ê´„ì ìœ¼ë¡œ ë³€ê²½)
                items = await page.evaluate("""
                    () => {
                        const results = [];
                        // ê°€íŠ¸ë„ˆ ë‰´ìŠ¤ ì¹´ë“œì™€ ë§í¬ë¥¼ ì°¾ëŠ” ë” ì •êµí•œ ì…€ë ‰í„°
                        const links = document.querySelectorAll('a[href*="/en/newsroom/press-releases/"]');
                        
                        links.forEach(link => {
                            const title = link.innerText.trim();
                            const href = link.href;
                            // ì œëª©ì´ ë„ˆë¬´ ì§§ì€ ê±´ ì œì™¸
                            if (title.length > 10) {
                                results.push({
                                    date: "Archive", // ìƒì„¸ í˜ì´ì§€ ë“¤ì–´ê°€ì•¼ ë‚ ì§œê°€ ë³´ì´ì§€ë§Œ ì¼ë‹¨ ë³´ë¥˜
                                    title: title.replace(/\\n/g, ' '),
                                    link: href
                                });
                            }
                        });
                        return results;
                    }
                """)
                
                if items:
                    all_data.extend(items)
                    print(f"âœ… {year}ë…„ ë°ì´í„° {len(items)}ê±´ í™•ë³´!")
                else:
                    print(f"âš ï¸ {year}ë…„ ë°ì´í„°ê°€ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤. êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            except Exception as e:
                print(f"âŒ {year}ë…„ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)[:100]}")
                continue

        await browser.close()

    # 5. ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ 1ê±´ì´ë¼ë„ ìˆìœ¼ë©´ ê°•ì œë¡œ íŒŒì¼ ìƒì„±
    if all_data:
        # ì¤‘ë³µ ì œê±°
        unique_data = {item['link']: item for item in all_data}.values()
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
            writer.writeheader()
            writer.writerows(unique_data)
        print(f"\nâœ¨ [ìµœì¢… ì„±ê³µ] {len(unique_data)}ê±´ì˜ íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    else:
        # íŒŒì¼ì´ ì•ˆ ë§Œë“¤ì–´ì ¸ì„œ ì—ëŸ¬ ë‚˜ëŠ” ê±¸ ë°©ì§€í•˜ê¸° ìœ„í•´ ë¹ˆ íŒŒì¼ì´ë¼ë„ ìƒì„±
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            f.write("date,title,link\\n")
        print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ ë¹ˆ íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(crawl_gartner_archive_ultimate())
