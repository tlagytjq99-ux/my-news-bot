import requests
from bs4 import BeautifulSoup
import csv
import time

def crawl_gartner_via_google_news():
    # 2026ë…„ ê°€íŠ¸ë„ˆ ë‰´ìŠ¤ë£¸ ë³´ë„ìë£Œë¥¼ êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ ê²€ìƒ‰
    # site:gartner.com í•„í„°ë¥¼ ì¨ì„œ ì •í™•ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤.
    search_url = "https://www.google.com/search?q=site:gartner.com/en/newsroom/press-releases+2026&tbm=nws"
    file_name = 'Gartner_Insight_Archive.csv'
    
    # ì‹¤ì œ ì‚¬ëŒì˜ ë¸Œë¼ìš°ì € í—¤ë” (í•µì‹¬)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://www.google.com/'
    }

    print(f"ğŸ“¡ êµ¬ê¸€ ë‰´ìŠ¤ë¥¼ í†µí•´ ê°€íŠ¸ë„ˆ ìë£Œ ìš°íšŒ ìˆ˜ì§‘ ì¤‘...")
    
    try:
        # êµ¬ê¸€ì— ìš”ì²­ ë³´ëƒ„ (íƒ€ì„ì•„ì›ƒ 20ì´ˆ)
        response = requests.get(search_url, headers=headers, timeout=20)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì˜ ë‰´ìŠ¤ ì¹´ë“œë“¤ì„ íƒ€ê²ŸíŒ… (div[data-ved] êµ¬ì¡°)
            articles = soup.select('div.SoS9be') # êµ¬ê¸€ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì˜ ê³µí†µ í´ë˜ìŠ¤
            
            # í´ë˜ìŠ¤ê°€ ë³€ê²½ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ a íƒœê·¸ ê¸°ë°˜ìœ¼ë¡œë„ íƒìƒ‰
            if not articles:
                articles = soup.select('div[data-ved] a[role="presentation"]')

            all_data = []
            for article in articles:
                # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                title_elem = article.select_one('div[role="heading"]')
                link_elem = article.get('href') if article.name == 'a' else article.select_one('a')['href']
                
                if title_elem and link_elem and "gartner.com" in link_elem:
                    title = title_elem.get_text().strip()
                    all_data.append({
                        "date": "2026-Fixed",
                        "title": title.replace('\n', ' '),
                        "link": link_elem
                    })

            # ìƒìœ„ 10ê°œë§Œ ì €ì¥
            final_data = all_data[:10]

            if final_data:
                with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                    writer.writeheader()
                    writer.writerows(final_data)
                print(f"âœ… êµ¬ê¸€ ìš°íšŒ ì„±ê³µ! {len(final_data)}ê±´ì˜ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
                return
            else:
                print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê°€íŠ¸ë„ˆ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ êµ¬ê¸€ ì ‘ì† ì‹¤íŒ¨ (ìƒíƒœ ì½”ë“œ: {response.status_code})")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ì‹¤íŒ¨ ì‹œ ë¹ˆ íŒŒì¼ ìƒì„± (Workflow ì—ëŸ¬ ë°©ì§€)
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        f.write("date,title,link\n")

if __name__ == "__main__":
    crawl_gartner_via_google_news()
