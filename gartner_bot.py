"""
Gartner Newsroom Crawler
ê°€íŠ¸ë„ˆ ë‰´ìŠ¤ë£¸ì—ì„œ ìµœì‹  10ê°œ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
"""

import json
import csv
import time
from datetime import datetime
from playwright.sync_api import sync_playwright


NEWSROOM_URL = "https://www.gartner.com/en/newsroom"


def crawl_gartner_newsroom() -> list[dict]:
    """ê°€íŠ¸ë„ˆ ë‰´ìŠ¤ë£¸ì—ì„œ ìµœì‹  10ê°œ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    results = []

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
            ],
        )
        context = browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            ),
            viewport={"width": 1280, "height": 900},
        )
        page = context.new_page()

        print(f"[{datetime.now()}] ê°€íŠ¸ë„ˆ ë‰´ìŠ¤ë£¸ ì ‘ì† ì¤‘...")
        page.goto(NEWSROOM_URL, wait_until="networkidle", timeout=60_000)

        # ì¿ í‚¤ ë™ì˜ íŒì—… ì²˜ë¦¬ (ìˆëŠ” ê²½ìš°)
        try:
            page.click("button#onetrust-accept-btn-handler", timeout=5_000)
            print("ì¿ í‚¤ ë™ì˜ ì™„ë£Œ")
            time.sleep(1)
        except Exception:
            pass

        # ë‰´ìŠ¤ ì¹´ë“œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        print("ë‰´ìŠ¤ ì¹´ë“œ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        page.wait_for_selector(
            "article, .newsroom-article, [class*='article-card'], [class*='news-card']",
            timeout=30_000,
        )
        time.sleep(2)

        # â”€â”€ ì—¬ëŸ¬ CSS ì„ íƒìë¥¼ ìˆœì„œëŒ€ë¡œ ì‹œë„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        selectors_to_try = [
            # ê°€ì¥ ì¼ë°˜ì ì¸ íŒ¨í„´
            "article",
            "[class*='article-card']",
            "[class*='news-card']",
            "[class*='newsroom-card']",
            ".card",
            "li[class*='item']",
        ]

        article_elements = []
        used_selector = ""
        for sel in selectors_to_try:
            els = page.query_selector_all(sel)
            if len(els) >= 3:
                article_elements = els
                used_selector = sel
                break

        print(f"ì„ íƒì '{used_selector}'ë¡œ {len(article_elements)}ê°œ ìš”ì†Œ ë°œê²¬")

        # ìµœì‹  10ê°œë§Œ ì²˜ë¦¬
        for idx, el in enumerate(article_elements[:10], start=1):
            try:
                # ì œëª©
                title = ""
                for title_sel in ["h1", "h2", "h3", "h4", "[class*='title']"]:
                    t_el = el.query_selector(title_sel)
                    if t_el:
                        title = t_el.inner_text().strip()
                        if title:
                            break

                # URL
                url = ""
                link_el = el.query_selector("a")
                if link_el:
                    href = link_el.get_attribute("href") or ""
                    url = href if href.startswith("http") else f"https://www.gartner.com{href}"

                # ë‚ ì§œ
                date = ""
                for date_sel in ["time", "[class*='date']", "[class*='time']", "[datetime]"]:
                    d_el = el.query_selector(date_sel)
                    if d_el:
                        date = (
                            d_el.get_attribute("datetime")
                            or d_el.inner_text().strip()
                        )
                        if date:
                            break

                # ì¹´í…Œê³ ë¦¬ / íƒœê·¸
                category = ""
                for cat_sel in [
                    "[class*='category']",
                    "[class*='tag']",
                    "[class*='topic']",
                    "[class*='label']",
                ]:
                    c_el = el.query_selector(cat_sel)
                    if c_el:
                        category = c_el.inner_text().strip()
                        if category:
                            break

                # ìš”ì•½ë¬¸ (description / summary)
                summary = ""
                for sum_sel in [
                    "p",
                    "[class*='description']",
                    "[class*='summary']",
                    "[class*='excerpt']",
                ]:
                    s_el = el.query_selector(sum_sel)
                    if s_el:
                        summary = s_el.inner_text().strip()
                        if summary:
                            break

                article = {
                    "rank": idx,
                    "title": title,
                    "url": url,
                    "date": date,
                    "category": category,
                    "summary": summary,
                    "crawled_at": datetime.now().isoformat(),
                }
                results.append(article)
                print(f"  [{idx}] {title[:60]}{'...' if len(title) > 60 else ''}")

            except Exception as e:
                print(f"  [{idx}] íŒŒì‹± ì˜¤ë¥˜: {e}")

        browser.close()

    return results


def save_results(data: list[dict]) -> None:
    """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ JSON / CSV ë‘ ê°€ì§€ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # JSON ì €ì¥
    json_path = f"gartner_news_{timestamp}.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"\nJSON ì €ì¥ ì™„ë£Œ: {json_path}")

    # CSV ì €ì¥
    csv_path = f"gartner_news_{timestamp}.csv"
    if data:
        with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
        print(f"CSV  ì €ì¥ ì™„ë£Œ: {csv_path}")

    # GitHub Actions summary ì¶œë ¥
    summary_lines = [
        "## ğŸ—ï¸ Gartner Newsroom â€“ ìµœì‹  10ê°œ ê¸°ì‚¬\n",
        f"í¬ë¡¤ë§ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "| # | ì œëª© | ë‚ ì§œ | ì¹´í…Œê³ ë¦¬ |",
        "|---|------|------|----------|",
    ]
    for item in data:
        title_link = f"[{item['title'][:50]}]({item['url']})" if item["url"] else item["title"][:50]
        summary_lines.append(
            f"| {item['rank']} | {title_link} | {item['date']} | {item['category']} |"
        )

    summary_md = "\n".join(summary_lines)

    # $GITHUB_STEP_SUMMARY ì— ê¸°ë¡ (GitHub Actions í™˜ê²½)
    import os
    gh_summary = os.environ.get("GITHUB_STEP_SUMMARY")
    if gh_summary:
        with open(gh_summary, "a", encoding="utf-8") as f:
            f.write(summary_md + "\n")
        print("GitHub Actions ìŠ¤í… ìš”ì•½ ê¸°ë¡ ì™„ë£Œ")
    else:
        print("\n" + summary_md)


if __name__ == "__main__":
    articles = crawl_gartner_newsroom()

    if articles:
        print(f"\nì´ {len(articles)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì„±ê³µ")
        save_results(articles)
    else:
        print("í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        raise SystemExit(1)
