import requests
import xml.etree.ElementTree as ET
import csv
from urllib.parse import quote

def crawl_gartner_google_rss():
    # 1. ê²€ìƒ‰ì–´ ì„¤ì •: site:gartner.com 2026 (URL ì¸ì½”ë”© í¬í•¨)
    query = quote("site:gartner.com/en/newsroom/press-releases 2026")
    # êµ¬ê¸€ ë‰´ìŠ¤ RSS ê³µì‹ ì£¼ì†Œ
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"
    
    file_name = 'Gartner_Insight_Archive.csv'
    all_data = []

    print(f"ğŸ“¡ êµ¬ê¸€ RSS í”¼ë“œ ì§ì ‘ í†µì‹  ì‹œì‘...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
    }

    try:
        response = requests.get(rss_url, headers=headers, timeout=20)
        
        if response.status_code == 200:
            # XML ë°ì´í„° íŒŒì‹±
            root = ET.fromstring(response.content)
            
            # RSS ë‚´ì˜ ê° ì•„ì´í…œ(ë‰´ìŠ¤) ìˆœíšŒ
            for item in root.findall('.//item')[:15]: # ìµœì‹  15ê°œ
                title = item.find('title').text
                link = item.find('link').text
                pub_date = item.find('pubDate').text
                
                # ê°€íŠ¸ë„ˆ ë§í¬ë§Œ í•„í„°ë§ (ê°€ë” ê´‘ê³  ì„ì„ ë°©ì§€)
                if "gartner.com" in link:
                    all_data.append({
                        "date": pub_date,
                        "title": title,
                        "link": link
                    })

            if all_data:
                with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                    writer.writeheader()
                    writer.writerows(all_data)
                print(f"âœ… RSS ìš°íšŒ ì„±ê³µ! ì´ {len(all_data)}ê±´ í™•ë³´ ì™„ë£Œ.")
                return
            else:
                print("âš ï¸ RSS ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ RSS ì ‘ì† ì‹¤íŒ¨ (ì½”ë“œ: {response.status_code})")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ì‹¤íŒ¨ ì‹œ ë¹ˆ íŒŒì¼ ìƒì„±
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        f.write("date,title,link\n")

if __name__ == "__main__":
    crawl_gartner_google_rss()
