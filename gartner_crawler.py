"""
Gartner Newsroom Crawler v3
ì „ëµ:
  1ìˆœìœ„ - ê³µì‹ RSS í”¼ë“œ (www.gartner.com/newsroom/rss) â†’ ë´‡ ì°¨ë‹¨ ì—†ì´ ì•ˆì •ì 
  2ìˆœìœ„ - Playwright í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € â†’ RSS ì‹¤íŒ¨ ì‹œ ë°±ì—…
"""

import csv
import json
import os
import time
import xml.etree.ElementTree as ET
from datetime import datetime

import requests

NEWSROOM_URL = "https://www.gartner.com/en/newsroom"
RSS_URL = "https://www.gartner.com/newsroom/rss"
MAX_ARTICLES = 10

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/122.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
}

# ê¸°ì‚¬ URL íŒë³„ íŒ¨í„´
ARTICLE_PATH_KEYWORDS = [
    "/newsroom/press-releases/",
    "/newsroom/announcements/",
    "/newsroom/q-and-a/",
    "/newsroom/conference-highlights/",
]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì „ëµ 1: RSS í”¼ë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def crawl_via_rss() -> list[dict]:
    """ê°€íŠ¸ë„ˆ ê³µì‹ RSS í”¼ë“œë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    print(f"\n[RSS] {RSS_URL} ìš”ì²­ ì¤‘...")
    try:
        resp = requests.get(RSS_URL, headers=HEADERS, timeout=30)
        resp.raise_for_status()
    except Exception as e:
        print(f"[RSS] ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []

    content_type = resp.headers.get("Content-Type", "")
    print(f"[RSS] ì‘ë‹µ ìƒíƒœ: {resp.status_code} | Content-Type: {content_type}")

    # HTMLì´ ë°˜í™˜ë˜ë©´ RSSê°€ ì•„ë‹˜
    text = resp.text.strip()
    if text.startswith("<!DOCTYPE") or text.startswith("<html"):
        print("[RSS] RSSê°€ ì•„ë‹Œ HTMLì´ ë°˜í™˜ë¨ â†’ ë°±ì—… ì „ëµìœ¼ë¡œ ì „í™˜")
        return []

    try:
        root = ET.fromstring(text)
    except ET.ParseError as e:
        print(f"[RSS] XML íŒŒì‹± ì‹¤íŒ¨: {e}")
        return []

    # RSS ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì²˜ë¦¬
    ns = {
        "dc": "http://purl.org/dc/elements/1.1/",
        "content": "http://purl.org/rss/1.0/modules/content/",
        "media": "http://search.yahoo.com/mrss/",
    }

    # <channel> â†’ <item> íƒìƒ‰ (RSS 2.0 / Atom ê³µí†µ ì²˜ë¦¬)
    items = root.findall(".//item")
    if not items:
        # Atom í”¼ë“œ í˜•ì‹
        atom_ns = "http://www.w3.org/2005/Atom"
        items = root.findall(f".//{{{atom_ns}}}entry")

    print(f"[RSS] í”¼ë“œ ì•„ì´í…œ ìˆ˜: {len(items)}")
    if not items:
        print("[RSS] ì•„ì´í…œ ì—†ìŒ â†’ ë°±ì—… ì „ëµìœ¼ë¡œ ì „í™˜")
        return []

    results = []
    for idx, item in enumerate(items[:MAX_ARTICLES], start=1):
        def _text(tag, default=""):
            el = item.find(tag) or item.find(f"dc:{tag}", ns)
            return el.text.strip() if el is not None and el.text else default

        title    = _text("title")
        url      = _text("link") or _text("guid")
        date     = _text("pubDate") or _text("dc:date", ns) or _text("published")
        category = _text("category")
        summary  = _text("description") or _text("summary")

        # HTML íƒœê·¸ ì œê±° (ìš”ì•½ì— ë§ˆí¬ì—… í¬í•¨ë  ìˆ˜ ìˆìŒ)
        import re
        summary = re.sub(r"<[^>]+>", "", summary).strip()

        article = {
            "rank": idx,
            "title": title,
            "url": url,
            "date": date,
            "category": category,
            "summary": summary[:300],
            "crawled_at": datetime.now().isoformat(),
        }
        results.append(article)
        print(f"  [{idx:02d}] {title[:70]}{'...' if len(title) > 70 else ''}")

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì „ëµ 2: Playwright (ë°±ì—…)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def crawl_via_playwright() -> list[dict]:
    """Playwrightë¡œ ê°€íŠ¸ë„ˆ ë‰´ìŠ¤ë£¸ì„ ì§ì ‘ ë Œë”ë§í•©ë‹ˆë‹¤."""
    try:
        from playwright.sync_api import sync_playwright, TimeoutError as PwTimeout
    except ImportError:
        print("[Playwright] playwright íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    results = []

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--window-size=1280,900",
            ],
        )
        context = browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/122.0.0.0 Safari/537.36"
            ),
            locale="en-US",
            viewport={"width": 1280, "height": 900},
            # ìë°”ìŠ¤í¬ë¦½íŠ¸ í™œì„±í™” ëª…ì‹œ
            java_script_enabled=True,
        )
        page = context.new_page()

        # â”€â”€ ì ‘ì† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(f"\n[Playwright] {NEWSROOM_URL} ì ‘ì† ì¤‘...")
        try:
            page.goto(NEWSROOM_URL, wait_until="networkidle", timeout=90_000)
        except PwTimeout:
            print("[Playwright] networkidle íƒ€ì„ì•„ì›ƒ â†’ domcontentloadedë¡œ ì¬ì‹œë„")
            page.goto(NEWSROOM_URL, wait_until="domcontentloaded", timeout=60_000)

        # â”€â”€ ì¿ í‚¤ íŒì—… ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for btn in ["#onetrust-accept-btn-handler", "button:has-text('Accept All')", "button:has-text('Accept')"]:
            try:
                page.click(btn, timeout=4_000)
                print("[Playwright] ì¿ í‚¤ ë™ì˜ ì™„ë£Œ")
                time.sleep(1)
                break
            except Exception:
                pass

        # â”€â”€ ìŠ¤í¬ë¡¤ë¡œ ë ˆì´ì§€ ë¡œë“œ ìœ ë„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("[Playwright] í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì¤‘ (ë ˆì´ì§€ ë¡œë“œ ìœ ë„)...")
        for _ in range(5):
            page.evaluate("window.scrollBy(0, 600)")
            time.sleep(0.8)
        page.evaluate("window.scrollTo(0, 0)")
        time.sleep(2)

        # â”€â”€ ê¸°ì‚¬ ë§í¬ ëŒ€ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        selector_css = ", ".join(
            f"a[href*='{kw}']" for kw in ARTICLE_PATH_KEYWORDS
        )
        try:
            page.wait_for_selector(selector_css, timeout=20_000)
            print("[Playwright] ê¸°ì‚¬ ë§í¬ ë°œê²¬!")
        except PwTimeout:
            print("[Playwright] ê¸°ì‚¬ ë§í¬ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼")
            # ë””ë²„ê·¸ìš© HTML ì €ì¥ (ì•„í‹°íŒ©íŠ¸ë¡œ í™•ì¸ ê°€ëŠ¥)
            with open("debug_page.html", "w", encoding="utf-8") as f:
                f.write(page.content())
            print("[Playwright] debug_page.html ì €ì¥ ì™„ë£Œ")

        # â”€â”€ ë§í¬ ìˆ˜ì§‘ ë° í•„í„°ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        all_links = page.query_selector_all("a[href]")
        print(f"[Playwright] ì „ì²´ ë§í¬ ìˆ˜: {len(all_links)}")

        seen: set[str] = set()
        article_links = []
        for link in all_links:
            href = link.get_attribute("href") or ""
            if not any(kw in href for kw in ARTICLE_PATH_KEYWORDS):
                continue
            full_url = href if href.startswith("http") else f"https://www.gartner.com{href}"
            if full_url in seen:
                continue
            seen.add(full_url)
            article_links.append((link, full_url))

        print(f"[Playwright] ê¸°ì‚¬ URL í•„í„°ë§ ê²°ê³¼: {len(article_links)}ê°œ")

        # â”€â”€ ê° ê¸°ì‚¬ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        import re
        for idx, (link_el, full_url) in enumerate(article_links[:MAX_ARTICLES], start=1):
            try:
                title = link_el.inner_text().strip()

                # ì œëª©ì´ ì§§ìœ¼ë©´ ë¶€ëª¨ ì»¨í…Œì´ë„ˆì—ì„œ íƒìƒ‰
                if len(title) < 10:
                    card_handle = link_el.evaluate_handle(
                        "el => el.closest('article, li, div[class*=\"card\"], div[class*=\"item\"]')"
                    )
                    card_el = card_handle.as_element() if card_handle else None
                    if card_el:
                        for h in ["h1", "h2", "h3", "h4"]:
                            h_el = card_el.query_selector(h)
                            if h_el:
                                t = h_el.inner_text().strip()
                                if t:
                                    title = t
                                    break

                card_handle = link_el.evaluate_handle(
                    "el => el.closest('article, li, section, "
                    "div[class*=\"card\"], div[class*=\"item\"], div[class*=\"result\"]')"
                )
                card_el = card_handle.as_element() if card_handle else None

                date = category = summary = ""
                if card_el:
                    for sel, attr in [("time[datetime]", "datetime"), ("time", None), ("[class*='date']", None)]:
                        d = card_el.query_selector(sel)
                        if d:
                            date = (d.get_attribute("datetime") if attr else None) or d.inner_text().strip()
                            if date:
                                break

                    for sel in ["[class*='category']", "[class*='topic']", "[class*='tag']", "[class*='label']"]:
                        c = card_el.query_selector(sel)
                        if c:
                            category = c.inner_text().strip()
                            if category:
                                break

                    for sel in ["p", "[class*='description']", "[class*='summary']", "[class*='excerpt']"]:
                        s = card_el.query_selector(sel)
                        if s:
                            text = re.sub(r"\s+", " ", s.inner_text().strip())
                            if text and text != title:
                                summary = text
                                break

                article = {
                    "rank": idx,
                    "title": title,
                    "url": full_url,
                    "date": date,
                    "category": category,
                    "summary": summary,
                    "crawled_at": datetime.now().isoformat(),
                }
                results.append(article)
                print(f"  [{idx:02d}] {title[:70]}{'...' if len(title) > 70 else ''}")

            except Exception as e:
                print(f"  [{idx}] íŒŒì‹± ì˜¤ë¥˜: {e}")

        browser.close()

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def save_results(data: list[dict]) -> None:
    import re
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    json_path = f"gartner_news_{timestamp}.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… JSON ì €ì¥: {json_path}")

    csv_path = f"gartner_news_{timestamp}.csv"
    with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=data[0].keys())
        writer.writeheader()
        writer.writerows(data)
    print(f"âœ… CSV  ì €ì¥: {csv_path}")

    # GitHub Actions Step Summary
    lines = [
        "## ğŸ—ï¸ Gartner Newsroom â€“ ìµœì‹  ê¸°ì‚¬\n",
        f"í¬ë¡¤ë§ ì‹œê°: {datetime.now():%Y-%m-%d %H:%M:%S}\n",
        "| # | ì œëª© | ë‚ ì§œ | ì¹´í…Œê³ ë¦¬ |",
        "|---|------|------|----------|",
    ]
    for item in data:
        linked = f"[{item['title'][:60]}]({item['url']})" if item["url"] else item["title"][:60]
        lines.append(f"| {item['rank']} | {linked} | {item['date']} | {item['category']} |")

    gh = os.environ.get("GITHUB_STEP_SUMMARY")
    if gh:
        with open(gh, "a", encoding="utf-8") as f:
            f.write("\n".join(lines) + "\n")
        print("âœ… GitHub Actions ìŠ¤í… ìš”ì•½ ê¸°ë¡ ì™„ë£Œ")
    else:
        print("\n" + "\n".join(lines))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    print("=" * 60)
    print("Gartner Newsroom Crawler v3")
    print("=" * 60)

    # 1ìˆœìœ„: RSS
    articles = crawl_via_rss()

    # 2ìˆœìœ„: Playwright
    if not articles:
        print("\n[!] RSS ì‹¤íŒ¨ â†’ Playwright ë°±ì—… ì „ëµ ì‹¤í–‰")
        articles = crawl_via_playwright()

    if articles:
        print(f"\nì´ {len(articles)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì„±ê³µ")
        save_results(articles)
    else:
        print("\nâŒ ëª¨ë“  ì „ëµ ì‹¤íŒ¨. ê°€íŠ¸ë„ˆ ì‚¬ì´íŠ¸ ì ‘ê·¼ ì •ì±…ì„ í™•ì¸í•˜ì„¸ìš”.")
        raise SystemExit(1)
