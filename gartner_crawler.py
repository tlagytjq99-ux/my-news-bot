"""
Gartner Newsroom Crawler v2
- ê¸°ì‚¬ ë§í¬ URL íŒ¨í„´(/newsroom/press-releases/, /newsroom/announcements/ ë“±)ìœ¼ë¡œ í•„í„°ë§
- í˜ì´ì§€ê°€ ì™„ì „íˆ ë Œë”ë§ë  ë•Œê¹Œì§€ ëª…ì‹œì ìœ¼ë¡œ ëŒ€ê¸°
"""

import json
import csv
import os
import time
from datetime import datetime
from playwright.sync_api import sync_playwright, TimeoutError as PwTimeout

NEWSROOM_URL = "https://www.gartner.com/en/newsroom"

# ê°€íŠ¸ë„ˆ ê¸°ì‚¬ URLì— í¬í•¨ë˜ëŠ” ê²½ë¡œ íŒ¨í„´
ARTICLE_PATH_KEYWORDS = [
    "/newsroom/press-releases/",
    "/newsroom/announcements/",
    "/newsroom/q-and-a/",
    "/newsroom/conference-highlights/",
]


def is_article_url(href: str) -> bool:
    """ê°€íŠ¸ë„ˆ ë‰´ìŠ¤ ê¸°ì‚¬ URLì¸ì§€ íŒë³„í•©ë‹ˆë‹¤."""
    return any(kw in href for kw in ARTICLE_PATH_KEYWORDS)


def crawl_gartner_newsroom(max_articles: int = 10) -> list[dict]:
    results = []

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
            ],
        )
        context = browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/122.0.0.0 Safari/537.36"
            ),
            locale="en-US",
            viewport={"width": 1280, "height": 900},
        )
        page = context.new_page()

        # â”€â”€ 1. í˜ì´ì§€ ì ‘ì† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] ê°€íŠ¸ë„ˆ ë‰´ìŠ¤ë£¸ ì ‘ì† ì¤‘...")
        page.goto(NEWSROOM_URL, wait_until="domcontentloaded", timeout=60_000)

        # â”€â”€ 2. ì¿ í‚¤ íŒì—… ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for btn_sel in [
            "#onetrust-accept-btn-handler",
            "button:has-text('Accept')",
            "button:has-text('Agree')",
        ]:
            try:
                page.click(btn_sel, timeout=4_000)
                print("  ì¿ í‚¤ ë™ì˜ ì™„ë£Œ")
                time.sleep(1)
                break
            except Exception:
                pass

        # â”€â”€ 3. ê¸°ì‚¬ ë§í¬ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("  ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        try:
            page.wait_for_selector(
                "a[href*='/newsroom/press-releases/'], "
                "a[href*='/newsroom/announcements/'], "
                "a[href*='/newsroom/q-and-a/']",
                timeout=30_000,
            )
        except PwTimeout:
            print("  âš ï¸  ê¸°ì‚¬ ë§í¬ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼. í˜„ì¬ ë¡œë“œëœ ë‚´ìš©ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

        # ë™ì  ë Œë”ë§ ì™„ë£Œë¥¼ ìœ„í•œ ì¶”ê°€ ëŒ€ê¸°
        time.sleep(3)

        # â”€â”€ 4. ëª¨ë“  <a> íƒœê·¸ ìˆ˜ì§‘ í›„ ê¸°ì‚¬ URL í•„í„°ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        all_links = page.query_selector_all("a[href]")
        print(f"  ì „ì²´ ë§í¬ ìˆ˜: {len(all_links)}ê°œ")

        seen_urls: set[str] = set()
        article_links = []

        for link in all_links:
            href = link.get_attribute("href") or ""
            if not is_article_url(href):
                continue

            full_url = href if href.startswith("http") else f"https://www.gartner.com{href}"
            if full_url in seen_urls:
                continue
            seen_urls.add(full_url)
            article_links.append((link, full_url))

        print(f"  ê¸°ì‚¬ ë§í¬ í•„í„°ë§ ê²°ê³¼: {len(article_links)}ê°œ")

        if not article_links:
            # ë””ë²„ê·¸ìš©: í˜„ì¬ í˜ì´ì§€ HTML ì €ì¥
            with open("debug_page.html", "w", encoding="utf-8") as f:
                f.write(page.content())
            print("  âŒ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. debug_page.htmlì„ í™•ì¸í•˜ì„¸ìš”.")
            browser.close()
            return []

        # â”€â”€ 5. ê° ë§í¬ì—ì„œ ì œëª©Â·ë‚ ì§œÂ·ì¹´í…Œê³ ë¦¬Â·ìš”ì•½ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for idx, (link_el, full_url) in enumerate(article_links[:max_articles], start=1):
            try:
                # â‘  ì œëª©: ë§í¬ í…ìŠ¤íŠ¸ ìš°ì„ 
                title = link_el.inner_text().strip()

                # ë§í¬ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ë¶€ëª¨ ì»¨í…Œì´ë„ˆì—ì„œ ì œëª© íƒœê·¸ íƒìƒ‰
                if len(title) < 10:
                    parent = link_el.evaluate_handle(
                        "el => el.closest('article, li, div[class*=\"card\"], div[class*=\"item\"]')"
                    )
                    parent_el = parent.as_element() if parent else None
                    if parent_el:
                        for h in ["h1", "h2", "h3", "h4"]:
                            h_el = parent_el.query_selector(h)
                            if h_el:
                                t = h_el.inner_text().strip()
                                if t:
                                    title = t
                                    break

                # â‘¡ ì¹´ë“œ(ì»¨í…Œì´ë„ˆ) íƒìƒ‰
                card_handle = link_el.evaluate_handle(
                    "el => el.closest('article, li, section, "
                    "div[class*=\"card\"], div[class*=\"item\"], div[class*=\"result\"]')"
                )
                card_el = card_handle.as_element() if card_handle else None

                # â‘¢ ë‚ ì§œ
                date = ""
                if card_el:
                    for date_sel in ["time[datetime]", "time", "[class*='date']", "[class*='time']"]:
                        d = card_el.query_selector(date_sel)
                        if d:
                            date = d.get_attribute("datetime") or d.inner_text().strip()
                            if date:
                                break

                # â‘£ ì¹´í…Œê³ ë¦¬
                category = ""
                if card_el:
                    for cat_sel in ["[class*='category']", "[class*='topic']", "[class*='tag']", "[class*='label']"]:
                        c = card_el.query_selector(cat_sel)
                        if c:
                            category = c.inner_text().strip()
                            if category:
                                break

                # â‘¤ ìš”ì•½
                summary = ""
                if card_el:
                    for sum_sel in ["p", "[class*='description']", "[class*='summary']", "[class*='excerpt']"]:
                        s = card_el.query_selector(sum_sel)
                        if s:
                            text = s.inner_text().strip()
                            if text and text != title:
                                summary = text
                                break

                article = {
                    "rank": idx,
                    "title": title,
                    "url": full_url,
                    "date": date,
                    "category": category,
                    "summary": summary,
                    "crawled_at": datetime.now().isoformat(),
                }
                results.append(article)
                print(f"  [{idx:02d}] {title[:70]}{'...' if len(title) > 70 else ''}")
                print(f"        ë‚ ì§œ: {date or '(ì—†ìŒ)'}  |  ì¹´í…Œê³ ë¦¬: {category or '(ì—†ìŒ)'}")

            except Exception as e:
                print(f"  [{idx}] íŒŒì‹± ì˜¤ë¥˜: {e}")

        browser.close()

    return results


def save_results(data: list[dict]) -> None:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # JSON
    json_path = f"gartner_news_{timestamp}.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… JSON ì €ì¥: {json_path}")

    # CSV (UTF-8 BOM â†’ Excel í•œê¸€ ê¹¨ì§ ë°©ì§€)
    csv_path = f"gartner_news_{timestamp}.csv"
    if data:
        with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
        print(f"âœ… CSV  ì €ì¥: {csv_path}")

    # GitHub Actions Step Summary
    lines = [
        "## ğŸ—ï¸ Gartner Newsroom â€“ ìµœì‹  ê¸°ì‚¬\n",
        f"í¬ë¡¤ë§ ì‹œê°: {datetime.now():%Y-%m-%d %H:%M:%S}\n",
        "| # | ì œëª© | ë‚ ì§œ | ì¹´í…Œê³ ë¦¬ |",
        "|---|------|------|----------|",
    ]
    for item in data:
        linked_title = (
            f"[{item['title'][:60]}]({item['url']})"
            if item["url"] else item["title"][:60]
        )
        lines.append(
            f"| {item['rank']} | {linked_title} | {item['date']} | {item['category']} |"
        )

    gh_summary = os.environ.get("GITHUB_STEP_SUMMARY")
    if gh_summary:
        with open(gh_summary, "a", encoding="utf-8") as f:
            f.write("\n".join(lines) + "\n")
        print("âœ… GitHub Actions ìŠ¤í… ìš”ì•½ ê¸°ë¡ ì™„ë£Œ")
    else:
        print("\n" + "\n".join(lines))


if __name__ == "__main__":
    articles = crawl_gartner_newsroom(max_articles=10)

    if articles:
        print(f"\nì´ {len(articles)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì„±ê³µ")
        save_results(articles)
    else:
        print("\nâŒ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        raise SystemExit(1)
