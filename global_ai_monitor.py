import feedparser
import csv
import urllib.parse
from datetime import datetime
from googletrans import Translator
from googlenewsdecoder import gnewsdecoder

def main():
    # ğŸ¯ ê²€ìƒ‰ì–´ ë³´ê°•: ì¸ë¬¼ í”„ë¡œí•„, íŒ€ ì†Œê°œ, ë‹¨ìˆœ ì´ë²¤íŠ¸ í˜ì´ì§€ ì œì™¸ (-)
    target_orgs = {
        "OECD": 'site:oecd.org (intitle:"Artificial Intelligence" OR intitle:AI) -intitle:PISA -intitle:team',
        "IMF": 'site:imf.org (intitle:"Artificial Intelligence" OR intitle:AI) -intitle:biography',
        "UN": 'site:un.org (intitle:"Artificial Intelligence" OR intitle:AI) -intitle:photo',
        "WorldBank": 'site:worldbank.org (intitle:"Artificial Intelligence" OR intitle:AI) -intitle:team -intitle:expert -intitle:profile',
        "EU": 'site:europa.eu (intitle:"Artificial Intelligence" OR intitle:AI) -intitle:directory'
    }

    file_name = 'global_ai_policy_monitor.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")
    all_data = []

    print(f"ğŸŒ ê¸€ë¡œë²Œ AI ì •ì±… ëª¨ë‹ˆí„°ë§ ê³ ë„í™” ì‹œì‘: {collected_date}")

    for org, query in target_orgs.items():
        print(f"ğŸ“¡ {org} ë¶„ì„ ì¤‘...")
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
        
        try:
            feed = feedparser.parse(rss_url)
            entries = sorted(feed.entries, key=lambda x: x.get('published_parsed'), reverse=True)
            
            count = 0
            for entry in entries:
                if count >= 3: break
                
                title_en = entry.title.split(' - ')[0]

                # ğŸ’¡ [í•„í„° ì¶”ê°€] ë„ˆë¬´ ì§§ì€ ì œëª©ì´ë‚˜ ì¸ë¬¼ ì´ë¦„ë§Œ ìˆëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
                if len(title_en.split()) <= 2: 
                    continue
                
                # ë§í¬ í•´ë…
                try:
                    decoded = gnewsdecoder(entry.link)
                    link = decoded.get('decoded_url', entry.link)
                except:
                    link = entry.link

                # ë²ˆì—­
                try:
                    title_ko = translator.translate(title_en, dest='ko').text
                except:
                    title_ko = title_en

                pub_date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d') if hasattr(entry, 'published_parsed') else collected_date

                all_data.append({
                    "ê¸°ê´€": org, "ë°œí–‰ì¼": pub_date, "ì œëª©": title_ko,
                    "ì›ë¬¸": title_en, "ë§í¬": link, "ìˆ˜ì§‘ì¼": collected_date
                })
                count += 1
        except Exception as e:
            print(f"âš ï¸ {org} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    all_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)

    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
        writer.writeheader()
        writer.writerows(all_data)
        print(f"âœ… í•„í„°ë§ ì™„ë£Œ! ì´ {len(all_data)}ê±´ì˜ í•µì‹¬ ë¦¬í¬íŠ¸ ì €ì¥.")

if __name__ == "__main__":
    main()
