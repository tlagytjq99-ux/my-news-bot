import requests
from bs4 import BeautifulSoup
import csv
import os
from datetime import datetime
from urllib.parse import urljoin

def main():
    # ğŸ¯ íƒ€ê²Ÿ: ì¼ë³¸ ë‚´ê°ë¶€ ê³¼í•™ê¸°ìˆ í˜ì‹ (AI/ì‹ ê¸°ìˆ ) ì†Œì‹ í˜ì´ì§€
    target_url = "https://www8.cao.go.jp/cstp/stmain/index.html"
    file_name = 'japan_ai_report.csv'
    
    print(f"ğŸ“¡ [ì¼ë³¸ ë‚´ê°ë¶€] AI ì •ì±… ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
    }

    try:
        response = requests.get(target_url, headers=headers, timeout=20)
        response.encoding = response.apparent_encoding # ì¼ë³¸ì–´ ì¸ì½”ë”© ì²˜ë¦¬
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì¼ë³¸ ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ (ìµœì‹  ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì˜ì—­)
        news_list = soup.find('dl', class_='top_news') or soup.find('dl')

        new_data = []
        existing_titles = set()

        # ê¸°ì¡´ì— ì €ì¥ëœ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€
        if os.path.exists(file_name):
            with open(file_name, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    existing_titles.add(row['ì œëª©'])

        # AI ë° ê¸°ìˆ  ê´€ë ¨ ì¼ë³¸ì–´ í•µì‹¬ í‚¤ì›Œë“œ
        # AI(ì¸ê³µì§€ëŠ¥), äººå·¥çŸ¥èƒ½(ì¸ê³µì§€ëŠ¥), ãƒ‡ã‚¸ã‚¿ãƒ«(ë””ì§€í„¸), æˆ¦ç•¥(ì „ëµ), æŠ€è¡“(ê¸°ìˆ )
        ai_keywords = ['AI', 'äººå·¥çŸ¥èƒ½', 'ãƒ‡ã‚¸ã‚¿ãƒ«', 'æˆ¦ç•¥', 'æŠ€è¡“', 'ë°ì´í„°']

        count = 0
        dts = news_list.find_all('dt') if news_list else []
        
        for dt in dts:
            if count >= 5: break
            
            # ë‚ ì§œ ì¶”ì¶œ
            date_text = dt.get_text().strip()
            # ì œëª© ë° ë§í¬ ì¶”ì¶œ
            dd = dt.find_next_sibling('dd')
            if not dd: continue
            
            a_tag = dd.find('a')
            if not a_tag: continue
            
            title = a_tag.get_text().strip()
            link = urljoin(target_url, a_tag['href'])
            
            # í•„í„°ë§: ì œëª©ì— í‚¤ì›Œë“œê°€ ìˆê³ , ê¸°ì¡´ì— ì—†ë˜ ìƒˆë¡œìš´ ì œëª©ì¼ ë•Œë§Œ ì €ì¥
            if any(kw in title.upper() for kw in ai_keywords):
                if title not in existing_titles:
                    print(f"   ğŸ†• ìƒˆ ì •ì±… ë°œê²¬: {title[:40]}...")
                    new_data.append({
                        "ê¸°ê´€": "ì¼ë³¸ ë‚´ê°ë¶€(CAO)",
                        "ë°œí–‰ì¼": date_text.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '').strip(),
                        "ì œëª©": title,
                        "ë§í¬": link,
                        "ìˆ˜ì§‘ì¼": datetime.now().strftime("%Y-%m-%d")
                    })
                    count += 1

        # ğŸ’¾ ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ ë°ì´í„° ë’¤ì— ì¶”ê°€í•˜ëŠ” Append ëª¨ë“œ)
        if new_data:
            file_exists = os.path.exists(file_name)
            with open(file_name, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬", "ìˆ˜ì§‘ì¼"])
                if not file_exists:
                    writer.writeheader()
                writer.writerows(new_data)
            print(f"âœ… ì„±ê³µ! ì¼ë³¸ ì •ì±… ë°ì´í„° {len(new_data)}ê±´ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("ğŸ’¡ ìƒˆë¡œìš´ ì¼ë³¸ AI ì •ì±… ì†Œì‹ì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
