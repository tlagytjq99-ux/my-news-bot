import asyncio
import csv
import os
from datetime import datetime
from crawl4ai import AsyncWebCrawler
from googletrans import Translator

async def main():
    target_url = "https://www.cao.go.jp/houdou/houdou.html"
    file_name = 'japan_ai_report.csv'
    translator = Translator()

    print(f"ğŸ“¡ [ì¼ë³¸ ë‚´ê°ë¶€] Crawl4AI ê°€ë™ - ì§€ëŠ¥í˜• ë°ì´í„° ì¶”ì¶œ ì‹œì‘...")

    async with AsyncWebCrawler() as crawler:
        # 1. í˜ì´ì§€ í¬ë¡¤ë§ (ë¸Œë¼ìš°ì € ì‹¤í–‰ ë° ë§ˆí¬ë‹¤ìš´ ë³€í™˜)
        result = await crawler.arun(url=target_url)

        # 2. ê²°ê³¼ë¬¼(ë§ˆí¬ë‹¤ìš´)ì—ì„œ ë§í¬ì™€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        # Crawl4AIê°€ ì •ë¦¬í•´ì¤€ ë°ì´í„°ì—ì„œ ì œëª©ì´ ê¸´ ê²ƒë“¤ë§Œ ì¶”ë¦½ë‹ˆë‹¤.
        content = result.markdown
        lines = content.split('\n')
        
        new_data = []
        count = 0
        
        for line in lines:
            # ë§ˆí¬ë‹¤ìš´ ë§í¬ íŒ¨í„´ ì¶”ì¶œ: [ì œëª©](ë§í¬)
            if '[' in line and '](' in line:
                try:
                    title_ja = line.split('[')[1].split(']')[0].strip()
                    link = line.split('(')[1].split(')')[0].strip()
                    
                    # ğŸ’¡ ì§€ëŠ¥í˜• í•„í„°ë§: ë‰´ìŠ¤ ì œëª©ì²˜ëŸ¼ ê¸´ ê²ƒë§Œ
                    if len(title_ja) > 20 and ('.html' in link or '.pdf' in link):
                        # í•œêµ­ì–´ ë²ˆì—­
                        translated = translator.translate(title_ja, src='ja', dest='ko')
                        title_ko = translated.text
                        
                        # ì ˆëŒ€ ê²½ë¡œ ë³´ì •
                        full_url = link if link.startswith('http') else f"https://www.cao.go.jp{link}"

                        print(f"   âœ… ë°œê²¬ & ë²ˆì—­: {title_ko[:35]}...")
                        new_data.append({
                            "ê¸°ê´€": "ì¼ë³¸ ë‚´ê°ë¶€(CAO)",
                            "ë°œí–‰ì¼": datetime.now().strftime("%Y-%m-%d"),
                            "ì œëª©": title_ko,
                            "ì›ë¬¸ì œëª©": title_ja,
                            "ë§í¬": full_url,
                            "ìˆ˜ì§‘ì¼": datetime.now().strftime("%Y-%m-%d")
                        })
                        count += 1
                        if count >= 10: break
                except:
                    continue

        # ğŸ’¾ CSV ì €ì¥
        if new_data:
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸ì œëª©", "ë§í¬", "ìˆ˜ì§‘ì¼"])
                writer.writeheader()
                writer.writerows(new_data)
            print(f"ğŸ‰ ì„±ê³µ! Crawl4AIê°€ {len(new_data)}ê±´ì„ ì™„ë²½í•˜ê²Œ ë‚šì•„ëƒˆìŠµë‹ˆë‹¤.")
        else:
            print("âŒ Crawl4AIë¡œë„ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. URLì„ ë‹¤ì‹œ í™•ì¸í•´ë´ì•¼ í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
