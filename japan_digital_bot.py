import requests
from bs4 import BeautifulSoup
import csv
import time

def crawl_digital_2025_no_limit():
    start_page = 21
    end_page = 188
    file_name = 'Japan_Digital_2025_Full_Archive.csv'
    base_url = "https://www.digital.go.jp/news?page="
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }
    
    unique_links = set() # ì¤‘ë³µ ì²´í¬ìš©
    all_data = []

    print(f"ğŸš€ [í•œê³„ ëŒíŒŒ] {start_page} ~ {end_page} í˜ì´ì§€ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ìƒ…ìƒ…ì´ ë’¤ì§‘ë‹ˆë‹¤.")

    for page in range(start_page, end_page + 1):
        url = f"{base_url}{page}"
        print(f"ğŸ“¡ {page}/{end_page} í˜ì´ì§€ ë¶„ì„ ì¤‘... (í˜„ì¬ ëˆ„ì  {len(all_data)}ê±´)", end='\r')
        
        try:
            res = requests.get(url, headers=headers, timeout=20)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ë””ì§€í„¸ì²­ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì˜ í•µì‹¬ì€ <a> íƒœê·¸ ì•ˆì— <span>ì´ë‚˜ <time>ì´ ì„ì—¬ ìˆëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.
            # ëª¨ë“  <a> íƒœê·¸ë¥¼ ì „ìˆ˜ ì¡°ì‚¬í•©ë‹ˆë‹¤.
            for a in soup.find_all('a', href=True):
                href = a['href']
                
                # ë‰´ìŠ¤ë‚˜ ë³´ë„ìë£Œ ì£¼ì†Œ íŒ¨í„´ë§Œ íƒ€ê²ŸíŒ…
                if '/news/' in href or '/press/' in href:
                    full_url = "https://www.digital.go.jp" + href if href.startswith('/') else href
                    
                    # ì´ë¯¸ ìˆ˜ì§‘í•œ ë§í¬ë©´ íŒ¨ìŠ¤
                    if full_url in unique_links:
                        continue
                        
                    # ì œëª© ì¶”ì¶œ (ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ í•©ì¹¨)
                    title = a.get_text(separator=" ", strip=True)
                    
                    # ë‚ ì§œ ì¶”ì¶œ ì‹œë„: í•´ë‹¹ ë§í¬ ë¶€ëª¨ë‚˜ ìì‹ ìš”ì†Œì—ì„œ '2025'ê°€ ìˆëŠ”ì§€ í™•ì¸
                    # <a> íƒœê·¸ ë‚´ë¶€ í˜¹ì€ ê·¼ì²˜ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ íƒìƒ‰
                    context_text = a.parent.get_text() if a.parent else title
                    
                    if "2025" in context_text or "ä»¤å’Œ7" in context_text:
                        # ë„ˆë¬´ ì§§ê±°ë‚˜ ë©”ë‰´ í•­ëª©ì¸ ê²½ìš° ì œì™¸
                        if len(title) < 10: continue
                        
                        # ë‚ ì§œ í…ìŠ¤íŠ¸ë§Œ ê¹”ë”í•˜ê²Œ ì •ì œ (ì˜ˆ: 2025.02.10)
                        date_match = re.search(r'2025[-./]\d{1,2}[-./]\d{1,2}', context_text)
                        date_val = date_match.group() if date_match else "2025-Policy"
                        
                        unique_links.add(full_url)
                        all_data.append({
                            "date": date_val,
                            "title": title,
                            "link": full_url
                        })

            if page % 30 == 0:
                time.sleep(1)

        except Exception as e:
            print(f"\nâŒ {page}í˜ì´ì§€ ì˜¤ë¥˜: {e}")
            continue

    # ë°ì´í„° ì €ì¥
    if all_data:
        # ë‚ ì§œìˆœ ì •ë ¬
        all_data.sort(key=lambda x: x['date'], reverse=True)
        
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
            writer.writeheader()
            writer.writerows(all_data)
        print(f"\n\nâœ… [ì„ë¬´ ì™„ìˆ˜] ì´ {len(all_data)}ê±´ì˜ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâš ï¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")

if __name__ == "__main__":
    import re # re ëª¨ë“ˆ ì¶”ê°€
    crawl_digital_2025_no_limit()
