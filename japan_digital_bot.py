import asyncio
from playwright.sync_api import sync_playwright
import csv
import os

def crawl_with_brute_force_browser():
    url = "https://www.digital.go.jp/press?category=1"
    file_name = 'Japan_Digital_Policy_2025.csv'
    
    print("ğŸš€ [ìµœí›„ì˜ ìˆ˜ë‹¨] ìŠ¤í¬ë¡¤ë§ ë° ì§€ì—° ë¡œë”© ëŒ€ì‘ ëª¨ë“œ ê°€ë™...")

    with sync_playwright() as p:
        # ì‹¤ì œ í¬ë¡¬ ë¸Œë¼ìš°ì €ì™€ ë˜‘ê°™ì´ ë³´ì´ë„ë¡ ì„¸íŒ…
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        
        try:
            # 1. í˜ì´ì§€ ì ‘ì†
            page.goto(url, wait_until="domcontentloaded", timeout=60000)
            
            # 2. ê°•ì œ ëŒ€ê¸° ë° ìŠ¤í¬ë¡¤ (ë°ì´í„° ë¡œë”© ìœ ë„)
            print("â³ ë°ì´í„° ë¡œë”©ì„ ìœ„í•´ 7ì´ˆê°„ ëŒ€ê¸°í•˜ë©° ìŠ¤í¬ë¡¤í•©ë‹ˆë‹¤...")
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            page.wait_for_timeout(7000) 

            # 3. í˜ì´ì§€ ë‚´ ëª¨ë“  <a> íƒœê·¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜´
            links = page.query_selector_all('a')
            policy_data = []

            for a in links:
                try:
                    href = a.get_attribute('href') or ""
                    title = a.inner_text() or ""
                    
                    # ì •ì±… ê¸°ì‚¬ íŒ¨í„´ (/press/ìˆ«ì í˜¹ì€ ID)
                    if '/press/' in href and len(title.strip()) > 10:
                        full_url = href if href.startswith('http') else "https://www.digital.go.jp" + href
                        policy_data.append({
                            "date": "2025-2026",
                            "title": title.strip().replace('\n', ' '),
                            "link": full_url
                        })
                except:
                    continue

            # 4. ë°ì´í„° ì €ì¥
            if policy_data:
                # ì¤‘ë³µ ì œê±°
                unique_data = list({v['link']: v for v in policy_data}.values())
                with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                    writer.writeheader()
                    writer.writerows(unique_data)
                print(f"âœ… [ê°ê²©] ë“œë””ì–´ {len(unique_data)}ê±´ì˜ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤!")
            else:
                # ëê¹Œì§€ ì•ˆ ë‚˜ì˜¬ ê²½ìš° ë¹ˆ íŒŒì¼ì´ë¼ë„ ìƒì„±
                print("âš ï¸ ëª¨ë“  ìˆ˜ë‹¨ì„ ë™ì›í–ˆìœ¼ë‚˜ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                    f.write("date,title,link\n")

        except Exception as e:
            print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            with open(file_name, 'w', encoding='utf-8-sig') as f:
                f.write("date,title,link\n")
        finally:
            browser.close()

if __name__ == "__main__":
    crawl_with_brute_force_browser()
