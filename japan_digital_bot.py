import requests
from bs4 import BeautifulSoup
import csv
import time

def crawl_digital_agency_2025_all():
    base_url = "https://www.digital.go.jp/news?page="
    file_name = 'Japan_Digital_All_2025.csv'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }
    
    all_2025_data = []
    page = 0
    keep_scanning = True

    print("ğŸš€ [2025 ì „ìˆ˜ ì¡°ì‚¬] ë””ì§€í„¸ì²­ ì•„ì¹´ì´ë¸Œ ì •ë°€ ìŠ¤ìº”ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    while keep_scanning:
        url = f"{base_url}{page}"
        print(f"ğŸ“„ í˜„ì¬ {page}í˜ì´ì§€ ìŠ¤ìº” ì¤‘... ({url})")
        
        try:
            res = requests.get(url, headers=headers, timeout=20)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì¶”ì¶œ (ecl-card í´ë˜ìŠ¤ í˜¹ì€ article íƒœê·¸)
            articles = soup.find_all(['article', 'div'], class_=lambda x: x and 'card' in x) or soup.find_all('li')
            
            if not articles:
                print("ğŸ ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break

            page_found_2025 = False
            for item in articles:
                link_tag = item.find('a')
                date_tag = item.find('time')
                
                if link_tag and date_tag:
                    title = link_tag.get_text(strip=True)
                    date_text = date_tag.get_text(strip=True)
                    href = link_tag['href']
                    
                    # 2025ë…„ ë°ì´í„°ì¸ì§€ í™•ì¸ (ì—°ë„ í˜¹ì€ ì—°í˜¸ ä»¤å’Œ7å¹´)
                    if "2025" in date_text or "ä»¤å’Œ7" in date_text:
                        all_2025_data.append({
                            "date": date_text,
                            "title": title,
                            "link": "https://www.digital.go.jp" + href if href.startswith('/') else href
                        })
                        page_found_2025 = True
                        page_has_2025_data = True
                    
                    # 2024ë…„ ë°ì´í„°ê°€ ë‚˜ì˜¤ê¸° ì‹œì‘í•˜ë©´ ì¤‘ë‹¨
                    elif "2024" in date_text or "ä»¤å’Œ6" in date_text:
                        print("ğŸ›‘ 2024ë…„ ë°ì´í„° êµ¬ê°„ì— ì§„ì…í–ˆìŠµë‹ˆë‹¤. ìŠ¤ìº”ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        keep_scanning = False
                        break
            
            # í˜„ì¬ í˜ì´ì§€ì— 2025ë…„ ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ê³ , ì´ë¯¸ 2026ë…„ êµ¬ê°„ì„ ì§€ë‚¬ë‹¤ë©´ ì¢…ë£Œ ì•ˆì „ì¥ì¹˜
            if not page_found_2025 and page > 50: # ì•ˆì „ì„ ìœ„í•´ 50í˜ì´ì§€ê¹Œì§€ëŠ” íƒìƒ‰
                keep_scanning = False

            page += 1
            time.sleep(1) # ì„œë²„ ë¶€í•˜ ë°©ì§€ ë§¤ë„ˆ íƒ€ì„

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            break

    # ë°ì´í„° ì €ì¥
    if all_2025_data:
        # ì¤‘ë³µ ì œê±°
        unique_data = list({v['link']: v for v in all_2025_data}.values())
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
            writer.writeheader()
            writer.writerows(unique_data)
        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(unique_data)}ê±´ì˜ 2025ë…„ ìë£Œë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸ 2025ë…„ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    crawl_digital_agency_2025_all()
