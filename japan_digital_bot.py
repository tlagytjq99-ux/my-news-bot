import requests
import re
import csv
from datetime import datetime

def crawl_via_google_proxy():
    file_name = 'Japan_Digital_Policy_2025.csv'
    
    # ì „ëžµ: êµ¬ê¸€ ê²€ìƒ‰ ìºì‹œ ì£¼ì†Œë¥¼ ì‚¬ìš©í•˜ì—¬ ë””ì§€í„¸ì²­ì˜ ì°¨ë‹¨ì„ ìš°íšŒí•©ë‹ˆë‹¤.
    # ì´ ì£¼ì†ŒëŠ” êµ¬ê¸€ ì„œë²„ê°€ ê¸ì–´ì˜¨ "ê¹¨ë—í•œ" ë³µì‚¬ë³¸ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
    urls = [
        "https://www.digital.go.jp/press?category=1",
        "https://www.digital.go.jp/news/press"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)', # êµ¬ê¸€ë´‡ìœ¼ë¡œ ìœ„ìž¥
        'Accept-Language': 'ja-JP,ja;q=0.9'
    }

    print("ðŸš€ [íŠ¹ìˆ˜ ìž‘ì „] êµ¬ê¸€ ì„œë²„ì˜ ì‹œê°ìœ¼ë¡œ ì¼ë³¸ ë””ì§€í„¸ì²­ì„ í›‘ìŠµë‹ˆë‹¤...")
    policy_data = []

    for target_url in urls:
        try:
            # SSL ì¸ì¦ì„œ ë¬´ì‹œ ë° ì„¸ì…˜ ìœ ì§€
            session = requests.Session()
            response = session.get(target_url, headers=headers, timeout=20, verify=False)
            
            # í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ /press/xxxx íŒ¨í„´ ê°•ì œ ì¶”ì¶œ
            # ì´ë²ˆì—ëŠ” ì •ê·œí‘œí˜„ì‹ì„ ë” ëŠìŠ¨í•˜ê²Œ ìž¡ì•„ ëª¨ë“  ê¸°ì‚¬ë¥¼ ë‚šìŠµë‹ˆë‹¤.
            matches = re.findall(r'href="([^"]*/press/[^"]*)"[^>]*>(.*?)</a>', response.text)
            
            for link, title in matches:
                clean_title = re.sub(r'<[^>]+>', '', title).strip()
                if len(clean_title) < 10: continue
                
                full_url = link if link.startswith('http') else "https://www.digital.go.jp" + link
                policy_data.append({
                    "date": datetime.now().strftime("%Y-%m-%d"),
                    "title": clean_title,
                    "link": full_url
                })
        except Exception as e:
            print(f"âš ï¸ {target_url} ì‹œë„ ì¤‘ ì˜¤ë¥˜: {e}")

    # ë°ì´í„° ì €ìž¥
    if policy_data:
        unique_data = list({v['link']: v for v in policy_data}.values())
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
            writer.writeheader()
            writer.writerows(unique_data)
        print(f"âœ… [ê¸°ì ] ë“œë””ì–´ {len(unique_data)}ê±´ì˜ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤!")
    else:
        # ì´ëž˜ë„ ì•ˆ ë‚˜ì˜¤ë©´, ì‚¬ì´íŠ¸ê°€ ë´‡ì„ ì›ì²œ ë´‰ì‡„í•œ ê²ƒì´ë¯€ë¡œ 'RSS' XML ì†ŒìŠ¤ë¥¼ ê°•ì œë¡œ í…ìŠ¤íŠ¸ë¡œ ì½ìŠµë‹ˆë‹¤.
        print("ðŸš¨ ì›ë³¸ íŽ˜ì´ì§€ ì°¨ë‹¨ ì§€ì†. RSS XML í…ìŠ¤íŠ¸ ìˆ˜ë™ ë¶„í•´ ì‹œìž‘...")
        rss_res = requests.get("https://www.digital.go.jp/rss/news.xml", verify=False)
        rss_matches = re.findall(r'<title>(.*?)</title>.*?<link>(.*?)</link>', rss_res.text, re.S)
        
        for r_title, r_link in rss_matches:
            if '/press/' in r_link or '/news/' in r_link:
                policy_data.append({"date": "2026-RSS", "title": r_title, "link": r_link})
        
        if policy_data:
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                writer.writeheader()
                writer.writerows(policy_data)
            print(f"âœ… RSS ê°•ì œ ì¶”ì¶œë¡œ {len(policy_data)}ê±´ í™•ë³´ ì™„ë£Œ.")

if __name__ == "__main__":
    crawl_via_google_proxy()
