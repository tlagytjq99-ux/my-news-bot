import asyncio
from playwright.async_api import async_playwright
import csv

async def crawl_digital_2025_playwright_fixed():
    start_page = 21
    end_page = 188
    file_name = 'Japan_Digital_2025_Full_Archive.csv'
    all_data = []
    seen_links = set()

    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹¤í–‰
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page_obj = await context.new_page()

        print(f"ğŸš€ [ë¸Œë¼ìš°ì € ëª¨ë“œ] {start_page} ~ {end_page} í˜ì´ì§€ ì •ë°€ ìŠ¤ìº” ì‹œì‘...")

        for p_num in range(start_page, end_page + 1):
            url = f"https://www.digital.go.jp/news?page={p_num}"
            
            try:
                # í˜ì´ì§€ ì ‘ì† ë° ë„¤íŠ¸ì›Œí¬ ì•ˆì •í™” ëŒ€ê¸°
                await page_obj.goto(url, wait_until="domcontentloaded", timeout=60000)
                # ë°ì´í„° ë¡œë”©ì„ ìœ„í•´ ì•„ì£¼ ì ê¹ ëŒ€ê¸°
                await asyncio.sleep(1.5) 

                # [ìˆ˜ì • ì™„ë£Œ] results.append -> results.push ë¡œ ë³€ê²½
                links = await page_obj.evaluate("""
                    () => {
                        const results = [];
                        const anchors = document.querySelectorAll('a[href*="/news/"], a[href*="/press/"], a[href*="/policies/"]');
                        anchors.forEach(a => {
                            const text = a.innerText.trim();
                            if (text.length > 15) {
                                results.push({
                                    title: text.replace(/\\n/g, ' '),
                                    href: a.href
                                });
                            }
                        });
                        return results;
                    }
                """)

                for link in links:
                    if link['href'] not in seen_links:
                        seen_links.add(link['href'])
                        all_data.append({
                            "title": link['title'],
                            "link": link['href']
                        })
                
                print(f"ğŸ“¡ {p_num}/{end_page} ì™„ë£Œ | í˜„ì¬ ëˆ„ì : {len(all_data)}ê±´", end='\r')

            except Exception as e:
                print(f"\nâŒ {p_num}í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {str(e)[:100]}")
                continue

        await browser.close()

    # CSV ì €ì¥ (UTF-8-SIGë¡œ ì—‘ì…€ í•œê¸€/ì¼ì–´ ê¹¨ì§ ë°©ì§€)
    if all_data:
        # ë‚ ì§œìˆœ ì •ë ¬ ì‹œë„ (íƒ€ì´í‹€ ì•ì— ë‚ ì§œê°€ ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ)
        all_data.sort(key=lambda x: x['title'], reverse=True)
        
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["title", "link"])
            writer.writeheader()
            writer.writerows(all_data)
        print(f"\n\nâœ… [ì„ë¬´ ì™„ìˆ˜] ì´ {len(all_data)}ê±´ì˜ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(crawl_digital_2025_playwright_fixed())
