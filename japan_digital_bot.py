import requests
import re
import csv
import os

def crawl_digital_agency_brute_force():
    # ì •ì±… ì¹´í…Œê³ ë¦¬ (Category 1)
    url = "https://www.digital.go.jp/press?category=1"
    file_name = 'Japan_Digital_Policy_2025.csv'
    
    # ë´‡ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•œ ì‹¤ì œ ë¸Œë¼ìš°ì €ì™€ ìœ ì‚¬í•œ í—¤ë”
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7,ja;q=0.6',
    }

    print(f"ğŸš€ [ìµœì¢… ëª¨ë“œ] {url}ì—ì„œ ë°ì´í„°ë¥¼ ê°•ì œ ì¶”ì¶œí•©ë‹ˆë‹¤...")

    try:
        response = requests.get(url, headers=headers, timeout=30)
        html_content = response.text
        
        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ /press/ë¡œ ì‹œì‘í•˜ëŠ” ì •ì±… ë§í¬ì™€ ì œëª© íŒ¨í„´ì„ ê°•ì œë¡œ ë‚šì•„ì±•ë‹ˆë‹¤.
        # ì¼ë³¸ ì‚¬ì´íŠ¸ íŠ¹ìœ ì˜ href="/press/xxxx" êµ¬ì¡°ë¥¼ íƒ€ê²ŸíŒ…
        pattern = r'href="(/press/[a-zA-Z0-9\-_]+)"[^>]*>(.*?)</a>'
        matches = re.findall(pattern, html_content)

        policy_results = []
        for link, title in matches:
            # HTML íƒœê·¸ ì œê±° ë° ì •ì œ
            clean_title = re.sub(r'<[^>]+>', '', title).strip()
            
            # ë©”ë‰´ë‚˜ ë¶ˆí•„ìš”í•œ ë§í¬ ì œì™¸ (ê¸€ì ìˆ˜ ê¸°ì¤€)
            if len(clean_title) < 10 or "ä¸€è¦§" in clean_title:
                continue
                
            policy_results.append({
                "date": "2025/2026",
                "title": clean_title,
                "link": "https://www.digital.go.jp" + link
            })

        # ê²°ê³¼ ì €ì¥
        if policy_results:
            # ì¤‘ë³µ ì œê±°
            unique_data = list({v['link']: v for v in policy_results}.values())
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                writer.writeheader()
                writer.writerows(unique_data)
            print(f"âœ… ë“œë””ì–´ ì„±ê³µ! {len(unique_data)}ê±´ì˜ ì •ì±… ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
        else:
            # RSS í”¼ë“œ ë°±ì—… ëª¨ë“œ ê°€ë™
            print("âš ï¸ ì›¹ í˜ì´ì§€ ì°¨ë‹¨ ê°ì§€. RSS ë°±ì—… ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
            rss_res = requests.get("https://www.digital.go.jp/rss/news.xml", headers=headers)
            rss_matches = re.findall(r'<item>.*?<title>(.*?)</title>.*?<link>(.*?)</link>', rss_res.text, re.S)
            
            for r_title, r_link in rss_matches:
                if '/press/' in r_link:
                    policy_results.append({"date": "RSS", "title": r_title, "link": r_link})
            
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                writer.writeheader()
                writer.writerows(policy_results)
            print(f"âœ… RSS ë°±ì—…ìœ¼ë¡œ {len(policy_results)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ.")

    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        # ë¹ˆ íŒŒì¼ì´ë¼ë„ ìƒì„±í•˜ì—¬ Actions ì‹¤íŒ¨ ë°©ì§€
        if not os.path.exists(file_name):
            with open(file_name, 'w', encoding='utf-8-sig') as f:
                f.write("date,title,link\n")

if __name__ == "__main__":
    crawl_digital_agency_brute_force()
