import requests
from bs4 import BeautifulSoup
import csv
import time

def crawl_digital_2025_fixed_range():
    # ëŒ€í‘œë‹˜ì´ í™•ì¸í•´ì£¼ì‹  2025ë…„ êµ¬ê°„
    start_page = 21
    end_page = 188
    
    file_name = 'Japan_Digital_2025_Full_Archive.csv'
    base_url = "https://www.digital.go.jp/news?page="
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }
    
    all_data = []

    print(f"ğŸš€ [ì •ë°€ íƒ€ê²©] Page {start_page}ë¶€í„° {end_page}ê¹Œì§€ 2025ë…„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")

    for page in range(start_page, end_page + 1):
        url = f"{base_url}{page}"
        print(f"ğŸ“¡ ìŠ¤ìº” ì¤‘: {page}/{end_page} í˜ì´ì§€...", end='\r')
        
        try:
            res = requests.get(url, headers=headers, timeout=20)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ë””ì§€í„¸ì²­ì˜ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ (ecl-card í´ë˜ìŠ¤ ê¸°ë°˜)
            articles = soup.select('div.ecl-card') or soup.select('article')
            
            # ë§Œì•½ ì„ íƒìê°€ ì•ˆ ì¡í ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ <a> íƒœê·¸ ì§ì ‘ ì¶”ì 
            if not articles:
                # ë§í¬ì™€ ë‚ ì§œë¥¼ í¬í•¨í•˜ëŠ” ê°€ì¥ ê°€ê¹Œìš´ ë¶€ëª¨ ìš”ì†Œë¥¼ ì°¾ìŒ
                articles = soup.find_all('li') 

            for item in articles:
                link_tag = item.find('a')
                date_tag = item.find('time')
                
                if link_tag and date_tag:
                    title = link_tag.get_text(strip=True)
                    date = date_tag.get('datetime') or date_tag.get_text(strip=True)
                    href = link_tag['href']
                    
                    # 2025ë…„ ë°ì´í„°ì¸ì§€ í•œ ë²ˆ ë” ê²€ì¦ (ì•ˆì „ì¥ì¹˜)
                    if "2025" in date or "2025" in title:
                        all_data.append({
                            "date": date[:10],
                            "title": title,
                            "link": "https://www.digital.go.jp" + href if href.startswith('/') else href
                        })

            # ì„œë²„ ë¶€í•˜ë¥¼ ê³ ë ¤í•´ 0.3ì´ˆì”© íœ´ì‹
            if page % 10 == 0:
                time.sleep(1)
                print(f"\nâœ¨ {page}í˜ì´ì§€ê¹Œì§€ ëˆ„ì  {len(all_data)}ê±´ í™•ë³´...")

        except Exception as e:
            print(f"\nâŒ {page}í˜ì´ì§€ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    # ë°ì´í„° ì €ì¥ (ì¤‘ë³µ ì œê±° í¬í•¨)
    if all_data:
        unique_data = list({v['link']: v for v in all_data}.values())
        # ë‚ ì§œìˆœ ì •ë ¬
        unique_data.sort(key=lambda x: x['date'], reverse=True)
        
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
            writer.writeheader()
            writer.writerows(unique_data)
        print(f"\n\nâœ… ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(unique_data)}ê±´ì˜ 2025ë…„ ì •ì±… ìë£Œë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“‚ íŒŒì¼ëª…: {file_name}")
    else:
        print("\nâš ï¸ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì„ íƒì(Selector)ë¥¼ ë‹¤ì‹œ ì ê²€í•´ì•¼ í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    crawl_digital_2025_fixed_range()
