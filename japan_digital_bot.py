import requests
from bs4 import BeautifulSoup
import csv
import os

def crawl_digital_agency_2026():
    # ì¼ë¬¸ ë³´ë„ìë£Œ í˜ì´ì§€
    url = "https://www.digital.go.jp/news/press"
    file_name = 'Japan_Digital_Policy_2025.csv'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }

    print("ğŸ¯ [ë°ì´í„° ì •ë°€ ì¶”ì ] ì¼ë³¸ ë””ì§€í„¸ì²­ ìŠ¤ìº” ì¤‘...")

    try:
        res = requests.get(url, headers=headers, timeout=20)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        # ë””ì§€í„¸ì²­ ë¦¬ìŠ¤íŠ¸ì˜ ì‹¤ì œ êµ¬ì¡°: article íƒœê·¸ ë˜ëŠ” íŠ¹ì • í´ë˜ìŠ¤ ë‚´ì˜ a íƒœê·¸
        # ë” ë„“ì€ ë²”ìœ„ë¡œ ì°¾ê¸° ìœ„í•´ h3ì™€ ì—°ê²°ëœ ë§í¬ë¥¼ íƒ€ê²ŸíŒ…í•©ë‹ˆë‹¤.
        items = soup.find_all('a') 

        policy_data = []
        for a in items:
            # ì œëª©ê³¼ ë‚ ì§œê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = a.get_text(strip=True)
            href = a.get('href', '')
            
            # 2025ë…„ ë˜ëŠ” 2026ë…„ ë‚ ì§œ í˜•ì‹ì´ í¬í•¨ëœ ë‰´ìŠ¤ ë§í¬ë§Œ í•„í„°ë§
            if href.startswith('/news/') and any(yr in text for yr in ['2025', '2026', 'ä»¤å’Œ7', 'ä»¤å’Œ8']):
                policy_data.append({
                    "date": text[:10], # ì•ë¶€ë¶„ ë‚ ì§œë§Œ ëŒ€ëµ ì¶”ì¶œ
                    "title": text[10:].strip(),
                    "link": "https://www.digital.go.jp" + href if href.startswith('/') else href
                })

        # [ì¤‘ìš”] ì¤‘ë³µ ì œê±° ë° ì €ì¥
        unique_data = list({v['link']: v for v in policy_data}.values())

        # íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠëŠ” ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ë¬´ì¡°ê±´ ìƒì„± í”„ë¡œì„¸ìŠ¤ ê°€ë™
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
            writer.writeheader()
            if unique_data:
                writer.writerows(unique_data)
                print(f"âœ… {len(unique_data)}ê±´ì˜ ë°ì´í„°ë¥¼ íŒŒì¼ì— ì¼ìŠµë‹ˆë‹¤.")
            else:
                print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì—ëŸ¬ê°€ ë‚˜ë„ ë¹ˆ íŒŒì¼ì„ ë§Œë“¤ì–´ì•¼ ë‹¤ìŒ ê¹ƒ ë‹¨ê³„ê°€ ê¹¨ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
        if not os.path.exists(file_name):
            with open(file_name, 'w', encoding='utf-8-sig') as f:
                f.write("date,title,link\n")

if __name__ == "__main__":
    crawl_digital_agency_2026()
