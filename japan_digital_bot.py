import requests
from bs4 import BeautifulSoup
import csv
import xml.etree.ElementTree as ET
from datetime import datetime

def crawl_digital_agency_policy_only():
    # ì •ì±…(Category=1) í•„í„°ê°€ ì ìš©ëœ ì£¼ì†Œ
    policy_url = "https://www.digital.go.jp/press?category=1"
    file_name = 'Japan_Digital_Policy_2025.csv'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }

    policy_results = []
    print(f"ğŸ¯ [ì •ì±… íŒŒíŠ¸ ì •ë°€ ì¶”ì¶œ] {policy_url} ìŠ¤ìº” ì‹œì‘...")

    try:
        # 1. ì›¹ í˜ì´ì§€ ìŠ¤ìºë‹ (ê³¼ê±°~í˜„ì¬ ì •ì±… ë¦¬ìŠ¤íŠ¸)
        res = requests.get(policy_url, headers=headers, timeout=20)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        # ë””ì§€í„¸ì²­ ë³´ë„ìë£Œ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì¶”ì¶œ
        # ê¸°ì‚¬ëŠ” ë³´í†µ article íƒœê·¸ë‚˜ íŠ¹ì • ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ì•ˆì— ìˆìŠµë‹ˆë‹¤.
        articles = soup.find_all(['article', 'div'], class_=lambda x: x and 'ecl-card' in x) or soup.find_all('a', href=True)

        for item in articles:
            href = item.get('href') if item.name == 'a' else (item.find('a')['href'] if item.find('a') else None)
            if not href or '/press/' not in href: continue

            title = item.get_text(strip=True)
            # ë©”ë‰´ë‚˜ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ í•„í„°ë§
            if len(title) < 10 or "ä¸€è¦§" in title: continue

            full_link = "https://www.digital.go.jp" + href if href.startswith('/') else href
            
            policy_results.append({
                "date": datetime.now().strftime("%Y-%m-%d"), # í˜ì´ì§€ì—” ì—°ë„ í‘œê¸°ê°€ ìƒëµë  ìˆ˜ ìˆì–´ ìˆ˜ì§‘ì¼ ê¸°ì¤€
                "title": title,
                "link": full_link
            })

        # 2. RSS í”¼ë“œ êµì°¨ ê²€ì¦ (ìµœì‹ ì„± í™•ë³´)
        try:
            rss_res = requests.get("https://www.digital.go.jp/rss/news.xml", timeout=10)
            root = ET.fromstring(rss_res.content)
            for entry in root.findall('.//item'):
                rss_link = entry.find('link').text
                # RSS ë°ì´í„° ì¤‘ ì •ì±…(press) ê´€ë ¨ ë§í¬ë§Œ ì„ ë³„
                if '/press/' in rss_link:
                    policy_results.append({
                        "date": "RSS_Latest",
                        "title": entry.find('title').text,
                        "link": rss_link
                    })
        except:
            print("âš ï¸ RSS êµì°¨ ê²€ì¦ì€ ê±´ë„ˆëœë‹ˆë‹¤.")

        # 3. ë°ì´í„° ì •ì œ ë° ì €ì¥
        if policy_results:
            # ë§í¬ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
            unique_policies = list({v['link']: v for v in policy_results}.values())
            
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                writer.writeheader()
                writer.writerows(unique_policies)
            print(f"âœ… ì¶”ì¶œ ì„±ê³µ! ì´ {len(unique_policies)}ê±´ì˜ ì •ì±… ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
        else:
            # ë¹ˆ íŒŒì¼ ìƒì„± (Git ì—ëŸ¬ ë°©ì§€)
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                f.write("date,title,link\n")
            print("âš ï¸ ì¡°ê±´ì— ë§ëŠ” ì •ì±…ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì¤‘ëª… ì˜¤ë¥˜: {e}")
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            f.write("date,title,link\n")

if __name__ == "__main__":
    crawl_digital_agency_policy_only()
