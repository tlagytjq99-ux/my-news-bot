import requests
from bs4 import BeautifulSoup
import csv
import time

def crawl_digital_2025_bulk_scan():
    start_page = 21
    end_page = 188
    file_name = 'Japan_Digital_2025_Full_Archive.csv'
    base_url = "https://www.digital.go.jp/news?page="
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8'
    }
    
    all_data = []
    seen_links = set()

    print(f"ğŸš€ [ë²Œí¬ ìŠ¤ìº”] Page {start_page} ~ {end_page} êµ¬ê°„ì˜ ëª¨ë“  ê¸°ì‚¬ ë¸”ë¡ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

    for page in range(start_page, end_page + 1):
        url = f"{base_url}{page}"
        print(f"ğŸ“¡ {page}/{end_page} í˜ì´ì§€ ì •ë°€ ë¶„í•´ ì¤‘... (í˜„ì¬ {len(all_data)}ê±´)", end='\r')
        
        try:
            res = requests.get(url, headers=headers, timeout=20)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ë””ì§€í„¸ì²­ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì„ ê°ì‹¸ëŠ” ëª¨ë“  ê°€ëŠ¥ì„± ìˆëŠ” íƒœê·¸ë“¤
            # ì¹´ë“œ í˜•íƒœ(ecl-card), ë¦¬ìŠ¤íŠ¸ í˜•íƒœ(li), ì¼ë°˜ ë¸”ë¡(div)ì„ ëª¨ë‘ ì¡ìŠµë‹ˆë‹¤.
            items = soup.select('.ecl-card, .ecl-list-item, article, li')
            
            for item in items:
                link_tag = item.find('a', href=True)
                if not link_tag:
                    continue
                
                href = link_tag['href']
                # ë³´ë„ìë£Œ, ë‰´ìŠ¤, ì •ì±… ë“± í•µì‹¬ ì½˜í…ì¸  ì£¼ì†Œ í™•ì¸
                if not any(path in href for path in ['/news/', '/press/', '/policies/', '/announcement/']):
                    continue
                
                full_url = "https://www.digital.go.jp" + href if href.startswith('/') else href
                
                if full_url not in seen_links:
                    # ë¸”ë¡ ì „ì²´ì˜ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ì„œ ì œëª©ìœ¼ë¡œ ì‚¬ìš© (ë‚ ì§œ í¬í•¨ë¨)
                    # separatorë¥¼ ì£¼ì–´ í…ìŠ¤íŠ¸ê°€ ë­‰ì¹˜ì§€ ì•Šê²Œ í•©ë‹ˆë‹¤.
                    title_content = item.get_text(separator=" ", strip=True)
                    
                    # ë©”ë‰´ í•­ëª©ì´ë‚˜ ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ í•„í„°ë§
                    if len(title_content) < 15:
                        continue
                        
                    seen_links.add(full_url)
                    all_data.append({
                        "title": title_content,
                        "link": full_url
                    })

            if page % 30 == 0:
                time.sleep(1)

        except Exception as e:
            continue

    # ë°ì´í„° ì €ì¥
    if all_data:
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["title", "link"])
            writer.writeheader()
            writer.writerows(all_data)
        print(f"\n\nâœ… ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(all_data)}ê±´ì˜ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    crawl_digital_2025_bulk_scan()
