import requests
from bs4 import BeautifulSoup
import csv
import time

def crawl_digital_2025_ultimate_archive():
    start_page = 21
    end_page = 188
    file_name = 'Japan_Digital_2025_Full_Archive.csv'
    base_url = "https://www.digital.go.jp/news?page="
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }
    
    all_data = []

    print(f"ğŸš€ [ì „ìˆ˜ ì¡°ì‚¬] Page {start_page} ~ {end_page}ì˜ ëª¨ë“  2025ë…„ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...")

    for page in range(start_page, end_page + 1):
        url = f"{base_url}{page}"
        print(f"ğŸ“¡ {page}/{end_page} í˜ì´ì§€ ì •ë°€ ìŠ¤ìº” ì¤‘... (í˜„ì¬ {len(all_data)}ê±´ í™•ë³´)", end='\r')
        
        try:
            res = requests.get(url, headers=headers, timeout=20)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # íŠ¹ì • í´ë˜ìŠ¤ì— ì–½ë§¤ì´ì§€ ì•Šê³ , ë‚ ì§œ(time íƒœê·¸)ê°€ í¬í•¨ëœ ëª¨ë“  êµ¬ì—­ì„ íƒ€ê²ŸíŒ…í•©ë‹ˆë‹¤.
            # 1. ëª¨ë“  time íƒœê·¸ë¥¼ ì°¾ì•„ì„œ ê·¸ ë¶€ëª¨ ìš”ì†Œë“¤ë¡œë¶€í„° ì •ë³´ë¥¼ ì¶”ì¶œ
            time_tags = soup.find_all('time')
            
            for time_tag in time_tags:
                date_text = time_tag.get_text(strip=True)
                
                # 2025ë…„ ë°ì´í„°ì¸ì§€ ê²€ì¦
                if "2025" in date_text:
                    # í•´ë‹¹ ë‚ ì§œ ê·¼ì²˜ì— ìˆëŠ” ê°€ì¥ ê°€ê¹Œìš´ ë§í¬(a íƒœê·¸)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
                    # ë¶€ëª¨ ìš”ì†Œë¥¼ íƒ€ê³  ì˜¬ë¼ê°€ë©° ë§í¬ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤.
                    parent = time_tag.parent
                    link_tag = None
                    
                    # ìµœëŒ€ 5ë‹¨ê³„ ë¶€ëª¨ê¹Œì§€ ì˜¬ë¼ê°€ë©° ë§í¬ íƒìƒ‰
                    for _ in range(5):
                        if parent:
                            link_tag = parent.find('a', href=True)
                            if link_tag: break
                            parent = parent.parent
                    
                    if link_tag:
                        title = link_tag.get_text(strip=True)
                        href = link_tag['href']
                        
                        # ë©”ë‰´ ë§í¬ë‚˜ ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
                        if len(title) < 10 or href.startswith('#'): continue
                        
                        full_url = "https://www.digital.go.jp" + href if href.startswith('/') else href
                        all_data.append({
                            "date": date_text[:10],
                            "title": title,
                            "link": full_url
                        })

            if page % 20 == 0:
                time.sleep(1) # ê³¼ë¶€í•˜ ë°©ì§€

        except Exception as e:
            print(f"\nâŒ {page}í˜ì´ì§€ ìŠ¤ìº” ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    # ë°ì´í„° ì •ì œ ë° ì €ì¥
    if all_data:
        # ë§í¬ ì¤‘ë³µ ì œê±°
        unique_data = list({v['link']: v for v in all_data}.values())
        # ë‚ ì§œìˆœ ì •ë ¬
        unique_data.sort(key=lambda x: x['date'], reverse=True)
        
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
            writer.writeheader()
            writer.writerows(unique_data)
        print(f"\n\nâœ… ìˆ˜ì§‘ ì™„ë£Œ! {len(unique_data)}ê±´ì˜ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    crawl_digital_2025_ultimate_archive()
