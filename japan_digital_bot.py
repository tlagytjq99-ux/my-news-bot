import requests
import re
import csv

def crawl_digital_agency_proxy():
    file_name = 'Japan_Digital_Policy_2025.csv'
    
    # ë””ì§€í„¸ì²­ URLì„ êµ¬ê¸€ ë²ˆì—­ê¸° í”„ë¡ì‹œ ì£¼ì†Œë¡œ ë³€í™˜ (ì°¨ë‹¨ ìš°íšŒìš©)
    proxy_url = "https://www.digital-go-jp.translate.goog/press?category=1&_x_tr_sl=ja&_x_tr_tl=ko"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }

    print(f"ðŸš€ [í”„ë¡ì‹œ ìš°íšŒ ëª¨ë“œ] êµ¬ê¸€ ì„œë²„ë¥¼ í†µí•´ ë””ì§€í„¸ì²­ì— ì ‘ì†í•©ë‹ˆë‹¤...")

    try:
        # êµ¬ê¸€ í”„ë¡ì‹œë¥¼ í†µí•´ HTML ê°€ì ¸ì˜¤ê¸°
        response = requests.get(proxy_url, headers=headers, timeout=30)
        html_content = response.text
        
        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë§í¬ì™€ ì œëª© ì¶”ì¶œ
        # êµ¬ê¸€ í”„ë¡ì‹œë¥¼ íƒ€ë©´ URL êµ¬ì¡°ê°€ ì‚´ì§ ë³€í•˜ë¯€ë¡œ ë²”ìš© íŒ¨í„´ ì‚¬ìš©
        pattern = r'href="([^"]*digital-go-jp\.translate\.goog/press/[^"]*)"[^>]*>(.*?)</a>'
        matches = re.findall(pattern, html_content)

        policy_results = []
        for link, title in matches:
            clean_title = re.sub(r'<[^>]+>', '', title).strip()
            if len(clean_title) < 10: continue
            
            # ì›ë³¸ ì£¼ì†Œë¡œ ë³µì›
            original_link = link.split('?')[0].replace('.translate.goog', '').replace('-', '.')
            
            policy_results.append({
                "date": "2025/2026",
                "title": clean_title,
                "link": original_link
            })

        if policy_results:
            unique_data = list({v['link']: v for v in policy_results}.values())
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                writer.writeheader()
                writer.writerows(unique_data)
            print(f"âœ… ë“œë””ì–´ ëš«ì—ˆìŠµë‹ˆë‹¤! {len(unique_data)}ê±´ ìˆ˜ì§‘ ì„±ê³µ.")
        else:
            print("âš ï¸ í”„ë¡ì‹œ ìš°íšŒë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìµœì¢… ìˆ˜ë‹¨ì¸ 'ê°€ìƒ ë¸Œë¼ìš°ì €'ë¡œ ë„˜ì–´ê°€ì•¼ í•©ë‹ˆë‹¤.")
            # ë¹ˆ íŒŒì¼ ìƒì„±
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                f.write("date,title,link\n")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        with open(file_name, 'w', encoding='utf-8-sig') as f:
            f.write("date,title,link\n")

if __name__ == "__main__":
    crawl_digital_agency_proxy()
