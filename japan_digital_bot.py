import requests
from bs4 import BeautifulSoup
import csv
import os
from datetime import datetime

def crawl_digital_agency_final():
    url = "https://www.digital.go.jp/news/press"
    file_name = 'Japan_Digital_Policy_2025.csv'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }

    print(f"ğŸ¯ [ì •ë°€ ìŠ¤ìº”] {datetime.now().year}ë…„ ìµœì‹  ì •ì±… ë°ì´í„°ë¥¼ ë‚šì•„ì±•ë‹ˆë‹¤...")

    try:
        res = requests.get(url, headers=headers, timeout=20)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        # ë””ì§€í„¸ì²­ì˜ ì‹¤ì œ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ëŠ” 'article' íƒœê·¸ë¡œ ê°ì‹¸ì ¸ ìˆìŠµë‹ˆë‹¤.
        articles = soup.find_all('article')
        
        policy_data = []
        for item in articles:
            # 1. ì œëª©ê³¼ ë§í¬ ì°¾ê¸°
            link_tag = item.find('a')
            if not link_tag: continue
            
            title = link_tag.get_text(strip=True)
            href = link_tag.get('href', '')
            link = "https://www.digital.go.jp" + href if href.startswith('/') else href

            # 2. ë‚ ì§œ ì°¾ê¸° (time íƒœê·¸ í˜¹ì€ íŠ¹ì • í´ë˜ìŠ¤)
            date_tag = item.find('time')
            date_text = date_tag.get_text(strip=True) if date_tag else ""

            # 3. 2025ë…„ í˜¹ì€ 2026ë…„ ë°ì´í„°ì¸ì§€ ê²€ì¦
            # ì¼ë³¸ ì—°í˜¸(ä»¤å’Œ7, ä»¤å’Œ8)ì™€ ì„œê¸°ë¥¼ ëª¨ë‘ ì²´í¬í•©ë‹ˆë‹¤.
            target_years = ['2025', '2026', 'ä»¤å’Œ7', 'ä»¤å’Œ8', 'R7', 'R8']
            if any(yr in date_text or yr in title for yr in target_years):
                policy_data.append({
                    "date": date_text,
                    "title": title,
                    "link": link
                })

        # ê²°ê³¼ ì €ì¥
        if policy_data:
            # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
            unique_data = list({v['link']: v for v in policy_data}.values())
            
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                writer.writeheader()
                writer.writerows(unique_data)
            print(f"âœ… ë“œë””ì–´ ì„±ê³µ! {len(unique_data)}ê±´ì˜ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
            print(f"ğŸ“Œ ìƒ˜í”Œ ì œëª©: {unique_data[0]['title'][:30]}...")
        else:
            # ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°, ê¹ƒ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ í—¤ë”ë§Œ ìˆëŠ” íŒŒì¼ ìƒì„±
            print("âš ï¸ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í•„í„°ë¥¼ ì™„í™”í•˜ì—¬ ë¹ˆ íŒŒì¼ì„ ìœ ì§€í•©ë‹ˆë‹¤.")
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                writer.writeheader()

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # íŒŒì¼ì´ ì•„ì˜ˆ ì•ˆ ë§Œë“¤ì–´ì§€ë©´ Git Pushê°€ ê¹¨ì§€ë¯€ë¡œ ë¹ˆ íŒŒì¼ ê°•ì œ ìƒì„±
        if not os.path.exists(file_name):
            with open(file_name, 'w', encoding='utf-8-sig') as f:
                f.write("date,title,link\n")

if __name__ == "__main__":
    crawl_digital_agency_final()
