import asyncio
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import csv
import os

def crawl_with_browser():
    url = "https://www.digital.go.jp/press?category=1"
    file_name = 'Japan_Digital_Policy_2025.csv'
    
    print("ğŸš€ [ë¸Œë¼ìš°ì € ê°€ë™] ì‹¤ì œ í™”ë©´ì„ ë Œë”ë§í•˜ì—¬ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...")

    with sync_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹¤í–‰ (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ)
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        try:
            # í˜ì´ì§€ ì ‘ì† ë° ëŒ€ê¸°
            page.goto(url, wait_until="networkidle", timeout=60000)
            # ìë°”ìŠ¤í¬ë¦½íŠ¸ê°€ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ì‹œê°„ì„ ì¤ë‹ˆë‹¤.
            page.wait_for_timeout(5000) 
            
            # ë Œë”ë§ëœ HTML ê°€ì ¸ì˜¤ê¸°
            content = page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # /press/ ë§í¬ ì¶”ì¶œ
            links = soup.find_all('a', href=True)
            policy_data = []

            for a in links:
                href = a['href']
                if '/press/' in href:
                    title = a.get_text(strip=True)
                    if len(title) < 10: continue
                    
                    policy_data.append({
                        "date": "2025/2026",
                        "title": title,
                        "link": "https://www.digital.go.jp" + href if href.startswith('/') else href
                    })

            if policy_data:
                unique_data = list({v['link']: v for v in policy_data}.values())
                with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                    writer.writeheader()
                    writer.writerows(unique_data)
                print(f"âœ… [ëŒ€ì„±ê³µ] ë¸Œë¼ìš°ì € ìš°íšŒë¡œ {len(unique_data)}ê±´ì˜ ì •ì±…ì„ ì°¾ì•„ëƒˆìŠµë‹ˆë‹¤!")
            else:
                print("âŒ ë¸Œë¼ìš°ì €ì—ì„œë„ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì„ íƒì ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                    f.write("date,title,link\n")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                f.write("date,title,link\n")
        finally:
            browser.close()

if __name__ == "__main__":
    crawl_with_browser()
