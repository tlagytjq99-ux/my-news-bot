import os
import csv
import re
from datetime import datetime

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í™•ì¸ (ì—ëŸ¬ ë°©ì§€ìš©)
try:
    import requests
    import xml.etree.ElementTree as ET
except ImportError:
    print("âŒ ì—ëŸ¬: requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. YAML íŒŒì¼ì—ì„œ pip install requestsë¥¼ ìˆ˜í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)

def crawl_japan_digital_final():
    file_name = 'Japan_Digital_Policy_2025.csv'
    # ì •ì±… ì¹´í…Œê³ ë¦¬ (Category 1)
    url = "https://www.digital.go.jp/press?category=1"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }

    print("ğŸš€ [ìµœì¢… ì ê²€] ë””ì§€í„¸ì²­ ì •ì±… ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    policy_data = []

    try:
        # 1. RSS í”¼ë“œ ë¨¼ì € ì‹œë„ (ê°€ì¥ ê¹”ë”í•œ ë°ì´í„° ì†ŒìŠ¤)
        print("ğŸ“¡ RSS í”¼ë“œ ë¶„ì„ ì¤‘...")
        rss_res = requests.get("https://www.digital.go.jp/rss/news.xml", timeout=15)
        if rss_res.status_code == 200:
            root = ET.fromstring(rss_res.content)
            for item in root.findall('.//item'):
                link = item.find('link').text
                if '/press/' in link:
                    policy_data.append({
                        "date": datetime.now().strftime("%Y-%m-%d"),
                        "title": item.find('title').text,
                        "link": link
                    })

        # 2. ì›¹ í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ ì§ì ‘ íŒ¨í„´ ë‚šì•„ì±„ê¸° (RSSì— ì—†ëŠ” ê³¼ê±° ë°ì´í„°ìš©)
        print("ğŸ¯ ì›¹ í˜ì´ì§€ ì•„ì¹´ì´ë¸Œ ìŠ¤ìº” ì¤‘...")
        web_res = requests.get(url, headers=headers, timeout=15)
        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë§í¬ì™€ ì œëª© ê°•ì œ ì¶”ì¶œ (HTML êµ¬ì¡°ê°€ ê¹¨ì ¸ë„ ì‘ë™)
        matches = re.findall(r'href="(/press/[^"]+)"[^>]*>(.*?)</a>', web_res.text)
        
        for link, title in matches:
            clean_title = re.sub(r'<[^>]+>', '', title).strip()
            if len(clean_title) > 10:
                policy_data.append({
                    "date": "2025-Policy",
                    "title": clean_title,
                    "link": "https://www.digital.go.jp" + link
                })

        # 3. ë°ì´í„° ì €ì¥ (ì¤‘ë³µ ì œê±°)
        if policy_data:
            unique_data = list({v['link']: v for v in policy_data}.values())
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                writer.writeheader()
                writer.writerows(unique_data)
            print(f"âœ… ëŒ€ì„±ê³µ! {len(unique_data)}ê±´ì˜ ì •ì±… ë°ì´í„°ë¥¼ íŒŒì¼ì— ë‹´ì•˜ìŠµë‹ˆë‹¤.")
        else:
            # ë¹ˆ íŒŒì¼ ìƒì„± (Git Push ì—ëŸ¬ ë°©ì§€ìš©)
            print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ ë¹ˆ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                f.write("date,title,link\n")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if not os.path.exists(file_name):
            with open(file_name, 'w', encoding='utf-8-sig') as f:
                f.write("date,title,link\n")

if __name__ == "__main__":
    crawl_japan_digital_final()
