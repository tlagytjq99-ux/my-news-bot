import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime

def crawl_japan_digital_agency():
    # ì¼ë¬¸ ë³´ë„ìë£Œ í˜ì´ì§€ (ê°€ì¥ ë¹ ë¥´ê³  ì •í™•í•¨)
    url = "https://www.digital.go.jp/news/press"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    print(f"ğŸš€ [ì¼ë³¸ ë””ì§€í„¸ì²­] 2025ë…„ ì •ì±… ìˆ˜ì§‘ ì‹œì‘: {datetime.now()}")

    try:
        res = requests.get(url, headers=headers)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        # ë³´ë„ìë£Œ ì•„ì´í…œ ì¶”ì¶œ (ë””ì§€í„¸ì²­ íŠ¹ìœ ì˜ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ë°˜ì˜)
        # ê° ê¸°ì‚¬ëŠ” ë³´í†µ ecl-card ë˜ëŠ” íŠ¹ì • ë¦¬ìŠ¤íŠ¸ í´ë˜ìŠ¤ ì•ˆì— ìˆìŠµë‹ˆë‹¤.
        articles = soup.select('a.ecl-link') 

        policy_data = []
        for article in articles:
            # ì œëª© ì¶”ì¶œ
            title_tag = article.find(['h2', 'h3'])
            if not title_tag: continue
            title = title_tag.get_text(strip=True)

            # ë§í¬ ì¶”ì¶œ
            link = article['href']
            if not link.startswith('http'):
                link = "https://www.digital.go.jp" + link

            # ë‚ ì§œ ì¶”ì¶œ (ì¼ë³¸ì€ 2025ë…„ ë˜ëŠ” ä»¤å’Œ7å¹´ìœ¼ë¡œ í‘œê¸°ë¨)
            date_tag = article.find('time') or article.find('span', class_='date')
            date_text = date_tag.get_text(strip=True) if date_tag else ""

            # 2025ë…„ ë°ì´í„° í•„í„°ë§ (ì„œê¸° 2025ë…„ ë˜ëŠ” ì¼ë³¸ ì—°í˜¸ ä»¤å’Œ7å¹´/R7 í™•ì¸)
            if "2025" in date_text or "ä»¤å’Œ7" in date_text or "R7" in date_text:
                policy_data.append({
                    "date": date_text,
                    "title": title,
                    "link": link,
                    "collected_at": datetime.now().strftime("%Y-%m-%d")
                })

        # ë°ì´í„° ì €ì¥ (ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©í•˜ê±°ë‚˜ ìƒˆë¡œ ì“°ê¸°)
        if policy_data:
            keys = policy_data[0].keys()
            with open('Japan_Digital_Policy_2025.csv', 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=keys)
                writer.writeheader()
                writer.writerows(policy_data)
            print(f"âœ… ì„±ê³µ: {len(policy_data)}ê±´ì˜ ì •ì±…ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ ìƒˆë¡œìš´ 2025ë…„ ì •ì±…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    crawl_japan_digital_agency()
