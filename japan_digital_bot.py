import requests
from bs4 import BeautifulSoup
import csv
import time

def crawl_digital_2025_fixed_range_all():
    # ëŒ€í‘œë‹˜ì´ í™•ì •í•´ì£¼ì‹  2025ë…„ êµ¬ê°„
    start_page = 21
    end_page = 188
    
    file_name = 'Japan_Digital_2025_Full_Archive.csv'
    base_url = "https://www.digital.go.jp/news?page="
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }
    
    all_data = []
    seen_links = set() # ì¤‘ë³µ ë°©ì§€

    print(f"ğŸš€ [ì „ëŸ‰ ìˆ˜ì§‘] Page {start_page} ~ {end_page} êµ¬ê°„ì˜ ëª¨ë“  ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

    for page in range(start_page, end_page + 1):
        url = f"{base_url}{page}"
        print(f"ğŸ“¡ {page}/{end_page} í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘... (í˜„ì¬ ëˆ„ì  {len(all_data)}ê±´)", end='\r')
        
        try:
            res = requests.get(url, headers=headers, timeout=20)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # 1. ë‰´ìŠ¤ ì¹´ë“œ ì„¹ì…˜ íƒìƒ‰ (ê°€ì¥ ì •í™•í•œ ë°ì´í„° ì˜ì—­)
            # ì¹´ë“œ êµ¬ì¡°ê°€ ì•„ë‹ˆë”ë¼ë„ ëª¨ë“  ë‰´ìŠ¤ ë§í¬(/news/...)ë¥¼ ì¶”ì í•©ë‹ˆë‹¤.
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link['href']
                
                # ìœ íš¨í•œ ë‰´ìŠ¤/ê³µì§€ ë§í¬ íŒ¨í„´ë§Œ í•„í„°ë§
                if '/news/' in href or '/press/' in href or '/policies/' in href:
                    full_url = "https://www.digital.go.jp" + href if href.startswith('/') else href
                    
                    if full_url not in seen_links:
                        # ì œëª©ê³¼ ë‚ ì§œë¥¼ í¬í•¨í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        title_text = link.get_text(separator=" ", strip=True)
                        
                        # ë„ˆë¬´ ì§§ì€ ë§í¬(ë©”ë‰´ ë“±)ëŠ” ì œì™¸
                        if len(title_text) < 10:
                            continue
                            
                        seen_links.add(full_url)
                        all_data.append({
                            "content": title_text,
                            "url": full_url
                        })

            # 20í˜ì´ì§€ë§ˆë‹¤ ì§§ì€ íœ´ì‹ (ì°¨ë‹¨ ë°©ì§€)
            if page % 20 == 0:
                time.sleep(0.5)

        except Exception as e:
            print(f"\nâŒ {page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘ ê±´ë„ˆëœ€: {e}")
            continue

    # ë°ì´í„° ì €ì¥
    if all_data:
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["content", "url"])
            writer.writeheader()
            writer.writerows(all_data)
        print(f"\n\nâœ… ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(all_data)}ê±´ì˜ 2025ë…„ êµ¬ê°„ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“‚ íŒŒì¼ëª…: {file_name}")
    else:
        print("\nâš ï¸ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. êµ¬ê°„ ì„¤ì •ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    crawl_digital_2025_fixed_range_all()
