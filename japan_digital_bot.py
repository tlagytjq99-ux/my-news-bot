import requests
import re
import csv
from datetime import datetime

def crawl_japan_digital_ultimate():
    file_name = 'Japan_Digital_Policy_2025.csv'
    # ë°ì´í„° ì†ŒìŠ¤ ë‹¤ê°í™” (ì „ì²´ ë‰´ìŠ¤ + ë³´ë„ìë£Œ)
    sources = [
        "https://www.digital.go.jp/rss/news.xml",
        "https://www.digital.go.jp/press?category=1" # ì •ì±… ì¹´í…Œê³ ë¦¬
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }

    print("ğŸ¦¾ [ë°ì´í„° í™•ì¥] ì •ì±… ë° ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í†µí•© ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    policy_data = []

    for url in sources:
        try:
            # SSL ê²€ì¦ ë¬´ì‹œë¡œ ì°¨ë‹¨ í™•ë¥  ìµœì†Œí™”
            res = requests.get(url, headers=headers, timeout=20, verify=False)
            content = res.text

            # 1. RSS í˜•ì‹ ë¶„ì„ (xml íŒ¨í„´)
            rss_items = re.findall(r'<item>.*?<title>(.*?)</title>.*?<link>(.*?)</link>', content, re.S)
            for t, l in rss_items:
                policy_data.append({"date": "RSS_Latest", "title": t, "link": l})

            # 2. ì¼ë°˜ HTML í˜•ì‹ ë¶„ì„ (href íŒ¨í„´)
            # /press/ í˜¹ì€ /news/ ê°€ í¬í•¨ëœ ëª¨ë“  2025-2026 ì •ì±… ë§í¬ ì¶”ì¶œ
            web_items = re.findall(r'href="([^"]*/(?:press|news)/[^"]*)"[^>]*>(.*?)</a>', content)
            for l, t in web_items:
                clean_title = re.sub(r'<[^>]+>', '', t).strip()
                if len(clean_title) > 10:
                    full_url = l if l.startswith('http') else "https://www.digital.go.jp" + l
                    policy_data.append({
                        "date": "2025-Policy",
                        "title": clean_title,
                        "link": full_url
                    })
        except:
            continue

    # ë°ì´í„° ì •ì œ ë° ì €ì¥
    if policy_data:
        # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
        unique_data = list({v['link']: v for v in policy_data}.values())
        
        # 2025ë…„/2026ë…„ í‚¤ì›Œë“œ í•„í„°ë§ (ì„ íƒ ì‚¬í•­)
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
            writer.writeheader()
            writer.writerows(unique_data)
        print(f"âœ¨ [ì„±ê³µ] ì´ {len(unique_data)}ê±´ì˜ ì •ì±…/ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸ ì¶”ê°€ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    crawl_japan_digital_ultimate()
