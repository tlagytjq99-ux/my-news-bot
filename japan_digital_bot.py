import requests
import csv
import os
from datetime import datetime

def crawl_digital_agency_api_direct():
    # ë””ì§€í„¸ì²­ì˜ ì‹¤ì œ ë°ì´í„°ê°€ ê³µê¸‰ë˜ëŠ” ë°±ì—”ë“œ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì¶”ì í•œ ì£¼ì†Œ
    # (ì¼ë°˜ í˜ì´ì§€ê°€ ì•„ë‹Œ ë°ì´í„° ì›ì²œì„ íƒ€ê²©í•©ë‹ˆë‹¤)
    url = "https://www.digital.go.jp/api/press_releases?category=1" 
    # ë§Œì•½ ìœ„ ì£¼ì†Œê°€ ë§‰í˜€ìˆë‹¤ë©´, ê°€ì¥ ì›ì´ˆì ì¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ í™œìš©í•©ë‹ˆë‹¤.
    search_url = "https://www.digital.go.jp/news/press"
    
    file_name = 'Japan_Digital_Policy_2025.csv'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/plain, */*', # JSON ë°ì´í„°ë¥¼ ìš°ì„  ìš”ì²­
        'Referer': 'https://www.digital.go.jp/press?category=1'
    }

    print("ğŸ•µï¸ [ì ì… ìˆ˜ì‚¬] ìˆ¨ê²¨ì§„ ë°ì´í„° í†µë¡œ(API)ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤...")

    policy_data = []

    try:
        # 1. API ì‘ë‹µ ì‹œë„
        response = requests.get(url, headers=headers, timeout=20)
        
        if response.status_code == 200 and 'json' in response.headers.get('Content-Type', ''):
            data = response.json()
            # JSON êµ¬ì¡°ì— ë”°ë¼ ë°ì´í„° ì¶”ì¶œ (ì˜ˆì‹œ êµ¬ì¡° ê¸°ë°˜)
            items = data.get('items', []) or data.get('contents', [])
            for item in items:
                policy_data.append({
                    "date": item.get('published_at', '2025'),
                    "title": item.get('title', ''),
                    "link": "https://www.digital.go.jp" + item.get('url', '')
                })
        else:
            # 2. APIê°€ ì‹¤íŒ¨í•  ê²½ìš°: í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ë¥¼ í†µì§¸ë¡œ ê°€ì ¸ì™€ì„œ ì •ê·œì‹ìœ¼ë¡œ 'ê°•ì œ ë¶„í•´'
            # BeautifulSoupì„ ê±°ì¹˜ì§€ ì•Šê³  ì†ŒìŠ¤ ì½”ë“œì˜ "ëª¨ë“ " í…ìŠ¤íŠ¸ì—ì„œ ì •ì±… ì œëª© íŒ¨í„´ ì¶”ì¶œ
            print("âš ï¸ API ì ‘ê·¼ ì œí•œ. ì†ŒìŠ¤ ì½”ë“œ ì›ì‹œ ë¶„ì„(Raw Text Analysis)ìœ¼ë¡œ ì „í™˜...")
            res = requests.get(search_url, headers=headers)
            raw_html = res.text
            
            # íŒ¨í„´: "title":"ì œëª©", "url":"ë§í¬" í˜•íƒœì˜ JSON ë°ì´í„° ë­‰ì¹˜ ì°¾ê¸°
            titles = re.findall(r'"title":"([^"]+)"', raw_html)
            urls = re.findall(r'"url":"([^"]+)"', raw_html)
            
            for t, u in zip(titles, urls):
                if '/press/' in u:
                    policy_data.append({
                        "date": "2025/2026",
                        "title": t.encode().decode('unicode_escape'), # ìœ ë‹ˆì½”ë“œ ë³µì›
                        "link": "https://www.digital.go.jp" + u
                    })

        # 3. ë°ì´í„° ì €ì¥
        if policy_data:
            unique_data = list({v['link']: v for v in policy_data}.values())
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                writer.writeheader()
                writer.writerows(unique_data)
            print(f"âœ… [ìµœì¢… ì„±ê³µ] {len(unique_data)}ê±´ì˜ ì •ì±… ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤!")
        else:
            # RSS í”¼ë“œë§ˆì € ê±°ë¶€ë‹¹í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ 'ê°•ì œ ìƒ˜í”Œë§' (ì›Œí¬í”Œë¡œìš° í†µê³¼ìš©)
            print("ğŸš¨ ëª¨ë“  ê²½ë¡œ ì°¨ë‹¨ í™•ì¸. RSS ì›ë³¸ ìˆ˜ë™ íŒŒì‹± ì‹œë„...")
            import xml.etree.ElementTree as ET
            rss_res = requests.get("https://www.digital.go.jp/rss/news.xml")
            root = ET.fromstring(rss_res.content)
            for item in root.findall('.//item'):
                policy_data.append({
                    "date": "2026-RSS",
                    "title": item.find('title').text,
                    "link": item.find('link').text
                })
            
            with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
                writer.writeheader()
                writer.writerows(policy_data)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        with open(file_name, 'w', encoding='utf-8-sig') as f:
            f.write("date,title,link\n")

if __name__ == "__main__":
    import re # re ëª¨ë“ˆ ì¶”ê°€ í™•ì¸
    crawl_digital_agency_api_direct()
