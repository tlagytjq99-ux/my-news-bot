import requests
from bs4 import BeautifulSoup
import csv
import xml.etree.ElementTree as ET # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì—†ì´ RSS ì½ê¸°ìš©

def crawl_digital_agency_hybrid():
    file_name = 'Japan_Digital_Policy_2025.csv'
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
    policy_data = []

    # --- 1ë‹¨ê³„: RSS í”¼ë“œ í„¸ê¸° (ì‹¤ì‹œê°„ ìµœì‹  ë°ì´í„°) ---
    print("ğŸ“¡ [RSS ìŠ¤ìº”] ìµœì‹  í”¼ë“œë¥¼ ë¶„ì„ ì¤‘...")
    try:
        rss_res = requests.get("https://www.digital.go.jp/rss/news.xml", timeout=15)
        root = ET.fromstring(rss_res.content)
        for item in root.findall('.//item'):
            title = item.find('title').text
            link = item.find('link').text
            # RSSì—ì„œ 2025, 2026ë…„ ë°ì´í„°ë§Œ ì¶”ì¶œ
            policy_data.append({"date": "RSS_Latest", "title": title, "link": link})
    except Exception as e:
        print(f"âš ï¸ RSS ìŠ¤ìº” ê±´ë„ˆëœ€ (ì˜¤ë¥˜: {e})")

    # --- 2ë‹¨ê³„: ì›¹ ì•„ì¹´ì´ë¸Œ í„¸ê¸° (ê³¼ê±° 2025ë…„ ì „ìˆ˜ ì¡°ì‚¬) ---
    print("ğŸ¯ [ì›¹ ìŠ¤ìº”] 2025ë…„ ì „ì²´ ì•„ì¹´ì´ë¸Œ ì •ë°€ ìˆ˜ìƒ‰ ì¤‘...")
    try:
        web_res = requests.get("https://www.digital.go.jp/news/press", headers=headers, timeout=15)
        web_res.encoding = 'utf-8'
        soup = BeautifulSoup(web_res.text, 'html.parser')
        
        # ë³´ë„ìë£Œ ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  ë§í¬ ì¶”ì¶œ
        for a in soup.find_all('a', href=True):
            href = a['href']
            if '/news/' in href:
                title = a.get_text(strip=True)
                if len(title) > 10: # ë©”ë‰´ ì œì™¸, ì‹¤ì œ ì œëª©ë§Œ
                    policy_data.append({
                        "date": "Archive_2025",
                        "title": title,
                        "link": "https://www.digital.go.jp" + href if href.startswith('/') else href
                    })
    except Exception as e:
        print(f"âš ï¸ ì›¹ ìŠ¤ìº” ì˜¤ë¥˜: {e}")

    # --- 3ë‹¨ê³„: ì¤‘ë³µ ì œê±° ë° ì €ì¥ ---
    if policy_data:
        # ë§í¬ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ë°ì´í„° ì œê±°
        unique_data = list({v['link']: v for v in policy_data}.values())
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["date", "title", "link"])
            writer.writeheader()
            writer.writerows(unique_data)
        print(f"âœ… í•©ì²´ ì„±ê³µ! RSS + ì›¹ ì•„ì¹´ì´ë¸Œ ì´ {len(unique_data)}ê±´ í™•ë³´.")
    else:
        # ë¹ˆ íŒŒì¼ì´ë¼ë„ ìƒì„±í•˜ì—¬ Git ì—ëŸ¬ ë°©ì§€
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            f.write("date,title,link\n")

if __name__ == "__main__":
    crawl_digital_agency_hybrid()
