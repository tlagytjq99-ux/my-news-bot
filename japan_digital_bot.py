import requests
from bs4 import BeautifulSoup
import csv
import time

def crawl_digital_2025_ultimate_wall_breaker():
    start_page = 21
    end_page = 188
    file_name = 'Japan_Digital_2025_Full_Archive.csv'
    base_url = "https://www.digital.go.jp/news?page="
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        'Referer': 'https://www.digital.go.jp/news'
    }
    
    all_data = []
    seen_links = set()

    print(f"ğŸš€ [ë²½ ê¹¨ê¸° ëª¨ë“œ] {start_page} ~ {end_page} í˜ì´ì§€ì˜ ëª¨ë“  'ìƒì¡´ ë§í¬'ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

    for page in range(start_page, end_page + 1):
        url = f"{base_url}{page}"
        print(f"ğŸ“¡ {page}/{end_page} í˜ì´ì§€ í…ìŠ¤íŠ¸ ë¶„í•´ ì¤‘... (í˜„ì¬ {len(all_data)}ê±´)", end='\r')
        
        try:
            # ì„¸ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì¿ í‚¤ì™€ ì—°ê²° ìœ ì§€ (ì°¨ë‹¨ ë°©ì§€)
            with requests.Session() as session:
                res = session.get(url, headers=headers, timeout=30)
                res.encoding = 'utf-8'
                soup = BeautifulSoup(res.text, 'html.parser')
                
                # [í•µì‹¬ ë³€ê²½] íŠ¹ì • í´ë˜ìŠ¤ë¥¼ ì°¾ì§€ ì•Šê³  í˜ì´ì§€ì˜ ëª¨ë“  <a> íƒœê·¸ë¥¼ íƒ€ê²ŸíŒ…
                all_links = soup.find_all('a', href=True)
                
                for a in all_links:
                    href = a['href']
                    
                    # ë‰´ìŠ¤ë‚˜ ë³´ë„ìë£Œ ì£¼ì†Œê°€ í¬í•¨ëœ ëª¨ë“  ë§í¬ë¥¼ ìˆ˜ì§‘
                    # 'news', 'press', 'policies', 'announcement', 'topics' ë“± ëª¨ë“  ê²½ë¡œ í—ˆìš©
                    if any(path in href for path in ['/news/', '/press/', '/policies/', '/topics/', '/announcement/']):
                        full_url = "https://www.digital.go.jp" + href if href.startswith('/') else href
                        
                        if full_url not in seen_links:
                            # <a> íƒœê·¸ ë‚´ë¶€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ê³µë°± í¬í•¨í•´ì„œ ì¶”ì¶œ
                            title = a.get_text(" ", strip=True)
                            
                            # ë©”ë‰´ë‚˜ í‘¸í„°ì— ìˆëŠ” ì§§ì€ ë§í¬ í•„í„°ë§ (ìµœì†Œ 12ì ì´ìƒ)
                            if len(title) < 12:
                                continue
                                
                            seen_links.add(full_url)
                            all_data.append({
                                "title": title,
                                "link": full_url
                            })

            # í˜ì´ì§€ë§ˆë‹¤ 0.2ì´ˆë§Œ ì‰¬ì–´ì„œ ì†ë„ í™•ë³´
            time.sleep(0.2)

        except Exception as e:
            continue

    # ë°ì´í„° ì €ì¥
    if all_data:
        # ë‚ ì§œìˆœ ì •ë ¬ ì‹œë„ (í…ìŠ¤íŠ¸ ì•ˆì— ë‚ ì§œê°€ ìˆì„ ê²½ìš°ë¥¼ ìœ„í•´)
        all_data.sort(key=lambda x: x['title'], reverse=True)
        
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["title", "link"])
            writer.writeheader()
            writer.writerows(all_data)
        print(f"\n\nâœ… [ì„±ê³µ] ì´ {len(all_data)}ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    crawl_digital_2025_ultimate_wall_breaker()
