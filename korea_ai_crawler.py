import feedparser
import csv
import urllib.parse
from datetime import datetime
from googlenewsdecoder import gnewsdecoder

def main():
    # ğŸ¯ êµ­ë‚´ í•µì‹¬ ì •ì±… ê¸°ê´€ ë¦¬ìŠ¤íŠ¸ (site: ì—°ì‚°ìë¡œ ê³µì‹ ë„ë©”ì¸ë§Œ íƒ€ê²ŸíŒ…)
    target_sources = {
        "ê³¼ê¸°ì •í†µë¶€": 'site:msit.go.kr (ì¸ê³µì§€ëŠ¥ OR AI)',
        "NIA(ì§€ëŠ¥ì •ë³´ì‚¬íšŒì§„í¥ì›)": 'site:nia.or.kr (ì¸ê³µì§€ëŠ¥ OR AI)',
        "NIPA(ì •ë³´í†µì‹ ì‚°ì—…ì§„í¥ì›)": 'site:nipa.kr (ì¸ê³µì§€ëŠ¥ OR AI)',
        "SPRI(ì†Œí”„íŠ¸ì›¨ì–´ì •ì±…ì—°êµ¬ì†Œ)": 'site:spri.kr (ì¸ê³µì§€ëŠ¥ OR AI)',
        "ETRI(ì „ìí†µì‹ ì—°êµ¬ì›)": 'site:etri.re.kr (ì¸ê³µì§€ëŠ¥ OR AI)',
        "ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ": 'site:pipc.go.kr (ì¸ê³µì§€ëŠ¥ OR AI)'
    }

    file_name = 'korea_ai_policy_report.csv'
    collected_date = datetime.now().strftime("%Y-%m-%d")
    final_data = []

    print(f"ğŸ‡°ğŸ‡· êµ­ë‚´ AI ì •ì±… ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")

    for agency, query in target_sources.items():
        print(f"ğŸ” {agency} ë¶„ì„ ì¤‘...")
        
        # í•œêµ­ì–´(ko) ë° í•œêµ­ ì§€ì—­(KR) ì„¤ì •
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        
        try:
            feed = feedparser.parse(rss_url)
            # ê¸°ê´€ë³„ ìµœì‹  5~7ê±´ ì¶”ì¶œ
            entries = sorted(feed.entries, key=lambda x: x.get('published_parsed'), reverse=True)[:7]
            
            for entry in entries:
                title = entry.title.split(' - ')[0]
                
                # 1. êµ¬ê¸€ ì•”í˜¸ ë§í¬ í•´ë…
                try:
                    decoded = gnewsdecoder(entry.link)
                    actual_link = decoded.get('decoded_url', entry.link)
                except:
                    actual_link = entry.link

                # 2. PDF ì—¬ë¶€ íŒë³„ (íŒŒì¼ í™•ì¥ì ì²´í¬)
                is_pdf = "YES" if actual_link.lower().endswith('.pdf') or ".pdf?" in actual_link.lower() else "NO"
                
                # 3. ë‚ ì§œ ë° ì œëª© êµ¬ì„±
                pub_date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d') if hasattr(entry, 'published_parsed') else collected_date
                display_title = f"[PDF] {title}" if is_pdf == "YES" else title

                final_data.append({
                    "ê¸°ê´€": agency,
                    "ë°œí–‰ì¼": pub_date,
                    "ì œëª©": display_title,
                    "ì›ë¬¸ì œëª©": title,
                    "PDFì—¬ë¶€": is_pdf,
                    "ë§í¬": actual_link,
                    "ìˆ˜ì§‘ì¼": collected_date
                })

        except Exception as e:
            print(f"âŒ {agency} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    # ìµœì‹  ë‚ ì§œìˆœ ì •ë ¬ í›„ CSV ì €ì¥ (utf-8-sigë¡œ í•œê¸€ ê¹¨ì§ ë°©ì§€)
    final_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)

    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        fieldnames = ["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸ì œëª©", "PDFì—¬ë¶€", "ë§í¬", "ìˆ˜ì§‘ì¼"]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(final_data)

    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ! {len(final_data)}ê±´ ì €ì¥ë¨.")

if __name__ == "__main__":
    main()
