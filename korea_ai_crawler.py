import feedparser
import csv
import urllib.parse
from datetime import datetime
from googlenewsdecoder import gnewsdecoder

def main():
    target_sources = {
        "ê³¼ê¸°ì •í†µë¶€": 'site:msit.go.kr (ì¸ê³µì§€ëŠ¥ OR AI)',
        "NIA": 'site:nia.or.kr (ì¸ê³µì§€ëŠ¥ OR AI)',
        "NIPA": 'site:nipa.kr (ì¸ê³µì§€ëŠ¥ OR AI)',
        "SPRI": 'site:spri.kr (ì¸ê³µì§€ëŠ¥ OR AI)',
        "ETRI": 'site:etri.re.kr (ì¸ê³µì§€ëŠ¥ OR AI)',
        "ê°œì¸ì •ë³´ìœ„": 'site:pipc.go.kr (ì¸ê³µì§€ëŠ¥ OR AI)'
    }

    # ğŸ›‘ ë…¸ì´ì¦ˆ ë° ì¤‘ë³µ ë‹¨ì–´ í•„í„° ëŒ€í­ ë³´ê°•
    exclude_keywords = [
        'ë§¨ ë’¤ë¡œ', 'ì§ì›ê²€ìƒ‰', 'ì¹´ë“œë‰´ìŠ¤', 'ì…ì°°ê³µê³ ', 'ê²Œì‹œíŒ ì¸ì‡„', 'ë¡œê·¸ì¸', 
        'í™ˆí˜ì´ì§€', 'ìƒì„¸ë³´ê¸°', 'ì‚¬ì „ì •ë³´ê³µí‘œ', 'ëˆ„ë¦¬ì§‘ì…ë‹ˆë‹¤', 'Untitled', 
        'ë³´ ë„ ì ë£Œ', 'êµ­ê°€ë³„ ì •ë³´', 'ë¹„ê³µê°œì •ë³´', 'ê²€ìƒ‰ê²°ê³¼', 'ëª©ë¡', 'ì§ì› ì•ˆë‚´'
    ]

    file_name = 'korea_ai_policy_report.csv'
    # ì´ˆ ë‹¨ìœ„ ìˆ˜ì§‘ì‹œê°„ì„ ì¶”ê°€í•˜ì—¬ GitHub Actionsì˜ ê°•ì œ ì—…ë°ì´íŠ¸ ìœ ë„
    collected_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    final_data = []

    print(f"ğŸš€ êµ­ë‚´ AI ì •ì±… ì •ë°€ ìˆ˜ì§‘ ë° ì œëª© ì •ì œ ì‹œì‘...")

    for agency, query in target_sources.items():
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        
        feed = feedparser.parse(rss_url)
        for entry in feed.entries[:25]: # ë” ë„“ì€ ë²”ìœ„ íƒìƒ‰
            raw_title = entry.title.split(' - ')[0]
            
            # 1. ì œëª© ì •ì œ: "HOME > ì•Œë¦¼ë§ˆë‹¹ > í•µì‹¬ì œëª©" êµ¬ì¡°ì—ì„œ ë§ˆì§€ë§‰ í•µì‹¬ì œëª©ë§Œ ì¶”ì¶œ
            if ">" in raw_title:
                clean_title = raw_title.split(">")[-1].strip()
            else:
                clean_title = raw_title.strip()
            
            # 2. í•„í„°ë§: ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë‹¨ì–´ í¬í•¨ ì‹œ ì œì™¸
            if any(key in clean_title for key in exclude_keywords): continue
            
            # 3. í•„í„°ë§: ë„ˆë¬´ ì§§ê±°ë‚˜ ë¬´ì˜ë¯¸í•œ ì œëª© ì œì™¸
            if len(clean_title) < 5 or clean_title == "ê³µì§€ì‚¬í•­": continue

            try:
                decoded = gnewsdecoder(entry.link)
                actual_link = decoded.get('decoded_url', entry.link)
            except:
                actual_link = entry.link

            # 4. í•„í„°ë§: 2025ë…„ ì´í›„ ìµœì‹  ë°ì´í„°ë§Œ ìœ ì§€
            pub_date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d') if hasattr(entry, 'published_parsed') else "2026-01-01"
            if pub_date < '2025-01-01': continue

            # PDF íŒë³„ ë¡œì§ (êµ­ë‚´ ê¸°ê´€ URL íŠ¹ì„± ë°˜ì˜)
            is_pdf = "NO"
            if any(x in actual_link.lower() for x in ['.pdf', 'download', 'filedown', 'attach']):
                is_pdf = "YES"

            final_data.append({
                "ê¸°ê´€": agency,
                "ë°œí–‰ì¼": pub_date,
                "ì œëª©": f"[ë¦¬í¬íŠ¸] {clean_title}" if is_pdf == "YES" else clean_title,
                "PDFì—¬ë¶€": is_pdf,
                "ë§í¬": actual_link,
                "ìµœì¢…ìˆ˜ì§‘ì‹œê°„": collected_time
            })

    # ìµœì‹  ë‚ ì§œìˆœ ì •ë ¬
    final_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)

    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        fieldnames = ["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "PDFì—¬ë¶€", "ë§í¬", "ìµœì¢…ìˆ˜ì§‘ì‹œê°„"]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(final_data)

    print(f"âœ… ì •ì œ ì™„ë£Œ! ì´ {len(final_data)}ê±´ ì €ì¥.")

if __name__ == "__main__":
    main()
