import asyncio
import csv
import json
import re
from datetime import datetime
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    # ğŸ”— ìˆ˜ì§‘í•˜ê³  ì‹¶ì€ AI ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë§í¬ë“¤ì…ë‹ˆë‹¤. 
    # ì•ìœ¼ë¡œ ë‹¤ë¥¸ AI ì‚¬ì´íŠ¸ë¥¼ ë°œê²¬í•˜ì‹œë©´ ì´ ë¦¬ìŠ¤íŠ¸ì— ì£¼ì†Œë§Œ ì¶”ê°€í•˜ì‹œë©´ ë©ë‹ˆë‹¤!
    urls = [
        "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1", # AIíƒ€ì„ìŠ¤
        "https://venturebeat.com/category/ai/", # ë²¤ì²˜ë¹„íŠ¸ (í•´ì™¸)
        "https://www.artificialintelligence-news.com/", # AI ë‰´ìŠ¤ (í•´ì™¸)
        "https://www.theverge.com/ai-artificial-intelligence" # ë” ë²„ì§€ AI ì„¹ì…˜
    ]

    # [ë²”ìš© ê·œì¹™] AI ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë“¤ì˜ ê³µí†µ êµ¬ì¡°ë¥¼ íƒ€ê²ŸíŒ…í•©ë‹ˆë‹¤.
    schema = {
        "name": "AI_News_Scanner",
        "baseSelector": "article, .item, .list-block, .post-block, li", # ë‰´ìŠ¤ í•œ ì¤„ì˜ ë‹¨ìœ„
        "fields": [
            {"name": "title", "selector": "h2, h3, h4, .tit, .title", "type": "text"},
            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"},
            {"name": "date", "selector": "time, .date, .dt, span.time", "type": "text"}
        ]
    }

    final_results = []
    today_str = datetime.now().strftime("%Y-%m-%d")

    async with AsyncWebCrawler() as crawler:
        for url in urls:
            print(f"ğŸ“¡ {url} ì—ì„œ AI ë‰´ìŠ¤ ì°¾ëŠ” ì¤‘...")
            
            result = await crawler.arun(
                url=url,
                extraction_strategy=JsonCssExtractionStrategy(schema),
                bypass_cache=True
            )

            if result.success and result.extracted_content:
                items = json.loads(result.extracted_content)
                count = 0
                for item in items:
                    title = item.get("title", "").strip()
                    link = item.get("link", "")
                    
                    # 1. ì“¸ëª¨ì—†ëŠ” ì§§ì€ í…ìŠ¤íŠ¸(ë©”ë‰´ ë“±) ì œì™¸
                    if len(title) < 12 or not link or "javascript" in link:
                        continue
                    
                    # 2. ë‚ ì§œ ì •ë¦¬ (í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ í˜•íƒœë§Œ ì¶”ì¶œ)
                    raw_date = item.get("date", "")
                    date_match = re.search(r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})', raw_date)
                    if date_match:
                        clean_date = f"{date_match.group(1)}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"
                    else:
                        clean_date = today_str # ë‚ ì§œ ëª» ì°¾ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œë¡œ í‘œì‹œ

                    # 3. ë§í¬ ì£¼ì†Œ ë³´ì •
                    full_link = link if link.startswith('http') else f"{url.split('/')[0]}//{url.split('/')[2]}{link}"

                    final_results.append({
                        "ìˆ˜ì§‘ì¼": today_str,
                        "ë°œí–‰ì¼": clean_date,
                        "ì œëª©": title,
                        "ë§í¬": full_link
                    })
                    count += 1
                    if count >= 5: break # ì‚¬ì´íŠ¸ë‹¹ 5ê°œë§Œ!
                
                print(f"âœ… {url}: {count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

    # ì—‘ì…€(CSV) ì €ì¥
    with open('ai_trend_report.csv', 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ìˆ˜ì§‘ì¼", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬"])
        writer.writeheader()
        writer.writerows(final_results)
    
    print(f"\nâœ¨ ëª¨ë“  ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(final_results)}ê°œì˜ ë‰´ìŠ¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
