import asyncio
import csv
import os
from datetime import datetime
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    browser_config = BrowserConfig(
        browser_type="chromium",
        headless=True,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )

    urls = [
        "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "https://venturebeat.com/category/ai/",
        "https://www.artificialintelligence-news.com/"
    ]

    final_data = []
    today = datetime.now().strftime("%Y-%m-%d")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for url in urls:
            try:
                print(f"ğŸ“¡ {url} ì‹œë„ ì¤‘...")
                # ì¶”ì¶œ ì „ëµ ì—†ì´ ê·¸ëƒ¥ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í†µì§¸ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
                result = await crawler.arun(url=url, bypass_cache=True)

                if result.success and result.markdown:
                    # ë§ˆí¬ë‹¤ìš´ ì•ˆì—ì„œ ë§í¬ í˜•íƒœ [ì œëª©](ì£¼ì†Œ) ë§Œ ê³¨ë¼ëƒ…ë‹ˆë‹¤.
                    import re
                    links = re.findall(r'\[([^\]]{10,})\]\(([^\)]+)\)', result.markdown)
                    
                    added = 0
                    for title, link in links:
                        if "http" not in link and not link.startswith("/"): continue
                        if any(x in title.lower() for x in ["terms", "privacy", "about", "contact"]): continue
                        
                        final_data.append({
                            "ìˆ˜ì§‘ì¼": today,
                            "ë°œí–‰ì¼": today,
                            "ì œëª©": title.strip(),
                            "ë§í¬": link if link.startswith("http") else url + link
                        })
                        added += 1
                        if added >= 5: break
                    print(f"âœ… {url}: {added}ê°œ ë°œê²¬")
            except Exception as e:
                print(f"âŒ {url} ì—ëŸ¬: {e}")

    # [í•µì‹¬] ë°ì´í„°ê°€ ì—†ì–´ë„ íŒŒì¼ì„ ë§Œë“­ë‹ˆë‹¤.
    if not final_data:
        final_data.append({"ìˆ˜ì§‘ì¼": today, "ë°œí–‰ì¼": "-", "ì œëª©": "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ ì°¨ë‹¨ì„ í™•ì¸í•˜ì„¸ìš”.", "ë§í¬": "-"})

    with open('ai_trend_report.csv', 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ìˆ˜ì§‘ì¼", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬"])
        writer.writeheader()
        writer.writerows(final_data)
    
    print(f"ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {os.path.abspath('ai_trend_report.csv')}")

if __name__ == "__main__":
    asyncio.run(main())
