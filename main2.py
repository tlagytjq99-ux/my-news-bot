import asyncio
import csv
import re
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def get_exact_date(crawler, url, config, site_name):
    """κΈ°μ‚¬ μƒμ„Έ νμ΄μ§€μ—μ„ μ‹¤μ  λ°ν–‰μΌμ„ μ •λ°€ μ¶”μ¶ν•©λ‹λ‹¤."""
    try:
        result = await crawler.arun(url=url, config=config)
        if not (result.success and result.markdown): return "λ‚ μ§ν™•μΈν•„μ”"
        
        content = result.markdown
        # π” μƒλ‹¨ 1000μκΉμ§€λ§ κ²€μƒ‰ (λ…Έμ΄μ¦ μ°¨λ‹¨)
        header_content = content[:1000]

        if site_name == "AIνƒ€μ„μ¤":
            # AIνƒ€μ„μ¤ ν¨ν„΄: '2026.01.28 14:30' λλ” 'μΉμΈ 2026.01.28'
            match = re.search(r'(\d{4}\.\d{2}\.\d{2})\s+\d{2}:\d{2}', header_content)
            if match: return match.group(1).replace('.', '-')
            match2 = re.search(r'(?:μΉμΈ|λ“±λ΅)\s+(\d{4}\.\d{2}\.\d{2})', header_content)
            if match2: return match2.group(1).replace('.', '-')

        # λ²¤μ²λΉ„νΈ/ν…ν¬ν¬λ°μΉμ©
        date_match = re.search(r'(\d{4}[-./]\d{2}[-./]\d{2})', header_content)
        if date_match:
            return date_match.group(1).replace('.', '-').replace('/', '-')
            
        eng_match = re.search(r'([A-Z][a-z]+ \d{1,2}, \d{4})', header_content)
        if eng_match:
            dt = datetime.strptime(eng_match.group(1), "%B %d, %Y")
            return dt.strftime("%Y-%m-%d")
            
    except: pass
    return "λ‚ μ§ν™•μΈν•„μ”"

async def main():
    target_sites = {
        "AIνƒ€μ„μ¤": "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "λ²¤μ²λΉ„νΈ": "https://venturebeat.com/category/ai/",
        "ν…ν¬ν¬λ°μΉ": "https://techcrunch.com/category/artificial-intelligence/"
    }

    allowed_years = ['2025', '2026']
    browser_config = BrowserConfig(browser_type="chromium", headless=True)
    run_config = CrawlerRunConfig(
        wait_for="body", 
        wait_for_timeout=30000,
        delay_before_return_html=5.0 
    )
    
    final_data = []
    today_str = datetime.now().strftime("%Y-%m-%d")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for site_name, url in target_sites.items():
            print(f"π“΅ [{site_name}] λ¶„μ„ μ¤‘...")
            list_result = await crawler.arun(url=url, config=run_config)

            if list_result.success and list_result.markdown:
                links = re.findall(r'\[([^\]]{28,})\]\(([^\)]+)\)', list_result.markdown)
                
                count = 0
                for title, link in links:
                    title_clean = re.sub(r'[\[\]\r\n\t]', '', title).strip()
                    if "![" in title or any(ext in link.lower() for ext in ['.jpg', '.png']): continue
                    
                    full_link = urljoin(url, link)
                    if any(d['μ λ©'] == title_clean for d in final_data): continue

                    print(f"   π” λ‚ μ§ ν™•μΈ μ¤‘: {title_clean[:15]}...")
                    exact_date = await get_exact_date(crawler, full_link, run_config, site_name)
                    
                    if not any(year in exact_date for year in allowed_years):
                        if exact_date != "λ‚ μ§ν™•μΈν•„μ”": continue

                    final_data.append({
                        "μ¶μ²": site_name,
                        "μμ§‘μΌ": today_str,
                        "λ°ν–‰μΌ": exact_date,
                        "μ λ©": title_clean,
                        "λ§ν¬": full_link
                    })
                    count += 1
                    if count >= 8: break

    # β… [μ •λ ¬ λ΅μ§ μμ •] 
    # 1μμ„: μ¶μ²(κ°€λ‚λ‹¤μ/ABCμ) 
    # 2μμ„: λ°ν–‰μΌ(μµμ‹ μ)
    final_data.sort(key=lambda x: (x['μ¶μ²'], x['λ°ν–‰μΌ']), reverse=False)
    # λ°ν–‰μΌλ§ μµμ‹ μμΌλ΅ λ³΄κ³  μ‹¶μΌμ‹λ©΄ μ•„λμ²λΌ μ •λ ¬ μ΅°κ±΄μ„ μ΅°ν•©ν•©λ‹λ‹¤.
    # final_data.sort(key=lambda x: (x['μ¶μ²'], datetime.strptime(x['λ°ν–‰μΌ'], '%Y-%m-%d') if '-' in x['λ°ν–‰μΌ'] else datetime.min), reverse=True)
    
    file_name = 'ai_trend_report.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["μ¶μ²", "μμ§‘μΌ", "λ°ν–‰μΌ", "μ λ©", "λ§ν¬"])
        writer.writeheader()
        writer.writerows(final_data)
    
    print(f"π‰ μ¶μ²λ³„ μ •λ ¬ μ™„λ£! λ¦¬ν¬νΈκ°€ μƒμ„±λμ—μµλ‹λ‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
