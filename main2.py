import asyncio
import csv
import re
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def get_exact_date_and_content(crawler, url, config, site_name):
    """μƒμ„Έ νμ΄μ§€μ—μ„ λ‚ μ§μ™€ AI κ΄€λ ¨μ„± μ—¬λ¶€λ¥Ό λ™μ‹μ— ν™•μΈν•©λ‹λ‹¤."""
    try:
        result = await crawler.arun(url=url, config=config)
        if not (result.success and result.markdown): 
            return "λ‚ μ§ν™•μΈν•„μ”", False
        
        content = result.markdown
        header = content[:3000].lower() # μƒλ‹¨ 3000μ λ¶„μ„

        # π“… [λ‚ μ§ μ¶”μ¶]
        final_date = "λ‚ μ§ν™•μΈν•„μ”"
        # 1. λ°±μ•…κ΄€/μλ¬Έ (January 28, 2026)
        eng_match = re.search(r'([a-z]+ \d{1,2}, \d{4})', header)
        if eng_match:
            try:
                dt = datetime.strptime(eng_match.group(1), "%B %d, %Y")
                final_date = dt.strftime("%Y-%m-%d")
            except: pass
        
        # 2. ν•κµ­ν•/μ«μν• (2026.01.28)
        if final_date == "λ‚ μ§ν™•μΈν•„μ”":
            match = re.search(r'(\d{4}[./-]\d{2}[./-]\d{2})', header)
            if match: final_date = match.group(1).replace('.', '-').replace('/', '-')

        # 3. ν…ν¬ν¬λ°μΉ λ“± URL λ‚ μ§ λ³΄μ •
        if final_date == "λ‚ μ§ν™•μΈν•„μ”":
            url_date = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', url)
            if url_date: final_date = f"{url_date.group(1)}-{url_date.group(2)}-{url_date.group(3)}"

        # π” [AI κ΄€λ ¨μ„± κ²€μ¦] - λ°±μ•…κ΄€ λ“± μ •λ¶€ λ¬Έμ„μ©
        # λ‹¨μ 'Tech'λ¥Ό λ„μ–΄ AI μ •μ±… ν•µμ‹¬ λ‹¨μ–΄λ“¤μ΄ ν¬ν•¨λμ—λ”μ§€ ν™•μΈ
        ai_focus_keywords = ['ai', 'artificial intelligence', 'machine learning', 'computing', 'semiconductor', 'llm', 'algorithm', 'cybersecurity']
        is_ai_related = any(kw in header for kw in ai_focus_keywords)

        return final_date, is_ai_related
            
    except: pass
    return "λ‚ μ§ν™•μΈν•„μ”", False

async def main():
    target_sites = {
        "AIνƒ€μ„μ¤": "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "λ²¤μ²λΉ„νΈ": "https://venturebeat.com/category/ai/",
        "ν…ν¬ν¬λ°μΉ": "https://techcrunch.com/category/artificial-intelligence/",
        "λ°±μ•…κ΄€(AI)": "https://www.whitehouse.gov/briefing-room/statements-releases/"
    }

    allowed_years = ['2025', '2026']
    browser_config = BrowserConfig(browser_type="chromium", headless=True)
    run_config = CrawlerRunConfig(wait_for="body", delay_before_return_html=7.0)
    
    final_data = []
    today_str = datetime.now().strftime("%Y-%m-%d")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for site_name, url in target_sites.items():
            print(f"π“΅ [{site_name}] λ°μ΄ν„° μ¤μΊ” λ° ν•„ν„°λ§ μ¤‘...")
            list_result = await crawler.arun(url=url, config=run_config)

            if list_result.success and list_result.markdown:
                links = re.findall(r'\[([^\]]{20,})\]\(([^\)]+)\)', list_result.markdown)
                
                count = 0
                for title, link in links:
                    title_clean = re.sub(r'[\[\]\r\n\t]', '', title).strip()
                    if "![" in title or any(ext in link.lower() for ext in ['.jpg', '.png']): continue
                    
                    full_link = urljoin(url, link)
                    if any(d['μ λ©'] == title_clean for d in final_data): continue

                    # π“… [ν•µμ‹¬] μƒμ„Έ νμ΄μ§€λ΅ λ“¤μ–΄κ°€μ„ λ‚ μ§μ™€ AI κ΄€λ ¨μ„± 'μ΄μ¤‘ μ²΄ν¬'
                    print(f"   π” μ •λ°€ λ¶„μ„ μ¤‘: {title_clean[:12]}...")
                    exact_date, is_ai = await get_exact_date_and_content(crawler, full_link, run_config, site_name)
                    
                    # λ°±μ•…κ΄€μ κ²½μ° AI κ΄€λ ¨ λ‚΄μ©μ΄ ν™•μΈλ κ²ƒλ§ μμ§‘
                    if site_name == "λ°±μ•…κ΄€(AI)" and not is_ai:
                        continue

                    # λ‚ μ§ λ―Έν™•μΈ μ‹ μ¤λ λ‚ μ§λ΅ λ³΄μ • ν›„ μ—°λ„ ν•„ν„°λ§
                    date_for_filter = exact_date if exact_date != "λ‚ μ§ν™•μΈν•„μ”" else today_str
                    if not any(year in date_for_filter for year in allowed_years):
                        continue

                    final_data.append({
                        "μ¶μ²": site_name,
                        "μμ§‘μΌ": today_str,
                        "λ°ν–‰μΌ": date_for_filter,
                        "μ λ©": title_clean,
                        "λ§ν¬": full_link
                    })
                    count += 1
                    if count >= 8: break

    # β… μ •λ ¬: μ¶μ²λ³„ -> λ°ν–‰μΌμ(μµμ‹ μ)
    final_data.sort(key=lambda x: (x['μ¶μ²'], x['λ°ν–‰μΌ']), reverse=False)
    
    file_name = 'ai_trend_report.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["μ¶μ²", "μμ§‘μΌ", "λ°ν–‰μΌ", "μ λ©", "λ§ν¬"])
        writer.writeheader()
        writer.writerows(final_data)
    
    print(f"\nπ‰ ν•„ν„°λ§ κ°•ν™” μ™„λ£! λ°±μ•…κ΄€μ 'μ§„μ§ AI' μ†μ‹λ§ λ‹΄μ•μµλ‹λ‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
