import asyncio
import csv
import os
import re
from datetime import datetime
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    browser_config = BrowserConfig(
        browser_type="chromium",
        headless=True,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )

    urls = [
        "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "https://venturebeat.com/category/ai/",
        "https://www.artificialintelligence-news.com/"
    ]

    final_data = []
    today = datetime.now().strftime("%Y-%m-%d")

    # ğŸš« ì œì™¸í•  ë‹¨ì–´ ëª©ë¡ (ì´ë¯¸ì§€ì— ë‚˜ì˜¨ ë…¸ì´ì¦ˆë“¤)
    exclude_keywords = [
        "ë°”ë¡œê°€ê¸°", "logo", "ë¡œê·¸ì¸", "íšŒì›ê°€ì…", "menu", "skip", 
        "copyright", "terms", "privacy", "owner", "click here",
        "english news", "future energy"
    ]

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for url in urls:
            try:
                print(f"ğŸ“¡ {url} ì •ë°€ í•„í„°ë§ ì¤‘...")
                result = await crawler.arun(url=url, bypass_cache=True)

                if result.success and result.markdown:
                    # [ì œëª©](ë§í¬) íŒ¨í„´ ì¶”ì¶œ
                    links = re.findall(r'\[([^\]]{15,})\]\(([^\)]+)\)', result.markdown)
                    
                    added = 0
                    for title, link in links:
                        title_clean = title.strip()
                        
                        # í•„í„°ë§ ì¡°ê±´ 1: ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
                        if len(title_clean) < 15: continue
                        # í•„í„°ë§ ì¡°ê±´ 2: ì œì™¸ ë‹¨ì–´ê°€ í¬í•¨ëœ ê²½ìš° íŒ¨ìŠ¤
                        if any(kw in title_clean.lower() for kw in exclude_keywords): continue
                        # í•„í„°ë§ ì¡°ê±´ 3: ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ë§ˆí¬ë‹¤ìš´ ì œì™¸
                        if "![" in title_clean: continue
                        
                        full_link = link if link.startswith("http") else url + link
                        
                        final_data.append({
                            "ìˆ˜ì§‘ì¼": today,
                            "ë°œí–‰ì¼": today,
                            "ì œëª©": title_clean,
                            "ë§í¬": full_link
                        })
                        added += 1
                        if added >= 5: break
                    print(f"âœ… {url}: {added}ê°œ ë‰´ìŠ¤ í™•ë³´")
            except Exception as e:
                print(f"âŒ {url} ì—ëŸ¬: {e}")

    # ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ëŒ€ë¹„
    if not final_data:
        final_data.append({"ìˆ˜ì§‘ì¼": today, "ë°œí–‰ì¼": "-", "ì œëª©": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (í•„í„°ë§ ê¸°ì¤€ ë¯¸ë‹¬)", "ë§í¬": "-"})

    with open('ai_trend_report.csv', 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ìˆ˜ì§‘ì¼", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬"])
        writer.writeheader()
        writer.writerows(final_data)
    
    print(f"ğŸ’¾ í•„í„°ë§ ì™„ë£Œ! íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
