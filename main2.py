import asyncio
import csv
import json
from datetime import datetime
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    # 1. ë¸Œë¼ìš°ì € ì„¤ì • (Playwrightì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ë“¤ì„ ì—¬ê¸°ì„œ ì„¸íŒ…)
    browser_config = BrowserConfig(
        browser_type="chromium", # í¬ë¡¬ ì—”ì§„ ì‚¬ìš©
        headless=True,           # í™”ë©´ ì—†ì´ ì‹¤í–‰ (ì†ë„ í–¥ìƒ)
        # ì¤‘ìš”: ì§„ì§œ ì‚¬ëŒì²˜ëŸ¼ ë³´ì´ê²Œ ë§Œë“œëŠ” 'ì§€ë¬¸(Fingerprint)' ì„¤ì •
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        extra_http_headers={"Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"}
    )

    # 2. í¬ë¡¤ë§ ì‹¤í–‰ ì„¤ì • (Playwrightê°€ ì‚¬ì´íŠ¸ ì ‘ì† ì‹œ í–‰ë™í•  ì§€ì¹¨)
    run_config = CrawlerRunConfig(
        # ë°ì´í„°ê°€ ëœ° ë•Œê¹Œì§€ ì¶©ë¶„íˆ ê¸°ë‹¤ë¦¼ (Playwrightì˜ ëŒ€ê¸° ê¸°ëŠ¥)
        wait_for="article, h2, h3, .list-block", 
        check_all_iframes=True,  # ìˆ¨ê²¨ì§„ í”„ë ˆì„ê¹Œì§€ í™•ì¸
        cache_mode=CacheMode.BYPASS, # ë§¤ë²ˆ ìƒˆë¡œ ê³ ì¹¨í•´ì„œ ìµœì‹  ë°ì´í„° ìˆ˜ì§‘
        # í˜ì´ì§€ ë¡œë”© í›„ 2ì´ˆê°„ ë” ëŒ€ê¸° (ìë°”ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ ê¸°ë‹¤ë¦¼)
        delay_before_return_html=2.0 
    )

    # 3. ë²”ìš©ì ì¸ ë‰´ìŠ¤ ì¶”ì¶œ ê·œì¹™
    schema = {
        "name": "AI_News_Extractor",
        "baseSelector": "article, .item, tr, li, .list-block", 
        "fields": [
            {"name": "title", "selector": "h2, h3, a.title, .tit", "type": "text"},
            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"},
            {"name": "date", "selector": "time, .date, .dt, span", "type": "text"}
        ]
    }
    extraction_strategy = JsonCssExtractionStrategy(schema)

    # ìˆ˜ì§‘ ëŒ€ìƒ AI ë‰´ìŠ¤ ì‚¬ì´íŠ¸
    urls = [
        "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "https://venturebeat.com/category/ai/",
        "https://www.artificialintelligence-news.com/"
    ]

    final_data = []
    today = datetime.now().strftime("%Y-%m-%d")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for url in urls:
            print(f"ğŸ“¡ {url} ìˆ˜ì§‘ ì‹œì‘ (Playwright ê°€ë™)...")
            
            result = await crawler.arun(
                url=url,
                config=run_config,
                extraction_strategy=extraction_strategy
            )

            if result.success and result.extracted_content:
                items = json.loads(result.extracted_content)
                count = 0
                for item in items:
                    title = item.get("title", "").strip()
                    link = item.get("link", "")
                    
                    if len(title) < 10 or not link: continue
                    
                    full_link = link if link.startswith('http') else f"{url.split('/')[0]}//{url.split('/')[2]}{link}"
                    
                    final_data.append({
                        "ìˆ˜ì§‘ì¼": today,
                        "ë°œí–‰ì¼": today,
                        "ì œëª©": title,
                        "ë§í¬": full_link
                    })
                    count += 1
                    if count >= 5: break
                print(f"âœ… {url}: {count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

    # ê²°ê³¼ë¬¼ ì €ì¥
    with open('ai_trend_report.csv', 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ìˆ˜ì§‘ì¼", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬"])
        writer.writeheader()
        writer.writerows(final_data)

if __name__ == "__main__":
    asyncio.run(main())
