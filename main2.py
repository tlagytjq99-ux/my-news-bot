import asyncio
import csv
import re
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def get_exact_date(crawler, url, config):
    """ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì‹¤ì œ ë°œí–‰ì¼ì„ ì •ë°€ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        result = await crawler.arun(url=url, config=config)
        if result.success and result.markdown:
            # 1. AIíƒ€ì„ìŠ¤ ì „ìš©: 'ê¸°ì‚¬ìŠ¹ì¸' ë˜ëŠ” 'ë“±ë¡' ë¬¸êµ¬ ì˜† ë‚ ì§œ ì°¾ê¸°
            ai_times_match = re.search(r'(?:ê¸°ì‚¬ìŠ¹ì¸|ë“±ë¡|ìˆ˜ì •)\s*[:\s]*(\d{4}[-./]\d{1,2}[-./]\d{1,2})', result.markdown)
            if ai_times_match:
                return ai_times_match.group(1).replace('.', '-').replace('/', '-')

            # 2. ì¼ë°˜ ìˆ«ìí˜• ë‚ ì§œ (YYYY-MM-DD)
            date_match = re.search(r'(\d{4}[-./]\d{1,2}[-./]\d{1,2})', result.markdown)
            if date_match:
                return date_match.group(1).replace('.', '-').replace('/', '-')
            
            # 3. ì˜ë¬¸í˜• ë‚ ì§œ (ë°±ì•…ê´€/í•´ì™¸ ì •ë¶€ê¸°ê´€ìš©: January 20, 2026)
            eng_match = re.search(r'([A-Z][a-z]+ \d{1,2}, \d{4})', result.markdown)
            if eng_match:
                try:
                    dt = datetime.strptime(eng_match.group(1), "%B %d, %Y")
                    return dt.strftime("%Y-%m-%d")
                except: pass
    except: pass
    return "í™•ì¸ë¶ˆê°€"

async def main():
    # âœ… ì—¬ê¸°ì— ì •ë¶€ê¸°ê´€ URLì„ ë§ˆìŒê» ì¶”ê°€í•´ ë³´ì„¸ìš”!
    target_sites = {
        "AIíƒ€ì„ìŠ¤": "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "ë²¤ì²˜ë¹„íŠ¸": "https://venturebeat.com/category/ai/",
        "í…Œí¬í¬ëŸ°ì¹˜": "https://techcrunch.com/category/artificial-intelligence/",
        "ë°±ì•…ê´€(AI)": "https://www.whitehouse.gov/briefing-room/statements-releases/"
    }

    browser_config = BrowserConfig(browser_type="chromium", headless=True)
    run_config = CrawlerRunConfig(wait_for="body", wait_for_timeout=15000)
    
    final_data = []
    today_str = datetime.now().strftime("%Y-%m-%d")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for site_name, url in target_sites.items():
            print(f"ğŸ“¡ [{site_name}] ë¶„ì„ ì‹œì‘...")
            list_result = await crawler.arun(url=url, config=run_config)

            if list_result.success and list_result.markdown:
                # ì œëª©ì´ í¬í•¨ëœ ë§í¬ ì¶”ì¶œ
                links = re.findall(r'\[([^\]]{25,})\]\(([^\)]+)\)', list_result.markdown)
                
                count = 0
                for title, link in links:
                    if "![" in title or any(ext in link.lower() for ext in ['.jpg', '.png', '.jpeg']): continue
                    
                    full_link = urljoin(url, link)
                    title_clean = re.sub(r'[\[\]\r\n\t]', '', title).strip()
                    
                    # ì¤‘ë³µ ì²´í¬
                    if any(d['ì œëª©'] == title_clean for d in final_data): continue

                    # ğŸ“… ìƒì„¸ í˜ì´ì§€ ê¹Šì´ ë¶„ì„
                    print(f"   ğŸ” ë‚ ì§œ ë§¤ì¹­ ì¤‘: {title_clean[:15]}...")
                    exact_date = await get_exact_date(crawler, full_link, run_config)
                    
                    # ëê¹Œì§€ ëª» ì°¾ìœ¼ë©´ URLì—ì„œ ì¶”ì¶œ ì‹œë„
                    if exact_date == "í™•ì¸ë¶ˆê°€":
                        url_date = re.search(r'/(\d{4})/(\d{1,2})/(\d{1,2})/', full_link)
                        if url_date:
                            exact_date = f"{url_date.group(1)}-{url_date.group(2).zfill(2)}-{url_date.group(3).zfill(2)}"
                        else:
                            exact_date = today_str # ìµœí›„ì˜ ìˆ˜ë‹¨

                    final_data.append({
                        "ì¶œì²˜": site_name,
                        "ìˆ˜ì§‘ì¼": today_str,
                        "ë°œí–‰ì¼": exact_date,
                        "ì œëª©": title_clean,
                        "ë§í¬": full_link
                    })
                    count += 1
                    if count >= 7: break # ì‚¬ì´íŠ¸ë‹¹ 7ê°œì”©

    # CSV ì €ì¥
    file_name = 'ai_trend_report.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ì¶œì²˜", "ìˆ˜ì§‘ì¼", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬"])
        writer.writeheader()
        writer.writerows(final_data)
    print(f"ğŸ‰ ëª¨ë“  ë‚ ì§œ êµì • ì™„ë£Œ! íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
