import asyncio
import csv
import os
import re
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    # ğŸ”— [ì •ë³´ì› ê´€ë¦¬] ì—¬ê¸°ì— ìƒˆë¡œìš´ ì‚¬ì´íŠ¸ë¥¼ ê³„ì† ì¶”ê°€í•˜ì„¸ìš”!
    target_sites = {
        "AIíƒ€ì„ìŠ¤": "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "ë²¤ì²˜ë¹„íŠ¸": "https://venturebeat.com/category/ai/",
        "í…Œí¬í¬ëŸ°ì¹˜": "https://techcrunch.com/category/artificial-intelligence/",
        "AIë‰´ìŠ¤(ì˜êµ­)": "https://www.artificialintelligence-news.com/",
        "ì „ìì‹ ë¬¸AI": "https://www.etnews.com/news/section.html?id1=20&id2=065",
        "ZDNet_AI": "https://zdnet.co.kr/newskey/?lstkey=ì¸ê³µì§€ëŠ¥"
    }

    browser_config = BrowserConfig(browser_type="chromium", headless=True)
    # ë¡œë”© ì‹œê°„ì„ ì¶©ë¶„íˆ ì£¼ì–´ ëˆ„ë½ ë°©ì§€
    run_config = CrawlerRunConfig(wait_for="body", wait_for_timeout=20000)
    
    final_data = []
    today = datetime.now().strftime("%Y-%m-%d")

    # ğŸš« ê°•í™”ëœ í•„í„°ë§ í‚¤ì›Œë“œ (ë©”ë‰´, ë¡œê³ , ì¹´í…Œê³ ë¦¬ ë“± ì œê±°)
    exclude_keywords = [
        "ë°”ë¡œê°€ê¸°", "ë¡œê·¸ì¸", "íšŒì›ê°€ì…", "copyright", "terms", "privacy", 
        "newsletter", "brand studio", "battlefield", "advertising", "contact",
        "policy", "media", "entertainment", "subscribe", "events"
    ]

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for site_name, url in target_sites.items():
            try:
                print(f"ğŸ“¡ [{site_name}] ë°ì´í„° ìˆ˜ì§‘ ì‹œë„...")
                result = await crawler.arun(url=url, config=run_config)

                if result.success and result.markdown:
                    # ë§ˆí¬ë‹¤ìš´ ë‚´ ë§í¬ íŒ¨í„´ [ì œëª©](ë§í¬) ì¶”ì¶œ
                    # ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë©”ë‰´ì¼ í™•ë¥ ì´ ë†’ìœ¼ë¯€ë¡œ 25ì ì´ìƒìœ¼ë¡œ í•„í„°ë§
                    links = re.findall(r'\[([^\]]{25,})\]\(([^\)]+)\)', result.markdown)
                    
                    added = 0
                    for title, link in links:
                        title_clean = title.replace("\n", " ").strip()
                        
                        # 1. ì œì™¸ í‚¤ì›Œë“œ ê²€ì‚¬
                        if any(kw in title_clean.lower() for kw in exclude_keywords): continue
                        # 2. ì´ë¯¸ì§€ê°€ ì„ì¸ ë§í¬ ì œê±° (![...])
                        if "![" in title_clean: continue
                        # 3. íŠ¹ìˆ˜ë¬¸ìë¡œë§Œ ëœ ì œëª© ì œê±°
                        if not re.search('[a-zA-Zê°€-í£]', title_clean): continue

                        full_link = urljoin(url, link)
                        
                        final_data.append({
                            "ì¶œì²˜": site_name,
                            "ìˆ˜ì§‘ì¼": today,
                            "ì œëª©": title_clean,
                            "ë§í¬": full_link
                        })
                        added += 1
                        if added >= 8: break # ì‚¬ì´íŠ¸ë‹¹ ìµœëŒ€ 8ê°œê¹Œì§€
                    
                    print(f"âœ… {site_name}: {added}ê°œ ë‰´ìŠ¤ í™•ë³´")
            except Exception as e:
                print(f"âŒ {site_name} ì˜¤ë¥˜: {e}")

    # ì €ì¥ ë¡œì§
    if final_data:
        with open('ai_trend_report.csv', 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["ì¶œì²˜", "ìˆ˜ì§‘ì¼", "ì œëª©", "ë§í¬"])
            writer.writeheader()
            writer.writerows(final_data)
        print(f"ğŸ‰ í•„í„°ë§ ì™„ë£Œ! ì´ {len(final_data)}ê°œì˜ ë‰´ìŠ¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
