import asyncio
import csv
import json
from datetime import datetime
from dateutil import parser
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

# ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜
def format_date(date_str):
    if not date_str: return datetime.now().strftime("%Y-%m-%d")
    try:
        return parser.parse(date_str, fuzzy=True).strftime("%Y-%m-%d")
    except:
        return datetime.now().strftime("%Y-%m-%d")

async def main():
    # ìˆ˜ì§‘ ëŒ€ìƒ (êµ¬ì¡°ê°€ ëª…í™•í•œ ê³³ ìœ„ì£¼ë¡œ ìš°ì„  ì„¸íŒ…)
    sources = [
        {"name": "NIA", "url": "https://www.nia.or.kr/site/nia_kor/ex/bbs/List.do?cbIdx=82618", "selector": "tr"},
        {"name": "AITimes", "url": "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1", "selector": ".list-block"},
        {"name": "VentureBeat", "url": "https://venturebeat.com/category/ai/", "selector": ".article-item"},
        {"name": "AINews", "url": "https://www.artificialintelligence-news.com/", "selector": ".type-post"}
    ]

    # ê³µí†µ ì¶”ì¶œ ìŠ¤í‚¤ë§ˆ
    schema = {
        "name": "News List",
        "baseSelector": "article, tr, .list-block, .article-item, .type-post",
        "fields": [
            {"name": "title", "selector": "a, .tit, h2, h3", "type": "text"},
            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"},
            {"name": "date", "selector": ".date, time, .dt", "type": "text"}
        ]
    }
    strategy = JsonCssExtractionStrategy(schema)

    today = datetime.now().strftime("%Y-%m-%d")
    final_data = []

    async with AsyncWebCrawler() as crawler:
        for source in sources:
            print(f"ğŸ“¡ {source['name']} ìˆ˜ì§‘ ì‹œë„ ì¤‘...")
            result = await crawler.arun(
                url=source['url'],
                extraction_strategy=strategy,
                bypass_cache=True,
                wait_for=source['selector'] # í˜ì´ì§€ê°€ ë‹¤ ë¡œë”©ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
            )

            if result.success and result.extracted_content:
                items = json.loads(result.extracted_content)
                count = 0
                for item in items:
                    title = item.get("title", "").strip()
                    link = item.get("link", "")
                    
                    # ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë§í¬ê°€ ì—†ìœ¼ë©´ ë¬´ì‹œ
                    if len(title) < 10 or not link: continue
                    
                    # ë§í¬ ë³´ì •
                    if link.startswith('/'):
                        from urllib.parse import urljoin
                        link = urljoin(source['url'], link)

                    final_data.append({
                        "ìˆ˜ì§‘ì¼": today,
                        "ë°œí–‰ì¼": format_date(item.get("date", "")),
                        "ì œëª©": title,
                        "ë§í¬": link
                    })
                    count += 1
                    if count >= 5: break
                print(f"âœ… {source['name']}: {count}ê°œ ìˆ˜ì§‘ ì„±ê³µ")
            else:
                print(f"âŒ {source['name']}: ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨")

    # ê²°ê³¼ ì €ì¥
    with open('ai_trend_report.csv', 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ìˆ˜ì§‘ì¼", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬"])
        writer.writeheader()
        if final_data:
            writer.writerows(final_data)
        else:
            # ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ì—ëŸ¬ í™•ì¸ìš© ìƒ˜í”Œ ë°ì´í„° í•œ ì¤„ ì‚½ì…
            writer.writerow({"ìˆ˜ì§‘ì¼": today, "ë°œí–‰ì¼": "-", "ì œëª©": "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ - ê·œì¹™ ì¬ì ê²€ í•„ìš”", "ë§í¬": "-"})

if __name__ == "__main__":
    asyncio.run(main())
