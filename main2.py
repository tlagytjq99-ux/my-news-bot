import asyncio
import csv
import os
import re
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    target_sites = {
        "AIíƒ€ì„ìŠ¤": "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "ë²¤ì²˜ë¹„íŠ¸": "https://venturebeat.com/category/ai/",
        "í…Œí¬í¬ëŸ°ì¹˜": "https://techcrunch.com/category/artificial-intelligence/",
        "ë°±ì•…ê´€(ë³´ë„ìë£Œ)": "https://www.whitehouse.gov/briefing-room/statements-releases/" # ì •ë¶€ê¸°ê´€ í…ŒìŠ¤íŠ¸ìš© ì¶”ê°€
    }

    browser_config = BrowserConfig(browser_type="chromium", headless=True)
    # ë‚ ì§œ ë°ì´í„° ë¡œë”©ì„ ìœ„í•´ ëŒ€ê¸° ì‹œê°„ ìµœì í™”
    run_config = CrawlerRunConfig(wait_for="body", wait_for_timeout=25000)
    
    final_data = []
    today_str = datetime.now().strftime("%Y-%m-%d")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for site_name, url in target_sites.items():
            try:
                print(f"ğŸ“¡ [{site_name}] ë°œí–‰ì¼ ì •ë°€ ë¶„ì„ ì¤‘...")
                result = await crawler.arun(url=url, config=run_config)

                if result.success and result.markdown:
                    # [ì œëª©](ë§í¬) íŒ¨í„´ ì¶”ì¶œ
                    links = re.findall(r'\[([^\]]{20,})\]\(([^\)]+)\)', result.markdown)
                    
                    added = 0
                    for title, link in links:
                        # ğŸš« ì´ë¯¸ì§€ ë° ë…¸ì´ì¦ˆ ì°¨ë‹¨ (ê°•í™”ë¨)
                        if "![" in title or any(ext in link.lower() for ext in ['.jpg', '.png', '.jpeg', '.gif', 'wp-content']): continue
                        
                        title_clean = re.sub(r'[\[\]\r\n\t]', '', title).strip()
                        if len(title_clean) < 25: continue
                        
                        full_link = urljoin(url, link)
                        if any(d['ì œëª©'] == title_clean for d in final_data): continue

                        # ğŸ“… [ì‚¬ì´íŠ¸ë³„ ë§ì¶¤í˜• ë°œí–‰ì¼ ì¶”ì¶œ]
                        pub_date = "í™•ì¸ë¶ˆê°€"
                        
                        # ê¸°ì‚¬ ì œëª© ê·¼ì²˜ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íƒìƒ‰ ë²”ìœ„ í™•ëŒ€
                        title_index = result.markdown.find(title)
                        search_area = result.markdown[max(0, title_index-150) : title_index+300]

                        # 1ìˆœìœ„: YYYY-MM-DD ë˜ëŠ” YYYY.MM.DD (AIíƒ€ì„ìŠ¤ ë“±)
                        date_pattern = re.search(r'(\d{4}[-./]\d{1,2}[-./]\d{1,2})', search_area)
                        
                        # 2ìˆœìœ„: ì˜ë¬¸ ë‚ ì§œ (January 20, 2026 ë“± - ë²¤ì²˜ë¹„íŠ¸/ë°±ì•…ê´€ìš©)
                        eng_date_pattern = re.search(r'([A-Z][a-z]+ \d{1,2}, \d{4})', search_area)

                        if date_pattern:
                            pub_date = date_pattern.group(1).replace('.', '-').replace('/', '-')
                        elif eng_date_pattern:
                            try:
                                # ì˜ë¬¸ ë‚ ì§œë¥¼ YYYY-MM-DDë¡œ ë³€í™˜
                                d = datetime.strptime(eng_date_pattern.group(1), "%B %d, %Y")
                                pub_date = d.strftime("%Y-%m-%d")
                            except:
                                pub_date = eng_date_pattern.group(1) # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€
                        elif "/202" in full_link: # 3ìˆœìœ„: URL ë‚´ ë‚ ì§œ í¬í•¨ ì‹œ
                            url_match = re.search(r'/(\d{4})/(\d{1,2})/(\d{1,2})/', full_link)
                            if url_match:
                                pub_date = f"{url_match.group(1)}-{url_match.group(2).zfill(2)}-{url_match.group(3).zfill(2)}"

                        # ë‚ ì§œë¥¼ ì „í˜€ ëª» ì°¾ì€ ê²½ìš°ì—ë§Œ ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì…
                        if pub_date == "í™•ì¸ë¶ˆê°€":
                            pub_date = today_str

                        final_data.append({
                            "ì¶œì²˜": site_name,
                            "ìˆ˜ì§‘ì¼": today_str,
                            "ë°œí–‰ì¼": pub_date,
                            "ì œëª©": title_clean,
                            "ë§í¬": full_link
                        })
                        added += 1
                        if added >= 10: break
                    
                    print(f"âœ… {site_name}: {added}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {site_name} ì˜¤ë¥˜: {e}")

    # CSV ì €ì¥
    file_name = 'ai_trend_report.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ì¶œì²˜", "ìˆ˜ì§‘ì¼", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬"])
        writer.writeheader()
        writer.writerows(final_data)
    
    print(f"ğŸ‰ ëª¨ë“  í•„í„°ë§ ë° ë‚ ì§œ êµì • ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(main())
