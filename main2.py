import asyncio
import csv
import os
import re
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    # 1. ğŸ”— [ì •ë³´ì› ì •ë°€ íƒ€ê²©] RSSê°€ ì•„ë‹Œ ì‹¤ì œ ë‰´ìŠ¤ ëª©ë¡ ì›¹ í˜ì´ì§€ ì£¼ì†Œ
    target_sites = {
        "AIíƒ€ì„ìŠ¤": "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "ë²¤ì²˜ë¹„íŠ¸": "https://venturebeat.com/category/ai/",
        "í…Œí¬í¬ëŸ°ì¹˜": "https://techcrunch.com/category/artificial-intelligence/",
        "AIë‰´ìŠ¤(ì˜êµ­)": "https://www.artificialintelligence-news.com/",
        "ë”ë²„ì§€(AI)": "https://www.theverge.com/ai-artificial-intelligence",
        "ì „ìì‹ ë¬¸AI": "https://www.etnews.com/news/section.html?id1=20&id2=065"
    }

    browser_config = BrowserConfig(
        browser_type="chromium", 
        headless=True,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    
    # Playwrightê°€ í˜ì´ì§€ ìë°”ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•  ì‹œê°„ì„ ì¶©ë¶„íˆ ì¤ë‹ˆë‹¤.
    run_config = CrawlerRunConfig(
        wait_for="body", 
        wait_for_timeout=20000,
        delay_before_return_html=2.0 
    )
    
    final_data = []
    today = datetime.now().strftime("%Y-%m-%d")

    # ğŸš« ë…¸ì´ì¦ˆ ì°¨ë‹¨ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    exclude_keywords = [
        "ë°”ë¡œê°€ê¸°", "ë¡œê·¸ì¸", "íšŒì›ê°€ì…", "copyright", "terms", "privacy", 
        "newsletter", "advertising", "contact", "policy", "subscribe",
        "media", "entertainment", "startup battlefield", "skip to content"
    ]

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for site_name, url in target_sites.items():
            try:
                print(f"ğŸ“¡ [{site_name}] ë‰´ìŠ¤ ëª©ë¡ ë¶„ì„ ì¤‘...")
                result = await crawler.arun(url=url, config=run_config)

                if result.success and result.markdown:
                    # [ì œëª©](ë§í¬) íŒ¨í„´ ì¶”ì¶œ (ì œëª©ì´ ìµœì†Œ 20ì ì´ìƒì¸ ê²ƒë§Œ)
                    links = re.findall(r'\[([^\]]{20,})\]\(([^\)]+)\)', result.markdown)
                    
                    added = 0
                    for title, link in links:
                        # 1. ì´ë¯¸ì§€ íƒœê·¸(![...]) ì›ì²œ ì°¨ë‹¨
                        if "![" in title: continue
                        
                        # 2. ì œëª© ì •ì œ (ë¶ˆí•„ìš”í•œ ëŒ€ê´„í˜¸, ì¤„ë°”ê¿ˆ ì œê±°)
                        title_clean = re.sub(r'[\[\]\r\n\t]', '', title).strip()
                        
                        # 3. í•„í„°ë§ ì¡°ê±´ (ì œì™¸ í‚¤ì›Œë“œ ë° ê¸¸ì´)
                        if any(kw in title_clean.lower() for kw in exclude_keywords): continue
                        if len(title_clean) < 25: continue # ë„ˆë¬´ ì§§ì€ ë©”ë‰´í˜• ì œëª© ë°°ì œ
                        
                        # 4. ë§í¬ ë³´ì •
                        full_link = urljoin(url, link)
                        
                        # 5. ì¤‘ë³µ ê¸°ì‚¬ ë°©ì§€ (ì œëª© ê¸°ì¤€)
                        if any(d['ì œëª©'] == title_clean for d in final_data): continue

                        final_data.append({
                            "ì¶œì²˜": site_name,
                            "ìˆ˜ì§‘ì¼": today,
                            "ì œëª©": title_clean,
                            "ë§í¬": full_link
                        })
                        added += 1
                        if added >= 8: break # ì‚¬ì´íŠ¸ë‹¹ ìµœëŒ€ 8ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
                    
                    print(f"âœ… {site_name}: {added}ê°œ ë‰´ìŠ¤ í™•ë³´")
            except Exception as e:
                print(f"âŒ {site_name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    # 2. ğŸ’¾ CSV ê²°ê³¼ ì €ì¥
    file_name = 'ai_trend_report.csv'
    if final_data:
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["ì¶œì²˜", "ìˆ˜ì§‘ì¼", "ì œëª©", "ë§í¬"])
            writer.writeheader()
            writer.writerows(final_data)
        print(f"ğŸ‰ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ! (ì´ {len(final_data)}ê±´)")
    else:
        # ë°ì´í„°ê°€ ì—†ì„ ë•Œë„ ë¹ˆ íŒŒì¼ì€ ìƒì„±í•˜ì—¬ ì—ëŸ¬ ë°©ì§€
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            f.write("ì¶œì²˜,ìˆ˜ì§‘ì¼,ì œëª©,ë§í¬\n-,2026-01-28,ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤,-")
        print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ ë¹ˆ íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
