import asyncio
import csv
import json
from datetime import datetime
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    browser_config = BrowserConfig(
        browser_type="chromium",
        headless=True,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )

    # 1. ì‹¤í–‰ ì„¤ì • ì¡°ì • (ê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„ì„ ì¡°ê¸ˆ ì¤„ì´ê³  ì—ëŸ¬ ì‹œ ë„˜ì–´ê°€ê²Œ í•¨)
    run_config = CrawlerRunConfig(
        wait_for="article, h2, h3", 
        wait_for_timeout=20000, # 20ì´ˆë§Œ ê¸°ë‹¤ë¦¬ê³  ì•ˆ ë‚˜ì˜¤ë©´ íŒ¨ìŠ¤
        cache_mode=CacheMode.BYPASS,
        delay_before_return_html=1.0 
    )

    schema = {
        "name": "AI_News_Extractor",
        "baseSelector": "article, .item, tr, li, .list-block", 
        "fields": [
            {"name": "title", "selector": "h2, h3, a.title, .tit", "type": "text"},
            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"},
            {"name": "date", "selector": "time, .date, .dt, span", "type": "text"}
        ]
    }
    extraction_strategy = JsonCssExtractionStrategy(schema)

    urls = [
        "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "https://venturebeat.com/category/ai/",
        "https://www.artificialintelligence-news.com/"
    ]

    final_data = []
    today = datetime.now().strftime("%Y-%m-%d")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for url in urls:
            try: # ğŸ’¡ ì•ˆì „ì¥ì¹˜ ì¶”ê°€: ì—ëŸ¬ ë‚˜ë„ ë©ˆì¶”ì§€ ë§ˆ!
                print(f"ğŸ“¡ {url} ìˆ˜ì§‘ ì‹œì‘...")
                result = await crawler.arun(
                    url=url,
                    config=run_config,
                    extraction_strategy=extraction_strategy
                )

                if result.success and result.extracted_content:
                    items = json.loads(result.extracted_content)
                    count = 0
                    for item in items:
                        title = item.get("title", "").strip()
                        link = item.get("link", "")
                        if len(title) < 10 or not link: continue
                        
                        from urllib.parse import urljoin
                        full_link = urljoin(url, link)

                        final_data.append({
                            "ìˆ˜ì§‘ì¼": today,
                            "ë°œí–‰ì¼": today,
                            "ì œëª©": title,
                            "ë§í¬": full_link
                        })
                        count += 1
                        if count >= 5: break
                    print(f"âœ… {url}: {count}ê°œ ì™„ë£Œ")
                else:
                    print(f"âš ï¸ {url}: ì¶”ì¶œ ê²°ê³¼ ì—†ìŒ")

            except Exception as e:
                print(f"âŒ {url} ì‘ì—… ì¤‘ ì—ëŸ¬ ë°œìƒ (ê±´ë„ˆëœë‹ˆë‹¤): {e}")
                continue

    # 2. ì—ëŸ¬ê°€ ë‚¬ì–´ë„ ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘ëœ ê±´ ë¬´ì¡°ê±´ ì €ì¥
    if final_data:
        with open('ai_trend_report.csv', 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["ìˆ˜ì§‘ì¼", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬"])
            writer.writeheader()
            writer.writerows(final_data)
        print(f"ğŸ‰ ì„±ê³µ! ì´ {len(final_data)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("ğŸ˜­ ì €ì¥í•  ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
