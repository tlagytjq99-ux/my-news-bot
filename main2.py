import asyncio
import csv
import re
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def get_exact_date(crawler, url, config):
    """ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì‹¤ì œ ë°œí–‰ì¼ì„ ì •ë°€ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        # âœ… AIíƒ€ì„ìŠ¤ì˜ ë™ì  ë‚ ì§œë¥¼ ì¡ê¸° ìœ„í•´ ìë°”ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¼
        result = await crawler.arun(url=url, config=config)
        if result.success and result.markdown:
            content = result.markdown
            
            # 1. AIíƒ€ì„ìŠ¤/êµ­ë‚´ì§€: 'ìŠ¹ì¸ 202X.XX.XX' ë˜ëŠ” 'ë“±ë¡ 202X...' íŒ¨í„´ (ê°€ì¥ ì •í™•)
            # ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ ë‚ ì§œ í˜•ì‹ì„ ë” ê¼¼ê¼¼íˆ ì°¾ìŠµë‹ˆë‹¤.
            date_match = re.search(r'(\d{4}[-./]\d{2}[-./]\d{2})', content)
            if date_match:
                found_date = date_match.group(1).replace('.', '-').replace('/', '-')
                # ê¸°ì‚¬ ë³¸ë¬¸ì˜ ë‚ ì§œê°€ í˜„ì¬ ì—°ë„(2025-2026)ì¸ì§€ í™•ì¸
                if found_date.startswith(('2025', '2026')):
                    return found_date

            # 2. ì˜ë¬¸í˜• íŒ¨í„´ (ë²¤ì²˜ë¹„íŠ¸/í…Œí¬í¬ëŸ°ì¹˜/ë°±ì•…ê´€)
            eng_match = re.search(r'([A-Z][a-z]+ \d{1,2}, \d{4})', content)
            if eng_match:
                dt = datetime.strptime(eng_match.group(1), "%B %d, %Y")
                return dt.strftime("%Y-%m-%d")
    except: pass
    return "ë‚ ì§œí™•ì¸í•„ìš”"

async def main():
    target_sites = {
        "AIíƒ€ì„ìŠ¤": "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "ë²¤ì²˜ë¹„íŠ¸": "https://venturebeat.com/category/ai/",
        "í…Œí¬í¬ëŸ°ì¹˜": "https://techcrunch.com/category/artificial-intelligence/",
        "ë°±ì•…ê´€(AI)": "https://www.whitehouse.gov/briefing-room/statements-releases/"
    }

    # ğŸ¯ ìµœì‹ ì„± ìœ ì§€ë¥¼ ìœ„í•œ ì„¤ì •
    allowed_years = ['2025', '2026']
    ai_keywords = ['ai', 'intelligence', 'tech', 'digital', 'data', 'algorithm', 'ì¸ê³µì§€ëŠ¥', 'ë°ì´í„°']

    browser_config = BrowserConfig(browser_type="chromium", headless=True)
    # âœ… AIíƒ€ì„ìŠ¤ ê°™ì€ ë™ì  ì‚¬ì´íŠ¸ë¥¼ ìœ„í•´ wait_for ì„¤ì •ì„ ê°•í™”í•©ë‹ˆë‹¤.
    run_config = CrawlerRunConfig(
        wait_for="body", 
        wait_for_timeout=30000,
        delay_before_return_html=3.0 # ìë°”ìŠ¤í¬ë¦½íŠ¸ê°€ ë‚ ì§œë¥¼ ë¿Œë ¤ì¤„ ì‹œê°„ì„ ì¤ë‹ˆë‹¤.
    )
    
    final_data = []
    today_str = datetime.now().strftime("%Y-%m-%d")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for site_name, url in target_sites.items():
            print(f"ğŸ“¡ [{site_name}] ìµœì‹  ë‰´ìŠ¤ ì •ë°€ ìŠ¤ìº” ì¤‘...")
            list_result = await crawler.arun(url=url, config=run_config)

            if list_result.success and list_result.markdown:
                # ë²¤ì²˜ë¹„íŠ¸ ë…¸ì´ì¦ˆ(ê³¼ê±° ê¸°ì‚¬) ì œê±°ë¥¼ ìœ„í•´ ì œëª© ê¸¸ì´ë¥¼ ë” ì—„ê²©íˆ ì œí•œ
                links = re.findall(r'\[([^\]]{30,})\]\(([^\)]+)\)', list_result.markdown)
                
                count = 0
                for title, link in links:
                    title_clean = re.sub(r'[\[\]\r\n\t]', '', title).strip()
                    
                    # ğŸ” ì •ë¶€ ê¸°ê´€ í‚¤ì›Œë“œ í•„í„°ë§
                    if site_name == "ë°±ì•…ê´€(AI)" and not any(kw in title_clean.lower() for kw in ai_keywords): continue
                    
                    full_link = urljoin(url, link)
                    if any(d['ì œëª©'] == title_clean for d in final_data): continue

                    # ğŸ“… ìƒì„¸ í˜ì´ì§€ ê¹Šì´ ë¶„ì„ (ë‚ ì§œ ì¶”ì¶œ)
                    print(f"   ğŸ” ë°œí–‰ì¼ í™•ì¸: {title_clean[:12]}...")
                    exact_date = await get_exact_date(crawler, full_link, run_config)
                    
                    # ğŸš« [ì—°ë„ í•„í„°] 2025ë…„ ì´í›„ ê¸°ì‚¬ë§Œ ì—„ì„ 
                    is_recent = any(year in exact_date for year in allowed_years)
                    if not is_recent and exact_date != "ë‚ ì§œí™•ì¸í•„ìš”":
                        continue

                    final_data.append({
                        "ì¶œì²˜": site_name,
                        "ìˆ˜ì§‘ì¼": today_str,
                        "ë°œí–‰ì¼": exact_date,
                        "ì œëª©": title_clean,
                        "ë§í¬": full_link
                    })
                    count += 1
                    if count >= 6: break # í’ˆì§ˆ ìœ ì§€ë¥¼ ìœ„í•´ ì‚¬ì´íŠ¸ë‹¹ 6ê°œë¡œ ì§‘ì¤‘

    # ğŸ’¾ ìµœì‹ ìˆœ ì •ë ¬ ë° ì €ì¥
    final_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)
    
    file_name = 'ai_trend_report.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ì¶œì²˜", "ìˆ˜ì§‘ì¼", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬"])
        writer.writeheader()
        writer.writerows(final_data)
    
    print(f"ğŸ‰ êµì • ì™„ë£Œ! 2025-2026 ìµœì‹  ê¸°ì‚¬ë§Œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
