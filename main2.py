import asyncio
import csv
import re
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def get_exact_date(crawler, url, config, site_name):
    """ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œë¥¼ ì •ë°€í•˜ê²Œ ì¶”ì¶œ"""
    try:
        result = await crawler.arun(url=url, config=config)
        if not (result.success and result.markdown): return "ë‚ ì§œí™•ì¸í•„ìš”"
        
        content = result.markdown
        header = content[:2500] # ìƒë‹¨ ì˜ì—­ ì§‘ì¤‘ ë¶„ì„

        # 1. í•œêµ­ ì‚¬ì´íŠ¸ (AIíƒ€ì„ìŠ¤ ë“±)
        if any(kw in site_name for kw in ["AIíƒ€ì„ìŠ¤", "êµ­ë‚´"]):
            match = re.search(r'(\d{4}\.\d{2}\.\d{2})', header)
            if match: return match.group(1).replace('.', '-')

        # 2. ì˜ë¬¸ ì‚¬ì´íŠ¸ (ë°±ì•…ê´€, ë²¤ì²˜ë¹„íŠ¸ ë“±)
        eng_match = re.search(r'([A-Z][a-z]+ \d{1,2}, \d{4})', header)
        if eng_match:
            dt = datetime.strptime(eng_match.group(1), "%B %d, %Y")
            return dt.strftime("%Y-%m-%d")
            
        # 3. URLì—ì„œ ë‚ ì§œ íŒŒë‚´ê¸° (í…Œí¬í¬ëŸ°ì¹˜ ë“±)
        url_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', url)
        if url_match: return f"{url_match.group(1)}-{url_match.group(2)}-{url_match.group(3)}"

    except: pass
    return "ë‚ ì§œí™•ì¸í•„ìš”"

async def main():
    # ìˆ˜ì§‘ ëŒ€ìƒ ì‚¬ì´íŠ¸ (ë°±ì•…ê´€ í¬í•¨)
    target_sites = {
        "AIíƒ€ì„ìŠ¤": "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "ë²¤ì²˜ë¹„íŠ¸": "https://venturebeat.com/category/ai/",
        "í…Œí¬í¬ëŸ°ì¹˜": "https://techcrunch.com/category/artificial-intelligence/",
        "ë°±ì•…ê´€(AI)": "https://www.whitehouse.gov/briefing-room/" # ë¸Œë¦¬í•‘ë£¸ ì „ì²´ì—ì„œ ê²€ìƒ‰
    }

    # ğŸ¯ [í•µì‹¬] AI ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œ (ì´ ì¤‘ í•˜ë‚˜ë¼ë„ ì œëª©ì— ìˆì–´ì•¼ í•¨)
    ai_keywords = [
        'AI', 'ì¸ê³µì§€ëŠ¥', 'GPT', 'LLM', 'CHATGPT', 'OPENAI', 'ANTHROPIC', 
        'DEEPMIND', 'ë¨¸ì‹ ëŸ¬ë‹', 'MACHINE LEARNING', 'GENAI', 'NVIDIA', 'CHIP'
    ]
    
    allowed_years = ['2025', '2026']
    browser_config = BrowserConfig(browser_type="chromium", headless=True)
    run_config = CrawlerRunConfig(wait_for="body", delay_before_return_html=7.0)
    
    final_data = []
    today_str = datetime.now().strftime("%Y-%m-%d")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for site_name, url in target_sites.items():
            print(f"ğŸ“¡ [{site_name}] AI ë‰´ìŠ¤ ì„ ë³„ ìˆ˜ì§‘ ì¤‘...")
            list_result = await crawler.arun(url=url, config=run_config)

            if list_result.success and list_result.markdown:
                # ë§ˆí¬ë‹¤ìš´ í•€ì…‹ìœ¼ë¡œ ì œëª©/ë§í¬ ì¶”ì¶œ
                links = re.findall(r'\[([^\]]{20,})\]\(([^\)]+)\)', list_result.markdown)
                
                count = 0
                for title, link in links:
                    title_clean = re.sub(r'[\[\]\r\n\t]', '', title).strip()
                    
                    # 1ï¸âƒ£ [AI í•„í„°] ì œëª©ì— AI í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê°€ì°¨ì—†ì´ ë²„ë¦¼
                    if not any(kw in title_clean.upper() for kw in ai_keywords):
                        continue
                    
                    # 2ï¸âƒ£ [ë…¸ì´ì¦ˆ í•„í„°] ì´ë¯¸ì§€, í˜ì´ìŠ¤ë¶ ê³µìœ  ë§í¬ ë“± ì œê±°
                    if any(x in link.lower() for x in ['facebook', 'twitter', '.jpg', '.png', 'wp-content']):
                        continue

                    full_link = urljoin(url, link)
                    if any(d['ì œëª©'] == title_clean for d in final_data): continue

                    print(f"   ğŸ” AI ë‰´ìŠ¤ í™•ì¸ë¨: {title_clean[:20]}...")
                    exact_date = await get_exact_date(crawler, full_link, run_config, site_name)
                    
                    # ì—°ë„ í™•ì¸ ë° ë¯¸í™•ì¸ ë‚ ì§œ ì²˜ë¦¬
                    final_date = exact_date if exact_date != "ë‚ ì§œí™•ì¸í•„ìš”" else today_str
                    if not any(year in final_date for year in allowed_years): continue

                    final_data.append({
                        "ì¶œì²˜": site_name,
                        "ìˆ˜ì§‘ì¼": today_str,
                        "ë°œí–‰ì¼": final_date,
                        "ì œëª©": title_clean,
                        "ë§í¬": full_link
                    })
                    count += 1
                    if count >= 6: break # í’ˆì§ˆì„ ìœ„í•´ ì‚¬ì´íŠ¸ë‹¹ 6ê°œ ì—„ì„ 

    # ì •ë ¬: ì¶œì²˜ -> ë°œí–‰ì¼ìˆœ
    final_data.sort(key=lambda x: (x['ì¶œì²˜'], x['ë°œí–‰ì¼']), reverse=False)
    
    file_name = 'ai_only_trend_report.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ì¶œì²˜", "ìˆ˜ì§‘ì¼", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬"])
        writer.writeheader()
        writer.writerows(final_data)
    
    print(f"\nğŸ‰ ì™„ë£Œ! AI ê¸°ì‚¬ë§Œ ì—„ì„ ëœ '{file_name}'ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    asyncio.run(main())
