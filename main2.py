import asyncio
import csv
import re
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def get_exact_date(crawler, url, config, site_name):
    """ì‚¬ì´íŠ¸ë³„ ë§ì¶¤í˜• ë‚ ì§œ ì¶”ì¶œ (AIíƒ€ì„ìŠ¤ ì •ë°€ íƒ€ê²©)"""
    try:
        # âœ… ë” ê¸´ ëŒ€ê¸°ì‹œê°„ ë¶€ì—¬ (ë°ì´í„° ë Œë”ë§ ë³´ì¥)
        result = await crawler.arun(url=url, config=config)
        if not (result.success and result.markdown): return "ë‚ ì§œí™•ì¸í•„ìš”"
        
        content = result.markdown
        # ğŸ” ìƒë‹¨ ì˜ì—­ë§Œ ì§‘ì¤‘ ë¶„ì„ (í•˜ë‹¨ ì¹´í”¼ë¼ì´íŠ¸/ì˜¤ëŠ˜ë‚ ì§œ ë…¸ì´ì¦ˆ ì œê±°)
        header_text = content[:2000]

        if site_name == "AIíƒ€ì„ìŠ¤":
            # 1ìˆœìœ„: 'ìŠ¹ì¸ 2026.01.28 14:30' íŒ¨í„´ (ê°€ì¥ ì •í™•)
            match = re.search(r'(?:ìŠ¹ì¸|ë“±ë¡|ìˆ˜ì •)\s+(\d{4}\.\d{2}\.\d{2})', header_text)
            if match: return match.group(1).replace('.', '-')
            # 2ìˆœìœ„: '2026.01.28 14:30' íŒ¨í„´
            match2 = re.search(r'(\d{4}\.\d{2}\.\d{2})\s+\d{2}:\d{2}', header_text)
            if match2: return match2.group(1).replace('.', '-')

        # ì˜ë¬¸ ì‚¬ì´íŠ¸ (ë°±ì•…ê´€, í…Œí¬í¬ëŸ°ì¹˜ ë“±)
        eng_match = re.search(r'([A-Z][a-z]+ \d{1,2}, \d{4})', header_text)
        if eng_match:
            try:
                dt = datetime.strptime(eng_match.group(1), "%B %d, %Y")
                return dt.strftime("%Y-%m-%d")
            except: pass

        # URLì—ì„œ ë‚ ì§œ ì¶”ì¶œ (í…Œí¬í¬ëŸ°ì¹˜/ë²¤ì²˜ë¹„íŠ¸ ë³´ì¡°)
        url_date = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', url)
        if url_date:
            return f"{url_date.group(1)}-{url_date.group(2)}-{url_date.group(3)}"
            
    except: pass
    return "ë‚ ì§œí™•ì¸í•„ìš”"

async def main():
    target_sites = {
        "AIíƒ€ì„ìŠ¤": "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "ë²¤ì²˜ë¹„íŠ¸": "https://venturebeat.com/category/ai/",
        "í…Œí¬í¬ëŸ°ì¹˜": "https://techcrunch.com/category/artificial-intelligence/",
        "ë°±ì•…ê´€(AI)": "https://www.whitehouse.gov/?s=AI" # ëŒ€í‘œë‹˜ ìš”ì²­í•˜ì‹  ê²€ìƒ‰ í•„í„° ì£¼ì†Œ
    }

    allowed_years = ['2025', '2026']
    browser_config = BrowserConfig(browser_type="chromium", headless=True)
    # âœ… AIíƒ€ì„ìŠ¤ì˜ ëŠë¦° ë Œë”ë§ì„ ìœ„í•´ delay_before_return_htmlì„ 8ì´ˆë¡œ ìƒí–¥
    run_config = CrawlerRunConfig(
        wait_for="body", 
        delay_before_return_html=8.0 
    )
    
    final_data = []
    today_str = datetime.now().strftime("%Y-%m-%d")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for site_name, url in target_sites.items():
            print(f"ğŸ“¡ [{site_name}] ë¶„ì„ ì¤‘...")
            list_result = await crawler.arun(url=url, config=run_config)

            if list_result.success and list_result.markdown:
                # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                links = re.findall(r'\[([^\]]{15,})\]\(([^\)]+)\)', list_result.markdown)
                
                count = 0
                for title, link in links:
                    title_clean = re.sub(r'[\[\]\r\n\t]', '', title).strip()
                    
                    # ë…¸ì´ì¦ˆ í•„í„°ë§
                    if any(x in link.lower() for x in ['search', 'category', 'facebook', 'twitter', '.jpg']): continue
                    if site_name == "ë°±ì•…ê´€(AI)" and "AI" not in title_clean.upper(): continue 

                    full_link = urljoin(url, link)
                    if any(d['ì œëª©'] == title_clean for d in final_data): continue

                    print(f"   ğŸ” ë‚ ì§œ ì •ë°€ ì¶”ì¶œ: {title_clean[:12]}...")
                    exact_date = await get_exact_date(crawler, full_link, run_config, site_name)
                    
                    # ğŸ“… ë‚ ì§œ ê²€ì¦ ë° ì—°ë„ í•„í„°
                    # 'ë‚ ì§œí™•ì¸í•„ìš”'ê°€ ë–´ì„ ë•Œ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ë®ì–´ì“°ì§€ ì•Šê³  ì‹¤ì œ ê³¼ê±° ë‚ ì§œë¥¼ ì°¾ë„ë¡ ìœ ë„
                    if exact_date == "ë‚ ì§œí™•ì¸í•„ìš”":
                        # ìµœí›„ì˜ ìˆ˜ë‹¨: URLì—ì„œë¼ë„ ë‚ ì§œë¥¼ ì°¾ìŒ
                        url_date = re.search(r'(\d{4})[-/](\d{2})[-/](\d{2})', full_link)
                        if url_date: exact_date = f"{url_date.group(1)}-{url_date.group(2)}-{url_date.group(3)}"
                    
                    if not any(year in exact_date for year in allowed_years):
                        if exact_date != "ë‚ ì§œí™•ì¸í•„ìš”": continue

                    final_data.append({
                        "ì¶œì²˜": site_name,
                        "ìˆ˜ì§‘ì¼": today_str,
                        "ë°œí–‰ì¼": exact_date,
                        "ì œëª©": title_clean,
                        "ë§í¬": full_link
                    })
                    count += 1
                    if count >= 8: break

    # âœ… ì •ë ¬: ì¶œì²˜ë³„ -> ë°œí–‰ì¼ìˆœ(ìµœì‹ ìˆœ)
    final_data.sort(key=lambda x: (x['ì¶œì²˜'], x['ë°œí–‰ì¼']), reverse=False)
    
    file_name = 'ai_trend_report.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ì¶œì²˜", "ìˆ˜ì§‘ì¼", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬"])
        writer.writeheader()
        writer.writerows(final_data)
    
    print(f"\nğŸ‰ êµì • ì™„ë£Œ! ì—‘ì…€ íŒŒì¼ì„ í™•ì¸í•´ë³´ì„¸ìš”.")

if __name__ == "__main__":
    asyncio.run(main())
