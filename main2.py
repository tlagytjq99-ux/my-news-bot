import asyncio
import csv
import re
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def get_exact_date(crawler, url, config, site_name):
    """ìƒì„¸ í˜ì´ì§€ ë‚ ì§œ ì¶”ì¶œ (AIíƒ€ì„ìŠ¤ ë¡œë”© ëŒ€ê¸° ê°•í™”)"""
    try:
        # AIíƒ€ì„ìŠ¤ì¼ ê²½ìš° ë” ì˜¤ë˜ ê¸°ë‹¤ë¦¬ë„ë¡ ì„¤ì • ë³€ê²½
        current_config = config
        if site_name == "AIíƒ€ì„ìŠ¤":
            current_config.wait_for = ".date" # ë‚ ì§œ í´ë˜ìŠ¤ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            current_config.delay_before_return_html = 8.0 # ì¶©ë¶„í•œ ë¡œë”© ì‹œê°„

        result = await crawler.arun(url=url, config=current_config)
        if not (result.success and result.markdown): return "ë‚ ì§œí™•ì¸í•„ìš”"
        
        content = result.markdown[:3000] # ìƒë‹¨ ë°ì´í„° ì§‘ì¤‘ ë¶„ì„

        # 1. AIíƒ€ì„ìŠ¤ ì •ë°€ ë¶„ì„ (2026.01.29 10:30 í˜•íƒœ ëŒ€ì‘)
        if site_name == "AIíƒ€ì„ìŠ¤":
            date_match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})', content)
            if date_match: return f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"

        # 2. ë°±ì•…ê´€/í•´ì™¸ ì‚¬ì´íŠ¸ (January 29, 2026 í˜•íƒœ)
        eng_match = re.search(r'([A-Z][a-z]+ \d{1,2}, \d{4})', content)
        if eng_match:
            dt = datetime.strptime(eng_match.group(1), "%B %d, %Y")
            return dt.strftime("%Y-%m-%d")

    except: pass
    return datetime.now().strftime("%Y-%m-%d") # ì‹¤íŒ¨ ì‹œ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ë°©ì–´

async def main():
    # ğŸ¯ ë°±ì•…ê´€ì€ 'AI' ê²€ìƒ‰ ê²°ê³¼ ì£¼ì†Œë¡œ ì§ì ‘ ì ‘ì†í•˜ë„ë¡ ìˆ˜ì •
    target_sites = {
        "AIíƒ€ì„ìŠ¤": "https://www.aitimes.com/news/articleList.html?sc_section_code=S1N1",
        "ë²¤ì²˜ë¹„íŠ¸": "https://venturebeat.com/category/ai/",
        "ë°±ì•…ê´€(AIê²€ìƒ‰)": "https://www.whitehouse.gov/?s=AI&post_type=briefing-room" 
    }

    browser_config = BrowserConfig(browser_type="chromium", headless=True)
    run_config = CrawlerRunConfig(wait_for="body", delay_before_return_html=5.0)
    
    final_data = []
    today_str = datetime.now().strftime("%Y-%m-%d")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        for site_name, url in target_sites.items():
            print(f"ğŸ“¡ [{site_name}] ìˆ˜ì§‘ ì¤‘...")
            list_result = await crawler.arun(url=url, config=run_config)

            if list_result.success and list_result.markdown:
                # ë§ˆí¬ë‹¤ìš´ í•€ì…‹ ì¶”ì¶œ
                links = re.findall(r'\[([^\]]{25,})\]\(([^\)]+)\)', list_result.markdown)
                
                count = 0
                for title, link in links:
                    title_clean = re.sub(r'[\[\]\r\n\t]', '', title).strip()
                    if any(x in link.lower() for x in ['facebook', 'twitter', '.jpg']): continue

                    full_link = urljoin(url, link)
                    if any(d['ì œëª©'] == title_clean for d in final_data): continue

                    print(f"   ğŸ” ìƒì„¸ ë¶„ì„: {title_clean[:20]}...")
                    exact_date = await get_exact_date(crawler, full_link, run_config, site_name)

                    final_data.append({
                        "ì¶œì²˜": site_name,
                        "ìˆ˜ì§‘ì¼": today_str,
                        "ë°œí–‰ì¼": exact_date,
                        "ì œëª©": title_clean,
                        "ë§í¬": full_link
                    })
                    count += 1
                    if count >= 5: break # ì‚¬ì´íŠ¸ë‹¹ 5ê°œì”©

    # ì €ì¥ (CSV)
    file_name = 'ai_trend_report.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ì¶œì²˜", "ìˆ˜ì§‘ì¼", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬"])
        writer.writeheader()
        writer.writerows(final_data)
    
    print(f"\nğŸ‰ ì™„ë£Œ! '{file_name}' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    asyncio.run(main())
