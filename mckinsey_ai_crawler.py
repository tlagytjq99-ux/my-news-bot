import feedparser
import csv
import time
import base64
import re
from datetime import datetime
from googletrans import Translator

def get_original_url(google_url):
    """êµ¬ê¸€ ë‰´ìŠ¤ ë§í¬ë¥¼ ì›ë˜ì˜ ì›ë³¸ URLë¡œ ë³€í™˜"""
    try:
        if "articles/" in google_url:
            base64_url = google_url.split("articles/")[1].split("?")[0]
            base64_url += "=" * ((4 - len(base64_url) % 4) % 4)
            decoded_bytes = base64.urlsafe_b64decode(base64_url)
            decoded_str = decoded_bytes.decode('latin-1')
            if "http" in decoded_str:
                start_idx = decoded_str.find("http")
                clean_url = re.split(r'[^\w\d\.\/\:\%\?\&\=\-\+\_\~\#]', decoded_str[start_idx:])[0]
                return clean_url
    except: pass
    return google_url

def main():
    # ğŸ¯ ë” ì •êµí•´ì§„ ì¿¼ë¦¬ (AI/Tech ë¦¬í¬íŠ¸ì— ì§‘ì¤‘)
    sources = [
        {"name": "McKinsey", "url": "https://www.mckinsey.com/insights/rss"},
        {"name": "MIT_Sloan", "url": "https://sloanreview.mit.edu/feed/"},
        {"name": "Deloitte", "url": "https://news.google.com/rss/search?q=site:deloitte.com/insights+AI+OR+Generative+OR+Technology&hl=en-US&gl=US&ceid=US:en"},
        {"name": "BCG", "url": "https://news.google.com/rss/search?q=site:bcg.com+AI+OR+Generative+OR+Tech+Insight&hl=en-US&gl=US&ceid=US:en"}
    ]
    
    file_name = 'ai_market_intelligence.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")
    
    # ğŸ’¡ ê¸ì • í‚¤ì›Œë“œ (ì´ ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•¨)
    positive_kws = ['AI', 'GEN', 'DIGITAL', 'TECH', 'INTELLIGENCE', 'DATA', 'CLOUD', 'AUTOMATION', 'ALGORITHM', 'TRENDS', 'OUTLOOK']
    # ğŸ’¡ ë¶€ì • í‚¤ì›Œë“œ (ì´ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ íƒˆë½)
    negative_kws = ['JOB', 'CAREER', 'HIRE', 'RECRUIT', 'WORKSHOP', 'WELCOME', 'APPLY', 'GRADUATE', 'STUDENT', 'HOME', 'CONTACT']

    print(f"ğŸ“¡ [ì •ë°€ í•„í„°ë§ ì—”ì§„] ìˆ˜ì§‘ ë° í•„í„°ë§ ì‹œì‘...")
    new_data = []

    for source in sources:
        print(f"ğŸ” {source['name']} ë¶„ì„ ì¤‘...")
        try:
            feed = feedparser.parse(source['url'])
            count = 0
            for entry in feed.entries:
                title_en = entry.title.split(' - ')[0]
                upper_title = title_en.upper()

                # 1ë‹¨ê³„: ë¶€ì • í‚¤ì›Œë“œ í•„í„°ë§ (ì±„ìš©/ê³µê³  ë“± ì œê±°)
                if any(nk in upper_title for nk in negative_kws):
                    continue
                
                # 2ë‹¨ê³„: ê¸ì • í‚¤ì›Œë“œ í•„ìˆ˜ í¬í•¨ í•„í„°ë§ (AI/í…Œí¬ ê´€ë ¨ì„± ë³´ì¥)
                if not any(pk in upper_title for pk in positive_kws):
                    continue

                # 3ë‹¨ê³„: ë‚ ì§œ ìœ íš¨ì„± ê²€ì‚¬ (ë„ˆë¬´ ì˜¤ë˜ëœ ë°ì´í„° ì œì™¸)
                raw_date = entry.get('published_parsed', None)
                if raw_date:
                    year = raw_date.tm_year
                    if year < 2024: continue # 2024ë…„ ì´ì „ ìë£ŒëŠ” íŒ¨ìŠ¤
                    published_date = time.strftime('%Y-%m-%d', raw_date)
                else:
                    published_date = collected_date

                # ë§í¬ ë³€í™˜
                final_link = get_original_url(entry.link) if "google.com" in entry.link else entry.link

                try:
                    title_ko = translator.translate(title_en, dest='ko').text
                except:
                    title_ko = title_en

                new_data.append({
                    "ê¸°ê´€": source['name'],
                    "ë°œí–‰ì¼": published_date,
                    "ì œëª©": title_ko,
                    "ì›ë¬¸": title_en,
                    "ë§í¬": final_link,
                    "ìˆ˜ì§‘ì¼": collected_date
                })
                count += 1
                if count >= 10: break
            print(f"   âœ… {source['name']} ì •ì˜ˆ ë¦¬í¬íŠ¸ {count}ê±´ í™•ë³´!")
        except Exception as e:
            print(f"   âŒ {source['name']} ì—ëŸ¬: {e}")

    # ğŸ’¾ ì €ì¥
    if new_data:
        new_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
            writer.writeheader()
            writer.writerows(new_data)
        print(f"\nğŸ‰ í•„í„°ë§ ì™„ë£Œ! {len(new_data)}ê±´ì˜ ê³ í€„ë¦¬í‹° ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
