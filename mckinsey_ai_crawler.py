import feedparser
import csv
import os
import time
from datetime import datetime
from googletrans import Translator

def main():
    # ğŸ¯ ë§¥í‚¨ì§€ + PwC ê³µì‹ ë³´ë„ìë£Œ ì±„ë„ (PR Newswire)
    sources = [
        {"name": "McKinsey", "url": "https://www.mckinsey.com/insights/rss"},
        # ğŸ’¡ PwCê°€ ê³µì‹ ë³´ë„ìë£Œë¥¼ ë¿Œë¦¬ëŠ” ê¸€ë¡œë²Œ ë‰´ìŠ¤ í”¼ë“œì…ë‹ˆë‹¤. (ì°¨ë‹¨ ë¶ˆê°€)
        {"name": "PwC_Official", "url": "https://www.prnewswire.com/rss/news-releases-list.rss?search=PwC"}
    ]
    
    file_name = 'ai_market_intelligence.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")
    
    print(f"ğŸ“¡ [ìµœí›„ì˜ ìˆ˜ë‹¨] PwC ë‰´ìŠ¤ í”¼ë“œ ìˆ˜ì§‘ ì‹œì‘...")

    new_data = []
    ai_keywords = ['AI', 'TECH', 'DIGITAL', 'INTELLIGENCE', 'DATA', 'GEN', 'CLOUD', 'ESG']

    for source in sources:
        print(f"ğŸ” {source['name']} ë¶„ì„ ì¤‘...")
        try:
            # PR NewswireëŠ” ë´‡ ì°¨ë‹¨ì´ ê±°ì˜ ì—†ì–´ ì˜ ëš«ë¦½ë‹ˆë‹¤.
            feed = feedparser.parse(source['url'])
            
            if not feed.entries:
                print(f"   âš ï¸ {source['name']} ì‘ë‹µ ì—†ìŒ. (ì£¼ì†Œë¥¼ ì ê²€ ì¤‘...)")
                continue

            count = 0
            for entry in feed.entries:
                title_en = entry.title
                link = entry.link
                
                raw_date = entry.get('published_parsed', None)
                published_date = time.strftime('%Y-%m-%d', raw_date) if raw_date else collected_date

                # ì œëª©ì— í‚¤ì›Œë“œ í™•ì¸ (PwCëŠ” AIë¿ë§Œ ì•„ë‹ˆë¼ ë””ì§€í„¸ ì „í™˜ ì „ì²´ë¥¼ ë´…ë‹ˆë‹¤)
                upper_title = title_en.upper()
                if any(kw in upper_title for kw in ai_keywords):
                    try:
                        res = translator.translate(title_en, src='en', dest='ko')
                        title_ko = res.text
                    except:
                        title_ko = title_en

                    print(f"   âœ… [ì„±ê³µ] {source['name']}: {title_ko[:25]}...")

                    new_data.append({
                        "ê¸°ê´€": source['name'],
                        "ë°œí–‰ì¼": published_date,
                        "ì œëª©": title_ko,
                        "ì›ë¬¸": title_en,
                        "ë§í¬": link,
                        "ìˆ˜ì§‘ì¼": collected_date
                    })
                    count += 1
                    if count >= 15: break
            
            print(f"   âœ… {source['name']}ì—ì„œ {count}ê±´ í™•ë³´!")

        except Exception as e:
            print(f"   âŒ {source['name']} ì—ëŸ¬: {e}")

    # ğŸ’¾ ì €ì¥ ë¡œì§
    if new_data:
        new_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(new_data)
        print(f"\nğŸ‰ ë“œë””ì–´ ë§ˆì¹¨í‘œ! ì´ {len(new_data)}ê±´ì˜ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nğŸ’¡ ìƒˆë¡œ ë°œê²¬ëœ ì „ëµ ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
