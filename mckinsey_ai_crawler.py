import requests
from bs4 import BeautifulSoup
import feedparser
import csv
import os
import time
from datetime import datetime
from googletrans import Translator

def main():
    file_name = 'ai_market_intelligence.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")
    
    print(f"ğŸ“¡ [í†µí•© ì—”ì§„] ì „ëµ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹œì‘ (McKinsey + PwC)...")
    new_data = []

    # --- [ì„¹ì…˜ 1: McKinsey ìˆ˜ì§‘ (RSS ë°©ì‹)] ---
    print(f"ğŸ” McKinsey ë¶„ì„ ì¤‘...")
    try:
        mck_feed = feedparser.parse("https://www.mckinsey.com/insights/rss")
        mck_count = 0
        for entry in mck_feed.entries:
            title_en = entry.title
            # AI ë° ë¹„ì¦ˆë‹ˆìŠ¤ í•µì‹¬ í‚¤ì›Œë“œ í•„í„°
            if any(kw in title_en.upper() for kw in ['AI', 'TECH', 'DIGITAL', 'DATA', 'GEN', 'STRATEGY']):
                try:
                    title_ko = translator.translate(title_en, src='en', dest='ko').text
                except:
                    title_ko = title_en
                
                new_data.append({
                    "ê¸°ê´€": "McKinsey",
                    "ë°œí–‰ì¼": time.strftime('%Y-%m-%d', entry.published_parsed) if 'published_parsed' in entry else collected_date,
                    "ì œëª©": title_ko,
                    "ì›ë¬¸": title_en,
                    "ë§í¬": entry.link,
                    "ìˆ˜ì§‘ì¼": collected_date
                })
                mck_count += 1
            if mck_count >= 10: break
        print(f"   âœ… McKinseyì—ì„œ {mck_count}ê±´ í™•ë³´!")
    except Exception as e:
        print(f"   âš ï¸ McKinsey ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    # --- [ì„¹ì…˜ 2: PwC ìˆ˜ì§‘ (ì§ì ‘ í¬ë¡¤ë§ ë°©ì‹)] ---
    print(f"ğŸ” PwC ê¸°ìˆ  ì„¹ì…˜ ê³µëµ ì¤‘...")
    pwc_url = "https://www.pwc.com/gx/en/issues/technology.html"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    try:
        response = requests.get(pwc_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # PwCì˜ ì‹¤ì œ ë¦¬í¬íŠ¸ ì œëª©ë“¤ì€ ë³´í†µ h3 ë˜ëŠ” íŠ¹ì • í´ë˜ìŠ¤ì˜ a íƒœê·¸ì— ë‹´ê¹ë‹ˆë‹¤.
            articles = soup.find_all(['h3', 'h4'])
            pwc_count = 0
            
            for art in articles:
                title_en = art.get_text().strip()
                
                # ğŸ’¡ í•µì‹¬ í•„í„°: 
                # 1. ì œëª©ì´ 25ì ì´ìƒì´ì–´ì•¼ í•¨ (ë©”ë‰´ ì´ë¦„ì€ ë³´í†µ ì§§ìŒ)
                # 2. í…Œí¬ ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ì•¼ í•¨
                tech_keywords = ['AI', 'GEN', 'DIGITAL', 'INTELLIGENCE', 'TECH', 'DATA', 'FUTURE', 'TRANSFORMATION']
                
                if len(title_en) > 25 and any(kw in title_en.upper() for kw in tech_keywords):
                    link_tag = art.find_parent('a') or art.find('a') or art.select_one('a')
                    if not link_tag: # ì£¼ë³€ì—ì„œ ë§í¬ ì°¾ê¸° ì‹œë„
                        link_tag = art.find_next('a') or art.find_previous('a')
                        
                    if link_tag:
                        href = link_tag.get('href', '')
                        full_url = f"https://www.pwc.com{href}" if href.startswith('/') else href
                        
                        # ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€
                        if not any(d['ì›ë¬¸'] == title_en for d in new_data):
                            try:
                                title_ko = translator.translate(title_en, src='en', dest='ko').text
                            except:
                                title_ko = title_en

                            new_data.append({
                                "ê¸°ê´€": "PwC",
                                "ë°œí–‰ì¼": collected_date, # ì›¹í˜ì´ì§€ ì§ì ‘ ìˆ˜ì§‘ì€ ì •í™•í•œ ë‚ ì§œ ì¶”ì¶œì´ ì–´ë ¤ì›Œ ìˆ˜ì§‘ì¼ë¡œ í‘œì‹œ
                                "ì œëª©": title_ko,
                                "ì›ë¬¸": title_en,
                                "ë§í¬": full_url,
                                "ìˆ˜ì§‘ì¼": collected_date
                            })
                            pwc_count += 1
                if pwc_count >= 10: break
            print(f"   âœ… PwCì—ì„œ {pwc_count}ê±´ ì •ë°€ í™•ë³´!")
        else:
            print(f"   âŒ PwC ì ‘ì† ì‹¤íŒ¨ (ì½”ë“œ: {response.status_code})")
    except Exception as e:
        print(f"   âŒ PwC í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")

    # --- [ì„¹ì…˜ 3: ë°ì´í„° ì €ì¥] ---
    if new_data:
        # ë°œí–‰ì¼ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        new_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(new_data)
        print(f"\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! {file_name} íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print("\nğŸ’¡ ìˆ˜ì§‘ëœ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
