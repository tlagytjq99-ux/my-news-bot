import feedparser
import csv
import time
import requests
import base64
from datetime import datetime
from googletrans import Translator

def get_original_url(google_url):
    """êµ¬ê¸€ ë‰´ìŠ¤ ë§í¬ë¥¼ ì›ë˜ì˜ ì›ë³¸ URLë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # êµ¬ê¸€ ë‰´ìŠ¤ ë§í¬ì˜ ì¤‘ê°„ ì•”í˜¸í™” ë¶€ë¶„ì„ ì¶”ì¶œí•˜ì—¬ ë³µí˜¸í™” ì‹œë„
        if "articles/" in google_url:
            base64_url = google_url.split("articles/")[1].split("?")[0]
            # êµ¬ê¸€ì˜ ë³€í˜•ëœ base64 íŒ¨ë”© ì²˜ë¦¬
            base64_url += "=" * ((4 - len(base64_url) % 4) % 4)
            decoded_bytes = base64.urlsafe_b64decode(base64_url)
            # ë³µí˜¸í™”ëœ ë°”ì´íŠ¸ ë°ì´í„°ì—ì„œ ì‹¤ì œ URL íŒ¨í„´ ì¶”ì¶œ
            decoded_str = decoded_bytes.decode('latin-1')
            if "http" in decoded_str:
                # ë¶ˆí•„ìš”í•œ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ ì œê±°í•˜ê³  URLë§Œ ì¶”ì¶œ
                start_idx = decoded_str.find("http")
                # URL ëë¶€ë¶„ì˜ ì°Œêº¼ê¸° ì œê±° (ì¼ë°˜ì ì¸ URL ë¬¸ì ë²”ìœ„ë¡œ í•œì •)
                import re
                clean_url = re.split(r'[^\w\d\.\/\:\%\?\&\=\-\+\_\~\#]', decoded_str[start_idx:])[0]
                return clean_url
    except:
        pass
    return google_url # ì‹¤íŒ¨ ì‹œ êµ¬ê¸€ ë§í¬ ìœ ì§€

def main():
    # ğŸ¯ ë”œë¡œì´íŠ¸ ì¿¼ë¦¬ë¥¼ ë” ì •êµí•˜ê²Œ ìˆ˜ì • (insights ì„¹ì…˜ ì§‘ì¤‘)
    sources = [
        {"name": "McKinsey", "url": "https://www.mckinsey.com/insights/rss"},
        {"name": "MIT_Sloan", "url": "https://sloanreview.mit.edu/feed/"},
        {"name": "Deloitte", "url": "https://news.google.com/rss/search?q=site:deloitte.com+AI+insights&hl=en-US&gl=US&ceid=US:en"},
        {"name": "BCG", "url": "https://news.google.com/rss/search?q=site:bcg.com+AI&hl=en-US&gl=US&ceid=US:en"}
    ]
    
    file_name = 'ai_market_intelligence.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")
    
    print(f"ğŸ“¡ [ë§í¬ ë³µì› ì—”ì§„] ìˆ˜ì§‘ ë° URL ë””ì½”ë”© ì‹œì‘...")
    new_data = []

    for source in sources:
        print(f"ğŸ” {source['name']} ë¶„ì„ ì¤‘...")
        try:
            feed = feedparser.parse(source['url'])
            count = 0
            for entry in feed.entries:
                title_en = entry.title.split(' - ')[0]
                google_link = entry.link
                
                # ğŸ’¡ êµ¬ê¸€ ë§í¬ë¥¼ ì›ë˜ ì£¼ì†Œë¡œ ë³€í™˜
                if "google.com" in google_link:
                    final_link = get_original_url(google_link)
                else:
                    final_link = google_link

                raw_date = entry.get('published_parsed', None)
                published_date = time.strftime('%Y-%m-%d', raw_date) if raw_date else collected_date

                # ì œëª© ë²ˆì—­ ë° ìˆ˜ì§‘
                try:
                    title_ko = translator.translate(title_en, dest='ko').text
                except:
                    title_ko = title_en

                new_data.append({
                    "ê¸°ê´€": source['name'],
                    "ë°œí–‰ì¼": published_date,
                    "ì œëª©": title_ko,
                    "ì›ë¬¸": title_en,
                    "ë§í¬": final_link,
                    "ìˆ˜ì§‘ì¼": collected_date
                })
                count += 1
                if count >= 10: break
            print(f"   âœ… {source['name']} {count}ê±´ í™•ë³´ ì™„ë£Œ!")
        except Exception as e:
            print(f"   âŒ {source['name']} ì—ëŸ¬: {e}")

    # ğŸ’¾ ì €ì¥
    if new_data:
        new_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
            writer.writeheader()
            writer.writerows(new_data)
        print(f"\nğŸ‰ ì„±ê³µ! ì´ì œ ê¹¨ë—í•œ ì›ë³¸ ë§í¬ë¡œ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()
