import feedparser
import csv
import urllib.parse
import requests
import base64
import re
from datetime import datetime
from googletrans import Translator

def get_original_url(google_url):
    """êµ¬ê¸€ ë‰´ìŠ¤ì˜ ì•”í˜¸í™”ëœ URLì„ ë¶„ì„í•˜ì—¬ ì›ë³¸ URLì„ ê°•ì œë¡œ ì¶”ì¶œ"""
    try:
        # 1. êµ¬ê¸€ ë‰´ìŠ¤ ë§í¬ì—ì„œ ì•”í˜¸í™”ëœ ë°ì´í„° ë¶€ë¶„ ì¶”ì¶œ
        # https://news.google.com/rss/articles/CBMi... í˜•íƒœì—ì„œ CBMi... ë¶€ë¶„
        path = google_url.split('/')[-1].split('?')[0]
        
        # 2. Base64 ë””ì½”ë”© ì‹œë„ (êµ¬ê¸€ì´ ì‚¬ìš©í•˜ëŠ” ë°©ì‹)
        # íŒ¨ë”© ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ '===' ì¶”ê°€
        decoded_bytes = base64.urlsafe_b64decode(path + '===')
        decoded_str = decoded_bytes.decode('latin-1')
        
        # 3. ë””ì½”ë”©ëœ ë¬¸ìì—´ì—ì„œ URL íŒ¨í„´(http...)ì„ ì •ê·œì‹ìœ¼ë¡œ ì°¾ì•„ëƒ„
        urls = re.findall(r'https?://[^\x00-\x1f\x7f-\xff]+', decoded_str)
        
        if urls:
            # ë°œê²¬ëœ URL ì¤‘ ê°€ì¥ ê¸´ ê²ƒì´ ëŒ€ê°œ ì›ë³¸ ì£¼ì†Œì…ë‹ˆë‹¤.
            actual_url = max(urls, key=len)
            # ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆ ì œê±°
            actual_url = actual_url.split('?')[0].split('\x01')[0].split('\x03')[0]
            return actual_url
            
        # 4. ìœ„ ë°©ì‹ ì‹¤íŒ¨ ì‹œ, ì‹¤ì œ ì ‘ì† í›„ ê²½ë¡œ ì¶”ì  (Fallback)
        res = requests.get(google_url, timeout=5, allow_redirects=True)
        return res.url
    except:
        return google_url

def main():
    query = 'site:oecd.org (intitle:"Artificial Intelligence" OR intitle:AI) -intitle:PISA'
    encoded_query = urllib.parse.quote(query)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
    
    file_name = 'oecd_ai_intelligence.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")

    print(f"ğŸ“¡ OECD ë°ì´í„° ìˆ˜ì§‘ ë° ë§í¬ í•´ë… ì‹œì‘...")
    raw_data = []

    try:
        feed = feedparser.parse(rss_url)
        entries = sorted(feed.entries, key=lambda x: x.get('published_parsed'), reverse=True)
        
        count = 0
        for entry in entries:
            if count >= 5: break
            
            title_en = entry.title.split(' - ')[0]
            
            # í‚¤ì›Œë“œ í•„í„°ë§
            keywords = ['AI', 'ARTIFICIAL', 'INTELLIGENCE', 'ALGORITHMS', 'GENERATIVE']
            if not any(kw in title_en.upper() for kw in keywords):
                continue

            print(f"ğŸ”— {count+1}ë²ˆì§¸ ì›ë³¸ ë§í¬ í•´ë… ì¤‘...")
            # ğŸ’¡ [í•µì‹¬] ì•”í˜¸ í•´ë… ë° ë¦¬ë‹¤ì´ë ‰íŠ¸ ì¶”ì 
            actual_link = get_original_url(entry.link)
            
            pub_date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d') if hasattr(entry, 'published_parsed') else collected_date

            try:
                title_ko = translator.translate(title_en.strip(), dest='ko').text
            except:
                title_ko = title_en

            raw_data.append({
                "ê¸°ê´€": "OECD", "ë°œí–‰ì¼": pub_date, "ì œëª©": title_ko,
                "ì›ë¬¸": title_en, "ë§í¬": actual_link, "ìˆ˜ì§‘ì¼": collected_date
            })
            count += 1

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")

    # ğŸ’¾ ì €ì¥
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
        writer.writeheader()
        if raw_data:
            writer.writerows(raw_data)
            print(f"âœ… ì™„ë£Œ! {len(raw_data)}ê±´ì˜ ì›ë³¸ ë§í¬ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
