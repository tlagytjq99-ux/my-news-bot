import feedparser
import csv
import urllib.parse
import requests
from datetime import datetime
from googletrans import Translator

def resolve_google_url(google_url):
    """êµ¬ê¸€ì˜ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë²½ì„ ëš«ê³  ì‹¤ì œ ì›ë³¸ URLì„ ì°¾ì•„ë‚´ëŠ” í•¨ìˆ˜"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    try:
        # ğŸ’¡ í•µì‹¬: ì„¸ì…˜ì„ ì‚¬ìš©í•˜ê³  allow_redirects=Trueë¡œ ëê¹Œì§€ ì¶”ì í•©ë‹ˆë‹¤.
        session = requests.Session()
        response = session.get(google_url, headers=headers, timeout=10, allow_redirects=True)
        
        # ë§ˆì§€ë§‰ìœ¼ë¡œ ë„ì°©í•œ URLì´ ì›ë³¸ ì£¼ì†Œì…ë‹ˆë‹¤.
        final_url = response.url
        
        # ë§Œì•½ ì—¬ì „íˆ google.comì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²´ì¸ì„ ë‹¤ì‹œ í™•ì¸
        if "google.com" in final_url and response.history:
            final_url = response.history[-1].headers.get('Location', final_url)
            
        return final_url
    except Exception as e:
        print(f"ğŸ”— ë§í¬ ë³€í™˜ ì¤‘ ì˜¤ë¥˜(ê±´ë„ˆëœ€): {e}")
        return google_url

def main():
    query = 'site:oecd.org (intitle:"Artificial Intelligence" OR intitle:AI) -intitle:PISA'
    encoded_query = urllib.parse.quote(query)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
    
    file_name = 'oecd_ai_intelligence.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")

    print(f"ğŸ“¡ OECD ìµœì‹  ë°ì´í„° ìˆ˜ì§‘ ë° ì›ë³¸ ë§í¬ ê°•ì œ ì¶”ì¶œ ì‹œì‘...")
    raw_data = []

    try:
        feed = feedparser.parse(rss_url)
        # ìµœì‹  ë°œí–‰ ìˆœ ì •ë ¬
        entries = sorted(feed.entries, key=lambda x: x.get('published_parsed'), reverse=True)
        
        count = 0
        for entry in entries:
            if count >= 5: break
            
            title_en = entry.title.split(' - ')[0]
            
            # AI ê´€ë ¨ í‚¤ì›Œë“œ ì¬ê²€ì¦
            keywords = ['AI', 'ARTIFICIAL', 'INTELLIGENCE', 'ALGORITHMS', 'GENERATIVE']
            if not any(kw in title_en.upper() for kw in keywords):
                continue

            print(f"ğŸ”„ {count+1}ë²ˆì§¸ ë§í¬ ë¶„ì„ ì¤‘: {title_en[:30]}...")
            
            # ğŸ’¡ [í•µì‹¬] ë¦¬ë‹¤ì´ë ‰íŠ¸ ì¶”ì  ì‹¤í–‰
            actual_link = resolve_google_url(entry.link)
            
            pub_date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d') if hasattr(entry, 'published_parsed') else collected_date

            try:
                title_ko = translator.translate(title_en.strip(), dest='ko').text
            except:
                title_ko = title_en

            raw_data.append({
                "ê¸°ê´€": "OECD", "ë°œí–‰ì¼": pub_date, "ì œëª©": title_ko,
                "ì›ë¬¸": title_en, "ë§í¬": actual_link, "ìˆ˜ì§‘ì¼": collected_date
            })
            count += 1

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")

    # ğŸ’¾ ê²°ê³¼ ì €ì¥
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
        writer.writeheader()
        if raw_data:
            writer.writerows(raw_data)
            print(f"âœ… ì„±ê³µ! ì›ë³¸ ë§í¬ë¥¼ í¬í•¨í•œ {len(raw_data)}ê±´ì˜ ë³´ê³ ì„œë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ ì¡°ê±´ì— ë§ëŠ” ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
