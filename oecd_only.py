import feedparser
import csv
import urllib.parse
import requests
from datetime import datetime
from googletrans import Translator

def get_real_url(google_url):
    """êµ¬ê¸€ ë‰´ìŠ¤ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë§í¬ë¥¼ ì›ë³¸ URLë¡œ ë³€í™˜"""
    try:
        # ğŸ’¡ ì›ë³¸ ë§í¬ë¡œ ì—°ê²°ë˜ëŠ”ì§€ í™•ì¸ (ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°)
        response = requests.get(google_url, timeout=5)
        # ğŸ’¡ ìµœì¢… ë„ì°©ì§€(ì›ë³¸ ì£¼ì†Œ) ë°˜í™˜
        return response.url
    except:
        # ì‹¤íŒ¨ ì‹œ êµ¬ê¸€ ë§í¬ë¼ë„ ìœ ì§€
        return google_url

def main():
    query = 'site:oecd.org (intitle:"Artificial Intelligence" OR intitle:AI) -intitle:PISA'
    encoded_query = urllib.parse.quote(query)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
    
    file_name = 'oecd_ai_intelligence.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")

    print(f"ğŸ“¡ OECD ìµœì‹  AI ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ë° ì›ë³¸ ë§í¬ ë³€í™˜ ì‹œì‘...")
    raw_data = []

    try:
        feed = feedparser.parse(rss_url)
        
        for entry in feed.entries:
            title_en = entry.title.split(' - ')[0]
            
            # ğŸ’¡ [í•„í„°ë§] AI ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œ ê²€ì‚¬
            keywords = ['AI', 'ARTIFICIAL', 'INTELLIGENCE', 'ALGORITHMS', 'GENERATIVE']
            if not any(kw in title_en.upper() for kw in keywords):
                continue

            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                pub_dt = datetime(*entry.published_parsed[:6])
                raw_data.append({
                    "ê¸°ê´€": "OECD",
                    "ë°œí–‰ì¼": pub_dt.strftime('%Y-%m-%d'),
                    "dt_obj": pub_dt,
                    "ì œëª©_en": title_en,
                    "google_link": entry.link # ì„ì‹œ ì €ì¥
                })

        # 1ï¸âƒ£ ìµœì‹ ìˆœ ì •ë ¬
        raw_data.sort(key=lambda x: x['dt_obj'], reverse=True)

        # 2ï¸âƒ£ ìµœìƒìœ„ 5ê°œë§Œ ì„ íƒ ë° ì›ë³¸ ë§í¬ ë³€í™˜
        final_data = []
        for item in raw_data[:5]:
            print(f"ğŸ”— ì›ë³¸ ë§í¬ ì¶”ì¶œ ì¤‘: {item['ì œëª©_en'][:30]}...")
            
            # ğŸ’¡ êµ¬ê¸€ ë§í¬ë¥¼ ì›ë³¸ ë§í¬ë¡œ ë³€í™˜
            actual_link = get_real_url(item['google_link'])
            
            try:
                title_ko = translator.translate(item['ì œëª©_en'].strip(), dest='ko').text
            except:
                title_ko = item['ì œëª©_en']
            
            final_data.append({
                "ê¸°ê´€": "OECD",
                "ë°œí–‰ì¼": item['ë°œí–‰ì¼'],
                "ì œëª©": title_ko,
                "ì›ë¬¸": item['ì œëª©_en'],
                "ë§í¬": actual_link,
                "ìˆ˜ì§‘ì¼": collected_date
            })

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ğŸ’¾ ê²°ê³¼ ì €ì¥
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
        writer.writeheader()
        if final_data:
            writer.writerows(final_data)
            print(f"âœ… ì„±ê³µ! ì›ë³¸ ë§í¬ê°€ í¬í•¨ëœ {len(final_data)}ê±´ ì €ì¥ ì™„ë£Œ.")
        else:
            print("âš ï¸ ì¡°ê±´ì— ë§ëŠ” ìµœì‹  AI ë¦¬í¬íŠ¸ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
