import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime
from googletrans import Translator

def main():
    # ğŸ¯ ëŒ€í‘œë‹˜ì´ ì£¼ì‹  ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ì£¼ì†Œ
    target_url = "https://www.oecd.org/en/search.html?facetTags=oecd-policy-issues:pi20&oecd-languages:en&orderBy=mostRelevant"
    
    file_name = 'oecd_ai_intelligence.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")

    # ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ í—¤ë” ì„¤ì • (ë§¤ìš° ì¤‘ìš”)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9,ko;q=0.8"
    }

    print(f"ğŸ“¡ [OECD ì •ë°€ ìŠ¤í¬ë˜í•‘] ì›¹í˜ì´ì§€ ë¶„ì„ ì‹œì‘...")
    new_data = []

    try:
        response = requests.get(target_url, headers=headers, timeout=30)
        # OECDê°€ ì°¨ë‹¨í–ˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì¸ì½”ë”© ê°•ì œ ì„¤ì •
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ğŸ’¡ OECD ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì˜ ì œëª©ê³¼ ë§í¬ íŒ¨í„´ ë¶„ì„
        # ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œë“¤ì€ ë³´í†µ 'a' íƒœê·¸ ë‚´ì— ì œëª©ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
        articles = soup.find_all('a', href=True)
        
        print(f"ğŸ” í˜ì´ì§€ ë‚´ ë§í¬ {len(articles)}ê°œ ë¶„ì„ ì¤‘...")

        for article in articles:
            title_en = article.get_text().strip()
            link = article['href']
            
            # ğŸ’¡ í•µì‹¬ í•„í„°: ì œëª©ì— AIë‚˜ ì •ì±… ê´€ë ¨ ë‹¨ì–´ê°€ ìˆê³ , ë§í¬ê°€ /en/ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê¸°ì‚¬ë§Œ ì¶”ì¶œ
            if len(title_en) > 20 and any(kw in title_en.upper() for kw in ['AI', 'ARTIFICIAL', 'DIGITAL', 'POLICY']):
                if link.startswith('/') or 'oecd.org' in link:
                    full_link = link if link.startswith('http') else "https://www.oecd.org" + link
                    
                    # ì¤‘ë³µ ì œê±°
                    if any(d['ì›ë¬¸'] == title_en for d in new_data): continue

                    # í•œêµ­ì–´ ë²ˆì—­
                    try:
                        title_ko = translator.translate(title_en, dest='ko').text
                    except:
                        title_ko = title_en

                    new_data.append({
                        "ê¸°ê´€": "OECD",
                        "ë°œí–‰ì¼": collected_date, # HTML ë°©ì‹ì€ ë‚ ì§œ íŒŒì‹±ì´ ê¹Œë‹¤ë¡œì›Œ ìˆ˜ì§‘ì¼ë¡œ ëŒ€ì²´
                        "ì œëª©": title_ko,
                        "ì›ë¬¸": title_en,
                        "ë§í¬": full_link,
                        "ìˆ˜ì§‘ì¼": collected_date
                    })
                    
            if len(new_data) >= 15: break # ìƒìœ„ 15ê±´ë§Œ

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ğŸ’¾ ê²°ê³¼ ì €ì¥
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
        writer.writeheader()
        if new_data:
            writer.writerows(new_data)
            print(f"âœ… ë“œë””ì–´ ì„±ê³µ! {len(new_data)}ê±´ì˜ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
        else:
            # ğŸ’¡ ë§Œì•½ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ 'ìƒ˜í”Œ ë°ì´í„°'ë¼ë„ ë„£ì–´ì„œ íŒŒì¼ ìƒì„±ì„ ë³´ì¥í•¨
            writer.writerow({
                "ê¸°ê´€": "OECD", "ë°œí–‰ì¼": collected_date, 
                "ì œëª©": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤ (ìƒˆë¡œìš´ ì—…ë°ì´íŠ¸ í™•ì¸ í•„ìš”)", 
                "ì›ë¬¸": "Checking for new updates", "ë§í¬": target_url, "ìˆ˜ì§‘ì¼": collected_date
            })
            print("âš ï¸ ì¡°ê±´ì— ë§ëŠ” ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ì—†ì–´ ì ê²€ ì•Œë¦¼ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
