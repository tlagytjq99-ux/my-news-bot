import feedparser
import csv
import urllib.parse
import base64
import re
from datetime import datetime
from googletrans import Translator

def decode_google_news_url(url):
    """êµ¬ê¸€ ë‰´ìŠ¤ ì£¼ì†Œì˜ ì•”í˜¸í™”ëœ ë¶€ë¶„ì„ í•´ë…í•˜ì—¬ ì›ë³¸ URL ì¶”ì¶œ"""
    try:
        # 1. URLì—ì„œ ì•”í˜¸í™”ëœ í•µì‹¬ ë¬¸ìì—´ë§Œ ì¶”ì¶œ
        base64_str = url.split("articles/")[1].split("?")[0]
        
        # 2. Base64 ë””ì½”ë”© (íŒ¨ë”© ë³´ì • ì‘ì—… í¬í•¨)
        padding = len(base64_str) % 4
        if padding != 0:
            base64_str += "=" * (4 - padding)
        
        decoded_bytes = base64.urlsafe_b64decode(base64_str)
        # ë‹¤ì–‘í•œ ì¸ì½”ë”© ëŒ€ì‘
        decoded_text = decoded_bytes.decode('latin-1', errors='ignore')
        
        # 3. ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸ ì•ˆì—ì„œ httpë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ìì—´ì„ ì •ê·œì‹ìœ¼ë¡œ ì°¾ê¸°
        match = re.search(r'https?://[^\s\x00-\x1f\x7f-\xff]+', decoded_text)
        if match:
            clean_url = match.group(0)
            # ëë¶€ë¶„ì— ë‚¨ì„ ìˆ˜ ìˆëŠ” ì“°ë ˆê¸° ë¬¸ì ì œê±°
            clean_url = clean_url.split('?')[0].split('')[0].split('\x01')[0]
            return clean_url
    except Exception:
        pass
    return url # í•´ë… ì‹¤íŒ¨ ì‹œ ì›ë˜ ë§í¬ ë°˜í™˜

def main():
    query = 'site:oecd.org (intitle:"Artificial Intelligence" OR intitle:AI) -intitle:PISA'
    encoded_query = urllib.parse.quote(query)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
    
    file_name = 'oecd_ai_intelligence.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")

    print(f"ğŸ“¡ OECD ë°ì´í„° ìˆ˜ì§‘ ë° ì•”í˜¸ ë§í¬ í•´ë… ì‹œì‘...")
    raw_data = []

    try:
        feed = feedparser.parse(rss_url)
        entries = sorted(feed.entries, key=lambda x: x.get('published_parsed'), reverse=True)
        
        count = 0
        for entry in entries:
            if count >= 5: break
            
            title_en = entry.title.split(' - ')[0]
            
            # AI í‚¤ì›Œë“œ ê²€ì¦
            keywords = ['AI', 'ARTIFICIAL', 'INTELLIGENCE', 'ALGORITHMS', 'GENERATIVE']
            if not any(kw in title_en.upper() for kw in keywords):
                continue

            print(f"ğŸ”‘ {count+1}ë²ˆì§¸ ì•”í˜¸ í•´ë… ì¤‘: {title_en[:30]}...")
            
            # ğŸ’¡ êµ¬ê¸€ ì„œë²„ì— ë¬»ì§€ ì•Šê³  ë‚´ë¶€ ìˆ˜ì‹ìœ¼ë¡œ ë§í¬ í•´ë…
            actual_link = decode_google_news_url(entry.link)
            
            pub_date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d') if hasattr(entry, 'published_parsed') else collected_date

            try:
                title_ko = translator.translate(title_en.strip(), dest='ko').text
            except:
                title_ko = title_en

            raw_data.append({
                "ê¸°ê´€": "OECD", "ë°œí–‰ì¼": pub_date, "ì œëª©": title_ko,
                "ì›ë¬¸": title_en, "ë§í¬": actual_link, "ìˆ˜ì§‘ì¼": collected_date
            })
            count += 1

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")

    # ğŸ’¾ ê²°ê³¼ ì €ì¥
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
        writer.writeheader()
        if raw_data:
            writer.writerows(raw_data)
            print(f"âœ… ì™„ë£Œ! {len(raw_data)}ê±´ì˜ ë¦¬í¬íŠ¸ë¥¼ í•´ë…í•˜ì—¬ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
