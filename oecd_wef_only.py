import feedparser
import csv
import time
from datetime import datetime
from googletrans import Translator

def main():
    # ğŸ¯ ê¸€ë¡œë²Œ ì •ì±… ë° ì˜ì œ ì„¤ì • ê¸°êµ¬ íƒ€ê²Ÿ
    # OECDëŠ” ëŒ€í‘œë‹˜ì´ ì£¼ì‹  'AI ì •ì±…(pi20)' í…Œë§ˆì˜ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì£¼ì†Œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    sources = [
        {
            "name": "OECD", 
            "url": "https://www.oecd.org/en/topics/subtopics/artificial-intelligence/jcr:content/feed"
        },
        {
            "name": "WEF", 
            "url": "https://www.weforum.org/agenda/feed"
        }
    ]
    
    file_name = 'oecd_wef_intelligence.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")
    
    # ğŸ’¡ ì •ì±… ë° ê¸€ë¡œë²Œ ê²½ì œ ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œ
    policy_keywords = ['AI', 'DIGITAL', 'ECONOMY', 'POLICY', 'GOVERNANCE', 'FRAMEWORK', 'OUTLOOK', 'REPORT', 'STRATEGY']

    print(f"ğŸ“¡ [OECD & WEF ì •ì±… ì—”ì§„] ìˆ˜ì§‘ ì‹œì‘...")
    new_data = []

    for source in sources:
        print(f"ğŸ” {source['name']} ì •ì±… ë¦¬í¬íŠ¸ ë¶„ì„ ì¤‘...")
        try:
            feed = feedparser.parse(source['url'])
            if not feed.entries:
                print(f"   âš ï¸ {source['name']} í”¼ë“œì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue

            count = 0
            for entry in feed.entries:
                title_en = entry.title
                
                # ì œëª©ì— í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ìˆ˜ì§‘ (ìˆœë„ ìœ ì§€)
                if any(kw in title_en.upper() for kw in policy_keywords):
                    
                    # ë°œí–‰ì¼ ì²˜ë¦¬
                    raw_date = entry.get('published_parsed', None)
                    published_date = time.strftime('%Y-%m-%d', raw_date) if raw_date else collected_date

                    # í•œêµ­ì–´ ë²ˆì—­
                    try:
                        # OECD/WEFëŠ” ë¬¸ì¥ì´ ê¸¸ì–´ ë²ˆì—­ ì—”ì§„ í˜¸ì¶œ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.
                        title_ko = translator.translate(title_en, dest='ko').text
                    except:
                        title_ko = title_en

                    new_data.append({
                        "ê¸°ê´€": source['name'],
                        "ë°œí–‰ì¼": published_date,
                        "ì œëª©": title_ko,
                        "ì›ë¬¸": title_en,
                        "ë§í¬": entry.link,
                        "ìˆ˜ì§‘ì¼": collected_date
                    })
                    count += 1
                    if count >= 20: break # ê¸°ê´€ë‹¹ ìµœì‹  20ê±´ ìˆ˜ì§‘
            
            print(f"   âœ… {source['name']}ì—ì„œ {count}ê±´ì˜ í•µì‹¬ ì˜ì œ í™•ë³´!")

        except Exception as e:
            print(f"   âŒ {source['name']} ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

    # ğŸ’¾ ê²°ê³¼ ì €ì¥ (ë°œí–‰ì¼ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)
    if new_data:
        new_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "ë§í¬", "ìˆ˜ì§‘ì¼"])
            writer.writeheader()
            writer.writerows(new_data)
        print(f"\nğŸ‰ ì‘ì—… ì™„ë£Œ! '{file_name}'ì— ê¸€ë¡œë²Œ ì •ì±… ì¸ì‚¬ì´íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nğŸ’¡ ìƒˆë¡œ ì—…ë°ì´íŠ¸ëœ ì •ì±… ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
