import feedparser
import csv
import urllib.parse
from datetime import datetime
from googletrans import Translator
from googlenewsdecoder import gnewsdecoder

def main():
    # ğŸ¯ ë¯¼ê°„ ì»¨ì„¤íŒ…ì‚¬ íƒ€ê²ŸíŒ…
    # ê° ì‚¬ì˜ ë„ë©”ì¸ì—ì„œ AI ë¦¬í¬íŠ¸ ìœ„ì£¼ë¡œ ìˆ˜ì§‘í•˜ë©°, ì±„ìš©(career) ì •ë³´ëŠ” ì œì™¸í•©ë‹ˆë‹¤.
    target_firms = {
        "McKinsey": 'site:mckinsey.com (intitle:"Artificial Intelligence" OR intitle:AI) -intitle:career',
        "BCG": 'site:bcg.com (intitle:"Artificial Intelligence" OR intitle:AI) -intitle:career',
        "Bain": 'site:bain.com (intitle:"Artificial Intelligence" OR intitle:AI)',
        "GoldmanSachs": 'site:goldmansachs.com (intitle:"Artificial Intelligence" OR intitle:AI)'
    }

    file_name = 'private_consulting_ai_monitor.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")
    all_data = []

    print(f"ğŸ’¼ ë¯¼ê°„ ì»¨ì„¤íŒ…ì‚¬ AI ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ë° í•´ë… ì‹œì‘...")

    for firm, query in target_firms.items():
        print(f"ğŸ“¡ {firm} ë¶„ì„ ì¤‘...")
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
        
        try:
            feed = feedparser.parse(rss_url)
            # ì‚¬ë³„ë¡œ ìµœì‹  3~4ê±´ì”© ê²€í† 
            entries = sorted(feed.entries, key=lambda x: x.get('published_parsed'), reverse=True)[:5]
            
            for entry in entries:
                title_en = entry.title.split(' - ')[0]
                if len(title_en.split()) <= 2: continue

                # ğŸ’¡ êµ¬ê¸€ ë‰´ìŠ¤ ì•”í˜¸ í•´ë…
                try:
                    decoded = gnewsdecoder(entry.link)
                    link = decoded.get('decoded_url', entry.link)
                except:
                    link = entry.link

                # ğŸ’¡ PDF ì—¬ë¶€ íŒë³„
                is_pdf = "YES" if link.lower().endswith('.pdf') or ".pdf?" in link.lower() else "NO"

                # ë‚ ì§œ ë° ë²ˆì—­
                pub_date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d') if hasattr(entry, 'published_parsed') else collected_date
                try:
                    title_ko = translator.translate(title_en, dest='ko').text
                except:
                    title_ko = title_en

                all_data.append({
                    "ê¸°ê´€": firm,
                    "ë°œí–‰ì¼": pub_date,
                    "ì œëª©": f"{'[PDF] ' if is_pdf == 'YES' else ''}{title_ko}",
                    "ì›ë¬¸": title_en,
                    "PDFì—¬ë¶€": is_pdf,
                    "ë§í¬": link,
                    "ìˆ˜ì§‘ì¼": collected_date
                })
        except Exception as e:
            print(f"âš ï¸ {firm} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    # ìµœì‹ ìˆœ ì •ë ¬
    all_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)

    # ğŸ’¾ ê²°ê³¼ ì €ì¥
    fieldnames = ["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "PDFì—¬ë¶€", "ë§í¬", "ìˆ˜ì§‘ì¼"]
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(all_data)
        print(f"âœ… ì™„ë£Œ! ì´ {len(all_data)}ê±´ì˜ ë¯¼ê°„ ì¸ì‚¬ì´íŠ¸ ì €ì¥.")

if __name__ == "__main__":
    main()
