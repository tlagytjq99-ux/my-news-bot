import feedparser
import csv
import urllib.parse
import requests
from bs4 import BeautifulSoup
from googlenewsdecoder import gnewsdecoder
import time

def get_whitehouse_content(url):
    """ë°±ì•…ê´€ ì›ë¬¸ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=10)
        if res.status_code == 200:
            soup = BeautifulSoup(res.text, 'html.parser')
            content = soup.find('section', class_='body-content')
            return content.get_text(strip=True).lower() if content else ""
    except:
        return ""
    return ""

def main():
    # 1. ì‚¬ì§„ ì† ICT ìœ í˜• ë° í‚¤ì›Œë“œ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
    ICT_DATABASE = {
        "Intelligent Services": ["Artificial Intelligence", "Machine Learning", "AI Education", "AI Governance", "AI Stack"],
        "Data": ["Information Silo", "Data Privacy", "Fraud Detection", "Data Sharing", "Digital Assets"],
        "Network": ["Connectivity", "Cybersecurity", "Spectrum", "Infrastructure", "Comm. Security"],
        "Security": ["National Security", "Threat Mitigation", "Critical Infra", "Cyber Defense", "Risk Assessment"],
        "Cloud": ["Efficiency", "Digital Sovereignty", "Cloud Hosting", "Government IT", "Modernization"],
        "SW/System": ["SW Innovation", "Defense Acquisition", "Interoperability", "Digital Transformation", "Open Source"],
        "Computing": ["High-Performance", "Semiconductor", "Quantum Tech", "Processing Power", "Hardware Security"]
    }

    # êµ¬ê¸€ ê²€ìƒ‰ìš© í†µí•© í‚¤ì›Œë“œ ìƒì„±
    all_keywords = []
    for kws in ICT_DATABASE.values():
        all_keywords.extend(kws)
    search_query_str = " OR ".join([f'"{k}"' for k in all_keywords[:10]]) # ê²€ìƒ‰ íš¨ìœ¨ì„ ìœ„í•´ ì£¼ìš” í‚¤ì›Œë“œ ì¡°í•©

    target_site = "whitehouse.gov/presidential-actions/"
    query = f"site:{target_site} {search_query_str} after:2025-01-01 before:2026-01-01"
    rss_url = f"https://news.google.com/rss/search?q={urllib.parse.quote(query)}&hl=en-US&gl=US&ceid=US:en"

    print(f"ğŸ“¡ ì‚¬ì§„ ê¸°ë°˜ 2025 ICT ì •ì±… ë”¥ ìŠ¤ìº” ì‹œì‘...")

    feed = feedparser.parse(rss_url)
    results = []

    for entry in feed.entries:
        try:
            decoded = gnewsdecoder(entry.link)
            actual_url = decoded.get('decoded_url', entry.link)
            
            # ë³¸ë¬¸ ë°ì´í„° í™•ë³´
            full_text = get_whitehouse_content(actual_url)
            title = entry.title.split(' - ')[0].strip().lower()

            # ì‚¬ì§„ ì† í‚¤ì›Œë“œ ë§¤ì¹­ ê²€ì‚¬
            matched_types = []
            matched_keywords = []

            for ict_type, keywords in ICT_DATABASE.items():
                for kw in keywords:
                    if kw.lower() in title or kw.lower() in full_text:
                        if ict_type not in matched_types:
                            matched_types.append(ict_type)
                        matched_keywords.append(kw)

            if matched_types:
                results.append({
                    "ë°œí–‰ì¼": entry.published if 'published' in entry else "2025",
                    "ICT ìœ í˜•": ", ".join(matched_types),
                    "ë§¤ì¹­ í‚¤ì›Œë“œ": ", ".join(list(set(matched_keywords))),
                    "ì œëª©": entry.title.split(' - ')[0].strip(),
                    "ì›ë¬¸ë§í¬": actual_url
                })
                print(f"âœ… ë§¤ì¹­ ë°œê²¬: [{matched_types[0]}] {entry.title[:30]}")
                time.sleep(1)
        except:
            continue

    # CSV ì €ì¥
    file_name = 'whitehouse_ict_2025_report.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ë°œí–‰ì¼", "ICT ìœ í˜•", "ë§¤ì¹­ í‚¤ì›Œë“œ", "ì œëª©", "ì›ë¬¸ë§í¬"])
        writer.writeheader()
        writer.writerows(results)

    print(f"ğŸ ë¶„ì„ ì™„ë£Œ! íŒŒì¼ëª…: {file_name}")

if __name__ == "__main__":
    main()
