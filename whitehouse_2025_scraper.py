import feedparser

import csv

import urllib.parse

from datetime import datetime

from googlenewsdecoder import gnewsdecoder

import time



def main():

    # 1. ëŒ€í†µë ¹ ì‹¤í–‰ ì¡°ì¹˜(Presidential Actions) í˜ì´ì§€ë¥¼ ì§‘ì¤‘ íƒ€ê²ŸíŒ…í•˜ëŠ” ì¿¼ë¦¬

    # site ì—°ì‚°ìì— ê²½ë¡œë¥¼ í¬í•¨ì‹œì¼œ í•´ë‹¹ ì„¹ì…˜ì˜ ì¸ë±ì‹±ì„ ìš°ì„ ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.

    query = 'site:whitehouse.gov/presidential-actions/executive-orders after:2025-01-01'

    encoded_query = urllib.parse.quote(query)

    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"



    # 2. ì‚¬ì§„ ì† 46ê°œ ì¹´í…Œê³ ë¦¬ ëŒ€ì‘ í‚¤ì›Œë“œ (ë²”ìš© ë‹¨ì–´ í¬í•¨)

    category_map = {

        "AI/Digital": ["AI", "Artificial Intelligence", "Algorithm", "Digital", "Automation"],

        "Semiconductor/Tech": ["Semiconductor", "Chip", "Critical Technology", "Supply Chain"],

        "Energy/Infrastructure": ["Nuclear", "Energy", "Infrastructure", "SMR", "Power"],

        "Cyber/Security": ["Cyber", "Security", "Defense", "Intelligence"],

        "Economy/Trade": ["Tariff", "Trade", "Investment", "Tax", "Finance"]

    }



    print(f"ğŸ“¡ ë°±ì•…ê´€ 'ëŒ€í†µë ¹ ì‹¤í–‰ ì¡°ì¹˜' ì„¹ì…˜ 2025ë…„ ë°ì´í„° ì •ë°€ ìŠ¤ìº” ì‹œì‘...")



    try:

        feed = feedparser.parse(rss_url)

        # ë§Œì•½ í•´ë‹¹ ê²½ë¡œ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì „ì²´ ê²½ë¡œë¡œ í™•ì¥í•´ì„œ ë‹¤ì‹œ ì‹œë„

        if len(feed.entries) < 3:

            print("ğŸ’¡ íŠ¹ì • ì„¹ì…˜ ë°ì´í„°ê°€ ì ì–´ ë°±ì•…ê´€ ì „ì²´ ì†Œì‹ìœ¼ë¡œ í™•ì¥ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

            query = 'site:whitehouse.gov after:2025-01-01'

            encoded_query = urllib.parse.quote(query)

            rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"

            feed = feedparser.parse(rss_url)



        results = []

        for entry in feed.entries:

            try:

                pub_date = datetime(*entry.published_parsed[:3])

                if pub_date.year == 2025:

                    title = entry.title.split(' - ')[0].strip()

                    link = entry.link

                    

                    # URL ë””ì½”ë”©

                    try:

                        decoded = gnewsdecoder(link)

                        actual_url = decoded.get('decoded_url', link)

                    except:

                        actual_url = link



                    # ìœ í˜• ë¶„ë¥˜

                    matched_types = []

                    for cat, kws in category_map.items():

                        if any(kw.lower() in title.lower() for kw in kws):

                            matched_types.append(cat)

                    

                    # 'ëŒ€í†µë ¹ ì‹¤í–‰ ì¡°ì¹˜' í˜ì´ì§€ ì¶œì²˜ì¸ì§€ í™•ì¸ (ìš°ì„ ìˆœìœ„ í‘œì‹œ)

                    doc_type = "Executive Action" if "/presidential-actions/" in actual_url else "General News"



                    results.append({

                        "ë°œí–‰ì¼": pub_date.strftime('%Y-%m-%d'),

                        "ë¬¸ì„œìœ í˜•": doc_type,

                        "ê¸°ìˆ ë¶„ë¥˜": ", ".join(matched_types) if matched_types else "Other Policy",

                        "ì œëª©": title,

                        "ì›ë¬¸ë§í¬": actual_url

                    })

                time.sleep(0.05)

            except: continue



        # 3. CSV ì €ì¥

        file_name = 'whitehouse_2025_tech_report.csv'

        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:

            writer = csv.DictWriter(f, fieldnames=["ë°œí–‰ì¼", "ë¬¸ì„œìœ í˜•", "ê¸°ìˆ ë¶„ë¥˜", "ì œëª©", "ì›ë¬¸ë§í¬"])

            writer.writeheader()

            if results:

                results.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)

                writer.writerows(results)

                print(f"âœ… ì™„ë£Œ: ì´ {len(results)}ê±´ì˜ 2025ë…„ ë°ì´í„°ë¥¼ ë¶„ë¥˜ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

            else:

                print("âš ï¸ 2025ë…„ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")



    except Exception as e:

        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")



if __name__ == "__main__":

    main()
