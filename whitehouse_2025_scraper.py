import feedparser
import csv
import urllib.parse
import requests
from bs4 import BeautifulSoup
from googlenewsdecoder import gnewsdecoder
import time

def get_whitehouse_content(url):
    """ë°±ì•…ê´€ ì›ë¬¸ ë§í¬ì— ì ‘ì†í•´ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        # ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì§ì ‘ ì ‘ì†
        res = requests.get(url, headers=headers, timeout=10)
        if res.status_code == 200:
            soup = BeautifulSoup(res.text, 'html.parser')
            # ë°±ì•…ê´€ ê³µì‹ ë¬¸ì„œì˜ ë³¸ë¬¸ ì„¹ì…˜ ì¶”ì¶œ
            content = soup.find('section', class_='body-content')
            return content.get_text(strip=True).lower() if content else ""
    except:
        return ""
    return ""

def main():
    target_site = "whitehouse.gov/presidential-actions/"
    # 5G/6G ê´€ë ¨ ì£¼íŒŒìˆ˜(Spectrum)ì™€ NTIA(ê´€ë¦¬ì²­) ë“± í•µì‹¬ í‚¤ì›Œë“œ
    keywords = "(5G OR 6G OR Spectrum OR Wireless OR NTIA OR Connectivity)"
    query = f"site:{target_site} {keywords} after:2025-01-01 before:2026-01-01"
    
    rss_url = f"https://news.google.com/rss/search?q={urllib.parse.quote(query)}&hl=en-US&gl=US&ceid=US:en"
    feed = feedparser.parse(rss_url)
    results = []

    print(f"ğŸ“¡ 2025ë…„ ì •ì±… ë³¸ë¬¸ ì •ë°€ ìŠ¤ìº” ì‹œì‘ (ì œëª©+ë³¸ë¬¸ ë‚´ìš© ê²€ì‚¬)...")

    for entry in feed.entries:
        try:
            # 1. ë§í¬ í•´ë…
            decoded = gnewsdecoder(entry.link)
            actual_url = decoded.get('decoded_url', entry.link)
            
            # 2. ë³¸ë¬¸ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            full_text = get_whitehouse_content(actual_url)
            title = entry.title.split(' - ')[0].strip()

            # 3. ì œëª©ì´ë‚˜ ë³¸ë¬¸ì— ìš°ë¦¬ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
            check_words = ["5g", "6g", "spectrum", "wireless", "ntia", "connectivity", "telecom"]
            if any(word in title.lower() for word in check_words) or any(word in full_text for word in check_words):
                
                # ë³¸ë¬¸ì—ì„œ ì•ë¶€ë¶„ 300ìë§Œ ìš”ì•½ìœ¼ë¡œ ì¶”ì¶œ
                summary = full_text[:300].replace(',', ' ') + "..." if full_text else "ë³¸ë¬¸ ë‚´ìš© í™•ì¸ í•„ìš”"
                
                results.append({
                    "ë°œí–‰ì¼": entry.published if 'published' in entry else "2025-Ongoing",
                    "ì œëª©": title,
                    "ë³¸ë¬¸ìš”ì•½(í•µì‹¬ë‚´ìš©)": summary,
                    "ì›ë¬¸ë§í¬": actual_url
                })
                print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {title[:30]}")
                time.sleep(1) # ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ê°„ê²©
        except Exception as e:
            continue

    # 4. CSV ì €ì¥ (ìš”ì•½ ì»¬ëŸ¼ ì¶”ê°€)
    file_name = 'whitehouse_5G6G_DeepScan_2025.csv'
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ë°œí–‰ì¼", "ì œëª©", "ë³¸ë¬¸ìš”ì•½(í•µì‹¬ë‚´ìš©)", "ì›ë¬¸ë§í¬"])
        writer.writeheader()
        writer.writerows(results)

    print(f"ğŸ ì™„ë£Œ: ì´ {len(results)}ê±´ì˜ ì •ì±… ë³¸ë¬¸ì„ ë¶„ì„í•˜ì—¬ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
