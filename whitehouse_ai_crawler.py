import asyncio
import csv
import re
import os
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    # ğŸ¯ ê²€ìƒ‰ì–´: Artificial Intelligence
    search_url = "https://www.whitehouse.gov/?s=Artificial+Intelligence"
    file_name = 'whitehouse_ai_report.csv'
    
    print(f"ğŸ“¡ ë°±ì•…ê´€ ë‰´ìŠ¤ë£¸ ì •ë°€ ìŠ¤ìº” ì‹œì‘ (ë²”ìš© ëª¨ë“œ)...")

    existing_titles = set()
    if os.path.exists(file_name):
        with open(file_name, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader: existing_titles.add(row['ì œëª©'])

    # ğŸ’¡ ë¸Œë¼ìš°ì € ì„¤ì •ì„ ë” 'ì‚¬ëŒ'ì²˜ëŸ¼ ë³´ì´ê²Œ ê°•í™”í•©ë‹ˆë‹¤.
    browser_config = BrowserConfig(
        browser_type="chromium", 
        headless=True,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
    )
    
    # ğŸ’¡ íŠ¹ì • ìš”ì†Œë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³ , í˜ì´ì§€ ë¡œë”© í›„ 10ì´ˆë§Œ ë”± ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
    run_config = CrawlerRunConfig(
        delay_before_return_html=10.0, 
        cache_mode="bypass"
    )

    new_data = []
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=search_url, config=run_config)
        
        if result.success and result.markdown:
            print("âœ… í˜ì´ì§€ ë¡œë“œ ì„±ê³µ! ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
            
            # ğŸ’¡ [í•µì‹¬] ë°±ì•…ê´€ ë‰´ìŠ¤ë£¸ì˜ ë§í¬ íŒ¨í„´ì„ ë” ë„“ê²Œ ì¡ìŠµë‹ˆë‹¤.
            # 1ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ì—ì„œ ëª¨ë“  ë§í¬ì™€ ì œëª© ì¶”ì¶œ
            all_links = re.findall(r'\[([^\]]{10,})\]\((https://www\.whitehouse\.gov/[^\)]+)\)', result.markdown)
            
            # AI í‚¤ì›Œë“œ (í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë²”ìœ„ë¥¼ ë„“í™ë‹ˆë‹¤)
            ai_keywords = ['AI', 'ARTIFICIAL INTELLIGENCE', 'TECH', 'CYBER', 'DIGITAL', 'DATA']
            
            count = 0
            for title, link in all_links:
                if count >= 5: break
                
                title_clean = title.strip().replace('\n', ' ')
                
                # ğŸ¯ í•„í„°: ì œëª©ì— AI í‚¤ì›Œë“œê°€ ìˆê³ , ì£¼ì†Œì— briefing-roomì´ í¬í•¨ëœ ì§„ì§œ ë‰´ìŠ¤ë§Œ!
                if any(kw in title_clean.upper() for kw in ai_keywords):
                    if 'briefing-room' in link and title_clean not in existing_titles:
                        print(f"   ğŸ†• ë°œê²¬: {title_clean[:40]}...")
                        
                        new_data.append({
                            "ê¸°ê´€": "ë°±ì•…ê´€(White House)",
                            "ë°œí–‰ì¼": datetime.now().strftime("%Y-%m-%d"), # ìƒì„¸í˜ì´ì§€ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ì¼ë‹¨ ì˜¤ëŠ˜ë‚ ì§œ
                            "ì œëª©": title_clean,
                            "ë§í¬": link,
                            "ìˆ˜ì§‘ì¼": datetime.now().strftime("%Y-%m-%d")
                        })
                        count += 1

    # ğŸ’¾ ê²°ê³¼ ì €ì¥ (ëˆ„ì  ëª¨ë“œ)
    if new_data:
        file_exists = os.path.exists(file_name)
        with open(file_name, 'a', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬", "ìˆ˜ì§‘ì¼"])
            if not file_exists: writer.writeheader()
            writer.writerows(new_data)
        print(f"âœ… ì„±ê³µ! {len(new_data)}ê±´ì˜ ë‰´ìŠ¤ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("ğŸ’¡ ìƒˆë¡œìš´ ì†Œì‹ì´ ì—†ê±°ë‚˜ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
