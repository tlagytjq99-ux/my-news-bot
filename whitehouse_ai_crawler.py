import asyncio
import csv
import re
import os
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

# --- [1. ìƒì„¸ í˜ì´ì§€ ë‚ ì§œ ì¶”ì¶œ í•¨ìˆ˜] ---
async def get_whitehouse_date(crawler, url, config):
    """ë°±ì•…ê´€ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì˜ë¬¸ ë‚ ì§œë¥¼ ì°¾ì•„ YYYY-MM-DDë¡œ ë³€í™˜"""
    try:
        result = await crawler.arun(url=url, config=config)
        if not (result.success and result.markdown):
            return "ë‚ ì§œí™•ì¸í•„ìš”"
        
        # ì˜ˆ: "January 29, 2026" íŒ¨í„´ ì°¾ê¸°
        content = result.markdown[:2500]
        date_match = re.search(r'([A-Z][a-z]+ \d{1,2}, \d{4})', content)
        
        if date_match:
            dt = datetime.strptime(date_match.group(1), "%B %d, %Y")
            return dt.strftime("%Y-%m-%d")
    except:
        pass
    return datetime.now().strftime("%Y-%m-%d")

# --- [2. ë©”ì¸ ìˆ˜ì§‘ ë¡œì§] ---
async def main():
    # ğŸ¯ íƒ€ì¼“: ë°±ì•…ê´€ ë¸Œë¦¬í•‘ë£¸ ë‚´ 'AI' ê²€ìƒ‰ ê²°ê³¼
    target_url = "https://www.whitehouse.gov/?s=AI&post_type=briefing-room"
    
    print(f"ğŸš€ [ì‹œì‘] ë°±ì•…ê´€ AI ì •ì±… ìˆ˜ì§‘ (Target: {target_url})")

    # ë¸Œë¼ìš°ì € ë° ì‹¤í–‰ ì„¤ì • (ì •ë¶€ê¸°ê´€ ëŒ€ì‘ìš© ì •ë°€ ì„¸íŒ…)
    browser_config = BrowserConfig(
        browser_type="chromium",
        headless=True,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
    )
    run_config = CrawlerRunConfig(
        wait_for="body", 
        delay_before_return_html=10.0, # ë„‰ë„‰í•œ ëŒ€ê¸° ì‹œê°„
        cache_mode="bypass"
    )

    # í•„í„°ë§ í‚¤ì›Œë“œ
    ai_keywords = ['AI', 'ARTIFICIAL INTELLIGENCE', 'LLM', 'GPT', 'ALGORITHM', 'TECHNOLOGY']
    final_data = []

    async with AsyncWebCrawler(config=browser_config) as crawler:
        print("ğŸ“¡ ë°±ì•…ê´€ ì„œë²„ì— ì ‘ì† ì¤‘...")
        result = await crawler.arun(url=target_url, config=run_config)
        
        if result.success and result.markdown:
            # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ê¸°ë°˜)
            links = re.findall(r'\[([^\]]{15,})\]\(([^\)]+)\)', result.markdown)
            print(f"ğŸ” í›„ë³´ {len(links)}ê±´ ë°œê²¬. í•„í„°ë§ ì‹œì‘...")
            
            count = 0
            for title, link in links:
                title_clean = title.strip()
                
                # 1. AI í‚¤ì›Œë“œ í•„í„°ë§
                if not any(kw in title_clean.upper() for kw in ai_keywords):
                    continue

                full_link = urljoin(target_url, link)
                
                # 2. ì¤‘ë³µ ë°©ì§€
                if any(d['ì œëª©'] == title_clean for d in final_data):
                    continue

                print(f"   ğŸ“‚ ë¶„ì„ ì¤‘: {title_clean[:30]}...")
                exact_date = await get_whitehouse_date(crawler, full_link, run_config)

                final_data.append({
                    "ê¸°ê´€": "ë°±ì•…ê´€(White House)",
                    "ë°œí–‰ì¼": exact_date,
                    "ì œëª©": title_clean,
                    "ë§í¬": full_link,
                    "ìˆ˜ì§‘ì¼": datetime.now().strftime("%Y-%m-%d")
                })
                
                count += 1
                if count >= 10: break # í•œ ë²ˆì— ìµœëŒ€ 10ê°œë§Œ
                await asyncio.sleep(2) # ì„œë²„ ë¶€í•˜ ë°©ì§€ íœ´ì‹

    # --- [3. ê²°ê³¼ ì €ì¥] ---
    if final_data:
        file_name = 'whitehouse_ai_report.csv'
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬", "ìˆ˜ì§‘ì¼"])
            writer.writeheader()
            writer.writerows(final_data)
        print(f"âœ… ì„±ê³µ! {file_name} íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ìˆ˜ì§‘ëœ ìƒˆë¡œìš´ AI ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
