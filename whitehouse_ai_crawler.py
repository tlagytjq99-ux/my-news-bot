import asyncio
import csv
import re
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def get_whitehouse_details(crawler, url, config):
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •í™•í•œ ë°œí–‰ì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        result = await crawler.arun(url=url, config=config)
        if not (result.success and result.markdown): return "ë‚ ì§œí™•ì¸í•„ìš”"
        
        # ë°±ì•…ê´€ ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ (ì˜ˆ: January 29, 2026)
        content = result.markdown[:2500]
        date_match = re.search(r'([A-Z][a-z]+ \d{1,2}, \d{4})', content)
        if date_match:
            dt = datetime.strptime(date_match.group(1), "%B %d, %Y")
            return dt.strftime("%Y-%m-%d")
    except: pass
    return datetime.now().strftime("%Y-%m-%d")

async def main():
    # ğŸ¯ [í•µì‹¬] ë°±ì•…ê´€ ë‰´ìŠ¤ë£¸ ë‚´ AI ê²€ìƒ‰ ê²°ê³¼ ì£¼ì†Œ
    search_url = "https://www.whitehouse.gov/?s=Artificial+Intelligence&post_type=briefing-room"
    
    print(f"ğŸ“¡ ë°±ì•…ê´€ ë‰´ìŠ¤ë£¸ì—ì„œ AI ê´€ë ¨ ìµœì‹  ì†Œì‹ì„ ì°¾ëŠ” ì¤‘...")

    browser_config = BrowserConfig(browser_type="chromium", headless=True)
    # ì •ë¶€ ì‚¬ì´íŠ¸ ë³´ì•ˆ ë° ë¡œë”© ì†ë„ë¥¼ ê³ ë ¤í•´ 10ì´ˆ ëŒ€ê¸° ì„¤ì •
    run_config = CrawlerRunConfig(wait_for="body", delay_before_return_html=10.0)

    final_data = []
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=search_url, config=run_config)
        
        if result.success and result.markdown:
            # 1. ë§ˆí¬ë‹¤ìš´ì—ì„œ ê¸°ì‚¬ ë§í¬ì™€ ì œëª© ì¶”ì¶œ
            # ë°±ì•…ê´€ ê²€ìƒ‰ ê²°ê³¼ì˜ ì „í˜•ì ì¸ ë§í¬ íŒ¨í„´ì„ íƒ€ê²ŸíŒ…í•©ë‹ˆë‹¤.
            links = re.findall(r'\[([^\]]{20,})\]\(([^\)]+)\)', result.markdown)
            
            count = 0
            for title, link in links:
                if count >= 5: break  # ğŸ¯ ë”± ìµœì‹  5ê°œë§Œ ìˆ˜ì§‘
                
                title_clean = title.strip()
                # ë¶ˆí•„ìš”í•œ ë©”ë‰´ ë§í¬ë‚˜ ì´ë¯¸ì§€ ë§í¬ ì œì™¸
                if any(x in link.lower() for x in ['facebook', 'twitter', '.jpg', '.png']): continue
                
                full_link = urljoin(search_url, link)
                
                # ì¤‘ë³µ ì²´í¬
                if any(d['ì œëª©'] == title_clean for d in final_data): continue

                print(f"   ğŸ” ({count+1}/5) ìƒì„¸ ë¶„ì„ ì¤‘: {title_clean[:30]}...")
                exact_date = await get_whitehouse_details(crawler, full_link, run_config)

                final_data.append({
                    "ê¸°ê´€": "ë°±ì•…ê´€(White House)",
                    "ë°œí–‰ì¼": exact_date,
                    "ì œëª©": title_clean,
                    "ë§í¬": full_link,
                    "ìˆ˜ì§‘ì¼": datetime.now().strftime("%Y-%m-%d")
                })
                count += 1
                await asyncio.sleep(2) # ì„œë²„ ë¶€í•˜ ë°©ì§€ìš© ë§¤ë„ˆ ëª¨ë“œ

    # ğŸ’¾ ê²°ê³¼ ì €ì¥ (CSV)
    if final_data:
        file_name = 'whitehouse_ai_search_results.csv'
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬", "ìˆ˜ì§‘ì¼"])
            writer.writeheader()
            writer.writerows(final_data)
        print(f"\nâœ… ì„±ê³µ! ë°±ì•…ê´€ ìµœì‹  AI ë‰´ìŠ¤ 5ê°œê°€ '{file_name}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì ì ˆí•œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
