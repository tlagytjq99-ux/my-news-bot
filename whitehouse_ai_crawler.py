import asyncio
import csv
import re
import os
from datetime import datetime
from urllib.parse import urljoin
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def get_whitehouse_details(crawler, url, config):
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •í™•í•œ ë°œí–‰ì¼ ì¶”ì¶œ"""
    try:
        result = await crawler.arun(url=url, config=config)
        if not (result.success and result.markdown): return "ë‚ ì§œí™•ì¸í•„ìš”"
        content = result.markdown[:2000]
        # ë‚ ì§œ íŒ¨í„´: January 29, 2026
        date_match = re.search(r'([A-Z][a-z]+ \d{1,2}, \d{4})', content)
        if date_match:
            dt = datetime.strptime(date_match.group(1), "%B %d, %Y")
            return dt.strftime("%Y-%m-%d")
    except: pass
    return datetime.now().strftime("%Y-%m-%d")

async def main():
    # ğŸ¯ ê²€ìƒ‰ì–´ 'Artificial Intelligence'ë¥¼ í¬í•¨í•œ ë‰´ìŠ¤ë£¸ ì£¼ì†Œ
    search_url = "https://www.whitehouse.gov/?s=Artificial+Intelligence"
    file_name = 'whitehouse_ai_report.csv'
    
    print(f"ğŸ“¡ ë°±ì•…ê´€ ë‰´ìŠ¤ë£¸ì—ì„œ 'AI' ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì •ë°€ ìŠ¤ìº”í•©ë‹ˆë‹¤...")

    existing_titles = set()
    if os.path.exists(file_name):
        with open(file_name, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader: existing_titles.add(row['ì œëª©'])

    browser_config = BrowserConfig(browser_type="chromium", headless=True)
    
    # ğŸ’¡ [í•µì‹¬ ì„¤ì •] ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸(.search-results)ê°€ ë‹¤ ëœ° ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
    run_config = CrawlerRunConfig(
        wait_for=".search-results__item", 
        delay_before_return_html=10.0, # ì¶©ë¶„íˆ ê¸°ë‹¤ë ¤ì•¼ ê²€ìƒ‰ ê²°ê³¼ê°€ ë¡œë”©ë©ë‹ˆë‹¤.
        cache_mode="bypass"
    )

    new_data = []
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=search_url, config=run_config)
        
        if result.success and result.markdown:
            # ğŸ’¡ [ì •ë°€ í•„í„°] ë‰´ìŠ¤ë£¸ ê²€ìƒ‰ ê²°ê³¼ëŠ” ë³´í†µ íŠ¹ì • íŒ¨í„´ì˜ ë§í¬ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
            # ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ ì•ˆì— ìˆëŠ” ì œëª©ê³¼ ë§í¬ë§Œ ê³¨ë¼ëƒ…ë‹ˆë‹¤.
            # ë§ˆí¬ë‹¤ìš´ì—ì„œ ê²€ìƒ‰ ì•„ì´í…œ íŒ¨í„´: [ì œëª©](https://www.whitehouse.gov/briefing-room/...)
            links = re.findall(r'\[([^\]]{20,})\]\((https://www\.whitehouse\.gov/briefing-room/[^\)]+)\)', result.markdown)
            
            # AI ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œ (ì œëª©ì— í¬í•¨ ì—¬ë¶€ í™•ì¸)
            ai_keywords = ['AI', 'ARTIFICIAL INTELLIGENCE', 'TECH', 'DIGITAL', 'CYBER', 'QUANTUM']
            
            count = 0
            for title, link in links:
                if count >= 5: break
                
                title_clean = title.strip()
                
                # ì œëª©ì— AI ê´€ë ¨ ë‹¨ì–´ê°€ ìˆëŠ”ì§€, ê·¸ë¦¬ê³  ì¤‘ë³µì€ ì•„ë‹Œì§€ í™•ì¸
                if any(kw in title_clean.upper() for kw in ai_keywords) and title_clean not in existing_titles:
                    print(f"   ğŸ†• ë°œê²¬: {title_clean[:40]}...")
                    exact_date = await get_whitehouse_details(crawler, link, run_config)

                    new_data.append({
                        "ê¸°ê´€": "ë°±ì•…ê´€(White House)",
                        "ë°œí–‰ì¼": exact_date,
                        "ì œëª©": title_clean,
                        "ë§í¬": link,
                        "ìˆ˜ì§‘ì¼": datetime.now().strftime("%Y-%m-%d")
                    })
                    count += 1
                    await asyncio.sleep(2)

    # ğŸ’¾ ê²°ê³¼ ì €ì¥
    if new_data:
        file_exists = os.path.exists(file_name)
        with open(file_name, 'a', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ë§í¬", "ìˆ˜ì§‘ì¼"])
            if not file_exists: writer.writeheader()
            writer.writerows(new_data)
        print(f"âœ… ì„±ê³µ! {len(new_data)}ê±´ì˜ AI ë‰´ìŠ¤ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("ğŸ’¡ ìƒˆë¡œìš´ AI ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
