import feedparser
import csv
import urllib.parse
from datetime import datetime
from googletrans import Translator
from googlenewsdecoder import gnewsdecoder

def main():
    # ğŸ¯ ì¿¼ë¦¬ì— filetype:pdfë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë„£ì§€ëŠ” ì•Šë˜(ëª¨ë“  í˜•íƒœ ìˆ˜ì§‘ ìœ„í•´), 
    # ìˆ˜ì§‘ í›„ PDF ì—¬ë¶€ë¥¼ íŒë³„í•˜ëŠ” ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    query = 'site:whitehouse.gov (intitle:"Artificial Intelligence" OR intitle:AI OR "Executive Order") -intitle:briefing'
    encoded_query = urllib.parse.quote(query)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
    
    file_name = 'whitehouse_ai_policy.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")

    print(f"ğŸ‡ºğŸ‡¸ ë°±ì•…ê´€ AI ì •ì±… ë° PDF ë¬¸ì„œ ë¶„ì„ ì‹œì‘...")
    final_data = []

    try:
        feed = feedparser.parse(rss_url)
        entries = sorted(feed.entries, key=lambda x: x.get('published_parsed'), reverse=True)[:10]
        
        for entry in entries:
            title_en = entry.title.split(' - ')[0]
            if len(title_en.split()) <= 3: continue

            # ğŸ’¡ [í•µì‹¬] ë§í¬ í•´ë…
            try:
                decoded = gnewsdecoder(entry.link)
                actual_link = decoded.get('decoded_url', entry.link)
            except:
                actual_link = entry.link

            # ğŸ’¡ [PDF íŒë³„ ë¡œì§]
            # ë§í¬ê°€ .pdfë¡œ ëë‚˜ê±°ë‚˜ ì œëª©ì— PDFê°€ í¬í•¨ëœ ê²½ìš°
            is_pdf = "NO"
            display_prefix = ""
            if actual_link.lower().endswith('.pdf') or ".pdf?" in actual_link.lower() or "[PDF]" in title_en.upper():
                is_pdf = "YES"
                display_prefix = "[PDF] "

            # ë‚ ì§œ ë° ë²ˆì—­
            pub_date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d') if hasattr(entry, 'published_parsed') else collected_date
            try:
                # ë²ˆì—­ ì‹œ PDF íƒœê·¸ëŠ” ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ë²ˆì—­
                title_ko = translator.translate(title_en.strip(), dest='ko').text
            except:
                title_ko = title_en

            final_data.append({
                "ê¸°ê´€": "WhiteHouse",
                "ë°œí–‰ì¼": pub_date,
                "ì œëª©": f"{display_prefix}{title_ko}", # ì œëª© ì•ì— [PDF] í‘œì‹œ
                "ì›ë¬¸": title_en,
                "PDFì—¬ë¶€": is_pdf,
                "ë§í¬": actual_link,
                "ìˆ˜ì§‘ì¼": collected_date
            })

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ğŸ’¾ ê²°ê³¼ ì €ì¥ (PDFì—¬ë¶€ ì»¬ëŸ¼ ì¶”ê°€)
    fieldnames = ["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "PDFì—¬ë¶€", "ë§í¬", "ìˆ˜ì§‘ì¼"]
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(final_data)
        print(f"âœ… ë¶„ì„ ì™„ë£Œ! PDF í¬í•¨ {len(final_data)}ê±´ ì €ì¥.")

if __name__ == "__main__":
    main()
