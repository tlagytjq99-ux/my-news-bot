import feedparser
import csv
import urllib.parse
from datetime import datetime
from googletrans import Translator
from googlenewsdecoder import gnewsdecoder

def main():
    # ğŸ¯ ì¿¼ë¦¬ ë°¸ëŸ°ìŠ¤ ì¡°ì •: 
    # PDFë¿ë§Œ ì•„ë‹ˆë¼ ì¼ë°˜ 'Priorities'ë‚˜ 'Fact Sheet'ë„ ê±¸ë¦¬ê²Œ ë²”ìœ„ë¥¼ ì‚´ì§ ë„“í˜”ìŠµë‹ˆë‹¤.
    query = 'site:whitehouse.gov (AI OR "Artificial Intelligence") -intitle:briefing -intitle:press -Cuba -Wildfire'
    
    encoded_query = urllib.parse.quote(query)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
    
    file_name = 'whitehouse_ai_policy.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")

    print(f"ğŸ‡ºğŸ‡¸ ë°±ì•…ê´€ AI ì¢…í•© ëª¨ë‹ˆí„°ë§ ì‹œì‘ (PDF ë° ì›¹í˜ì´ì§€)...")
    final_data = []

    try:
        feed = feedparser.parse(rss_url)
        # ì¢€ ë” í­ë„“ê²Œ ê²€í† í•˜ê¸° ìœ„í•´ ìƒìœ„ 20ê°œë¥¼ ì‚´í•ë‹ˆë‹¤.
        entries = feed.entries[:20]
        
        for entry in entries:
            title_en = entry.title.split(' - ')[0]
            
            # AI ê´€ë ¨ì„± ì¬ê²€ì¦
            if not any(kw in title_en.upper() for kw in ['AI', 'ARTIFICIAL', 'INTELLIGENCE']):
                continue

            try:
                decoded = gnewsdecoder(entry.link)
                actual_link = decoded.get('decoded_url', entry.link)
            except:
                actual_link = entry.link

            # PDF íŒë³„
            is_pdf = "YES" if actual_link.lower().endswith('.pdf') or ".pdf?" in actual_link.lower() else "NO"
            
            # ìš°ì„ ìˆœìœ„ ì ìˆ˜ (PDFì— ê°€ì‚°ì ì„ ì£¼ì–´ ìƒë‹¨ ë°°ì¹˜ ìœ ë„)
            priority_score = 10 if is_pdf == "YES" else 5
            
            # ë‚ ì§œ ë° ë²ˆì—­
            pub_date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d') if hasattr(entry, 'published_parsed') else collected_date
            try:
                title_ko = translator.translate(title_en.strip(), dest='ko').text
            except:
                title_ko = title_en

            final_data.append({
                "ê¸°ê´€": "WhiteHouse",
                "ë°œí–‰ì¼": pub_date,
                "ì œëª©": f"{'[PDF] ' if is_pdf == 'YES' else ''}{title_ko}",
                "ì›ë¬¸": title_en,
                "PDFì—¬ë¶€": is_pdf,
                "ë§í¬": actual_link,
                "ìˆ˜ì§‘ì¼": collected_date,
                "score": priority_score # ì •ë ¬ìš© ì„ì‹œ í•„ë“œ
            })

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 1. ë°œí–‰ì¼ìˆœ ì •ë ¬ -> 2. PDF ìš°ì„  ì •ë ¬
    final_data.sort(key=lambda x: (x['ë°œí–‰ì¼'], x['score']), reverse=True)

    # ğŸ’¾ ê²°ê³¼ ì €ì¥ (ì„ì‹œ í•„ë“œ score ì œì™¸)
    fieldnames = ["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "PDFì—¬ë¶€", "ë§í¬", "ìˆ˜ì§‘ì¼"]
    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
        writer.writeheader()
        writer.writerows(final_data)
        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(final_data)}ê±´ì˜ ì •ì±… ë¬¸ì„œ í™•ë³´.")

if __name__ == "__main__":
    main()
