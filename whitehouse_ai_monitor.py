import feedparser
import csv
import urllib.parse
from datetime import datetime
from googletrans import Translator
from googlenewsdecoder import gnewsdecoder

def main():
    # ğŸ¯ ê²€ìƒ‰ í•„í„°ë¥¼ ìµœì†Œí™”í•˜ì—¬ 'ëª¨ë“  AI ê´€ë ¨ í˜ì´ì§€'ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    # íŠ¹ì • ë‹¨ì–´(Report ë“±)ë¥¼ ê°•ì œí•˜ì§€ ì•Šì•„ì•¼ ì¼ë°˜ ì›¹ ë‰´ìŠ¤ë„ ê±¸ëŸ¬ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
    query = 'site:whitehouse.gov (AI OR "Artificial Intelligence") -intitle:briefing -intitle:press'
    
    encoded_query = urllib.parse.quote(query)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
    
    file_name = 'whitehouse_ai_policy.csv'
    translator = Translator()
    collected_date = datetime.now().strftime("%Y-%m-%d")

    print(f"ğŸ‡ºğŸ‡¸ ë°±ì•…ê´€ AI ì¢…í•© ìˆ˜ì§‘ ì‹œì‘ (ì›¹í˜ì´ì§€ & PDF í†µí•©)...")
    final_data = []

    try:
        feed = feedparser.parse(rss_url)
        # ë‹¤ì–‘í•œ í˜•íƒœë¥¼ ë³´ê¸° ìœ„í•´ ìƒìœ„ 20ê°œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        entries = feed.entries[:20]
        
        for entry in entries:
            title_en = entry.title.split(' - ')[0]
            
            # AI í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if not any(kw in title_en.upper() for kw in ['AI', 'ARTIFICIAL', 'INTELLIGENCE']):
                continue

            # ë§í¬ í•´ë…
            try:
                decoded = gnewsdecoder(entry.link)
                actual_link = decoded.get('decoded_url', entry.link)
            except:
                actual_link = entry.link

            # ğŸ’¡ [í•µì‹¬] PDF ì—¬ë¶€ë§Œ íŒë³„ (ìˆ˜ì§‘ì„ ì œí•œí•˜ì§€ ì•ŠìŒ)
            is_pdf = "YES" if actual_link.lower().endswith('.pdf') or ".pdf?" in actual_link.lower() else "NO"
            
            # ë°œí–‰ì¼ ë° ë²ˆì—­
            pub_date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d') if hasattr(entry, 'published_parsed') else collected_date
            try:
                title_ko = translator.translate(title_en.strip(), dest='ko').text
            except:
                title_ko = title_en

            # ğŸ’¡ ì œëª© ì˜†ì— í‘œì‹œë¥¼ ì›í•˜ì…¨ìœ¼ë‹ˆ ì œëª© ì•ì—ë§Œ [PDF]ë¥¼ ë¶™ì´ê³  
            # ì¼ë°˜ í˜ì´ì§€(NO)ëŠ” ê¹”ë”í•˜ê²Œ ì œëª©ë§Œ ë‚˜ê°‘ë‹ˆë‹¤.
            display_title = f"[PDF] {title_ko}" if is_pdf == "YES" else title_ko

            final_data.append({
                "ê¸°ê´€": "WhiteHouse",
                "ë°œí–‰ì¼": pub_date,
                "ì œëª©": display_title,
                "ì›ë¬¸": title_en,
                "PDFì—¬ë¶€": is_pdf,
                "ë§í¬": actual_link,
                "ìˆ˜ì§‘ì¼": collected_date
            })

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ìµœì‹ ìˆœ ì •ë ¬
    final_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)

    with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=["ê¸°ê´€", "ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸", "PDFì—¬ë¶€", "ë§í¬", "ìˆ˜ì§‘ì¼"])
        writer.writeheader()
        writer.writerows(final_data)
        print(f"âœ… ì™„ë£Œ! ì´ {len(final_data)}ê±´ì˜ í˜¼í•© ë¬¸ì„œ ì €ì¥.")

if __name__ == "__main__":
    main()
