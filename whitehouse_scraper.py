import feedparser
import csv
import urllib.parse
from datetime import datetime, timedelta
from googlenewsdecoder import gnewsdecoder
import time

def main():
    # 1. ì„¤ì •: 3ê°œì›”(90ì¼) ë° ì •ì‹ í‚¤ì›Œë“œ
    days_limit = 90
    # "Artificial Intelligence" ë¬¸êµ¬ê°€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ë§Œ ì°¾ë„ë¡ ì„¤ì •
    keyword = '"artificial intelligence"' 
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days_limit)
    
    # 2. êµ¬ê¸€ ë‰´ìŠ¤ RSS ì¿¼ë¦¬ ìƒì„±
    # site ì—°ì‚°ìì™€ í‚¤ì›Œë“œë¥¼ ì¡°í•©í•˜ì—¬ ë°±ì•…ê´€ ë‚´ ì •ì‹ ëª…ì¹­ ì–¸ê¸‰ ë¬¸ì„œ íƒ€ê²ŸíŒ…
    query = f'{keyword} site:whitehouse.gov'
    encoded_query = urllib.parse.quote(query)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"

    print(f"ğŸ“¡ ë°±ì•…ê´€ {keyword} ê´€ë ¨ ì •ì±… ìˆ˜ì§‘ ë° ë§í¬ í•´ë… ì¤‘... (ìµœê·¼ {days_limit}ì¼)")

    try:
        feed = feedparser.parse(rss_url)
        all_data = []

        for entry in feed.entries:
            try:
                pub_date_struct = entry.published_parsed
                pub_date_obj = datetime(*pub_date_struct[:3])
            except:
                continue

            # ê¸°ê°„ í•„í„°ë§
            if pub_date_obj >= start_date:
                raw_title = entry.title.split(' - ')[0].strip()
                
                # 3. êµ¬ê¸€ ë‰´ìŠ¤ ë§í¬ í•´ë… (ë°±ì•…ê´€ ê³µì‹ URLë¡œ ë³€í™˜)
                try:
                    decoded = gnewsdecoder(entry.link)
                    actual_link = decoded.get('decoded_url', entry.link)
                except:
                    actual_link = entry.link

                all_data.append({
                    "ë°œí–‰ì¼": pub_date_obj.strftime('%Y-%m-%d'),
                    "ì œëª©": raw_title,
                    "ì›ë¬¸ë§í¬": actual_link
                })
                time.sleep(0.1)

        # 4. CSV íŒŒì¼ ì €ì¥
        file_name = 'whitehouse_ai_report.csv'
        with open(file_name, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=["ë°œí–‰ì¼", "ì œëª©", "ì›ë¬¸ë§í¬"])
            writer.writeheader()
            if all_data:
                all_data.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)
                writer.writerows(all_data)
                print(f"âœ… ìˆ˜ì§‘ ì„±ê³µ: ì´ {len(all_data)}ê±´ì˜ ì „ë¬¸ ì •ì±… ìë£Œë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
            else:
                print(f"âš ï¸ ê²°ê³¼ ì—†ìŒ: ìµœê·¼ {days_limit}ì¼ ë‚´ì— í•´ë‹¹ í‚¤ì›Œë“œì˜ ìë£Œê°€ ì¸ë±ì‹±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
